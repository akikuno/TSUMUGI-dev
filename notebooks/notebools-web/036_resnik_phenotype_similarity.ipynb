{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnik Similarity for Phenotype Analysis\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Jaccard indexã®ä»£ã‚ã‚Šã«Resnik similarityã‚’ä½¿ç”¨ã—ã¦è¡¨ç¾å‹é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
    "é »å‡ºè¡¨ç¾å‹ï¼ˆpreweaning lethalityãªã©ï¼‰ã«ã‚ˆã‚‹åã‚Šã‚’è»½æ¸›ã—ã€ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆGenotype, Sex, Life stageï¼‰ã‚’è€ƒæ…®ã—ãŸæ‹¡å¼µç‰ˆã‚’å®Ÿè£…ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up to top directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "while not Path(\"LICENSE\").exists():\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MPO Ontology Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPOParser:\n",
    "    \"\"\"MPO (Mammalian Phenotype Ontology) OBOãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ã‚µãƒ¼\"\"\"\n",
    "\n",
    "    def __init__(self, obo_file_path: str):\n",
    "        self.obo_file_path = obo_file_path\n",
    "        self.terms = {}  # MP_ID -> {name, is_a, ...}\n",
    "        self.name_to_id = {}  # name -> MP_ID\n",
    "        self.hierarchy = defaultdict(set)  # child_id -> {parent_ids}\n",
    "        self.children = defaultdict(set)  # parent_id -> {child_ids}\n",
    "\n",
    "    def parse(self):\n",
    "        \"\"\"OBOãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼æ§‹é€ ã‚’æ§‹ç¯‰\"\"\"\n",
    "        print(f\"Parsing MPO ontology from {self.obo_file_path}...\")\n",
    "\n",
    "        with open(self.obo_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Termãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²\n",
    "        term_blocks = content.split(\"[Term]\")[1:]  # æœ€åˆã®ç©ºè¦ç´ ã‚’é™¤ã\n",
    "\n",
    "        for block in term_blocks:\n",
    "            term_data = self._parse_term_block(block)\n",
    "            if term_data and not term_data.get(\"is_obsolete\", False):\n",
    "                self.terms[term_data[\"id\"]] = term_data\n",
    "                self.name_to_id[term_data[\"name\"]] = term_data[\"id\"]\n",
    "\n",
    "        # éšå±¤é–¢ä¿‚ã‚’æ§‹ç¯‰\n",
    "        self._build_hierarchy()\n",
    "\n",
    "        print(f\"Parsed {len(self.terms)} MP terms\")\n",
    "        return self\n",
    "\n",
    "    def _parse_term_block(self, block: str) -> Optional[Dict]:\n",
    "        \"\"\"å€‹åˆ¥ã®Termãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒ‘ãƒ¼ã‚¹\"\"\"\n",
    "        lines = [line.strip() for line in block.strip().split(\"\\n\") if line.strip()]\n",
    "\n",
    "        term_data = {\"is_a\": [], \"synonyms\": [], \"is_obsolete\": False}\n",
    "\n",
    "        for line in lines:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "\n",
    "            key, value = line.split(\":\", 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if key == \"id\":\n",
    "                term_data[\"id\"] = value\n",
    "            elif key == \"name\":\n",
    "                term_data[\"name\"] = value\n",
    "            elif key == \"def\":\n",
    "                # \"definition text\" [references] ã®å½¢å¼\n",
    "                match = re.match(r'\"([^\"]+)\"', value)\n",
    "                if match:\n",
    "                    term_data[\"def\"] = match.group(1)\n",
    "            elif key == \"is_a\":\n",
    "                # MP:0000001 ! parent term name ã®å½¢å¼\n",
    "                parent_id = value.split(\" !\")[0].strip()\n",
    "                term_data[\"is_a\"].append(parent_id)\n",
    "            elif key == \"synonym\":\n",
    "                # \"synonym text\" EXACT [] ã®å½¢å¼\n",
    "                match = re.match(r'\"([^\"]+)\"', value)\n",
    "                if match:\n",
    "                    term_data[\"synonyms\"].append(match.group(1))\n",
    "            elif key == \"is_obsolete\":\n",
    "                term_data[\"is_obsolete\"] = value.lower() == \"true\"\n",
    "\n",
    "        return term_data if \"id\" in term_data and \"name\" in term_data else None\n",
    "\n",
    "    def _build_hierarchy(self):\n",
    "        \"\"\"è¦ªå­é–¢ä¿‚ã‹ã‚‰éšå±¤æ§‹é€ ã‚’æ§‹ç¯‰\"\"\"\n",
    "        for term_id, term_data in self.terms.items():\n",
    "            for parent_id in term_data.get(\"is_a\", []):\n",
    "                if parent_id in self.terms:\n",
    "                    self.hierarchy[term_id].add(parent_id)\n",
    "                    self.children[parent_id].add(term_id)\n",
    "\n",
    "    def get_ancestors(self, term_id: str) -> Set[str]:\n",
    "        \"\"\"æŒ‡å®šã•ã‚ŒãŸtermã®å…¨ç¥–å…ˆã‚’å–å¾—\"\"\"\n",
    "        ancestors = set()\n",
    "        stack = [term_id]\n",
    "\n",
    "        while stack:\n",
    "            current = stack.pop()\n",
    "            for parent in self.hierarchy.get(current, set()):\n",
    "                if parent not in ancestors:\n",
    "                    ancestors.add(parent)\n",
    "                    stack.append(parent)\n",
    "\n",
    "        return ancestors\n",
    "\n",
    "    def get_term_depth(self, term_id: str) -> int:\n",
    "        \"\"\"ãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®æ·±ã•ã‚’è¨ˆç®—\"\"\"\n",
    "        if not self.hierarchy.get(term_id):\n",
    "            return 0  # ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰\n",
    "\n",
    "        max_depth = 0\n",
    "        for parent in self.hierarchy[term_id]:\n",
    "            max_depth = max(max_depth, self.get_term_depth(parent) + 1)\n",
    "\n",
    "        return max_depth\n",
    "\n",
    "    def find_lowest_common_ancestor(self, term_a: str, term_b: str) -> Optional[str]:\n",
    "        \"\"\"2ã¤ã®termã®æœ€ä¸‹ä½å…±é€šç¥–å…ˆï¼ˆLCAï¼‰ã‚’è¦‹ã¤ã‘ã‚‹\"\"\"\n",
    "        # termåã‹ã‚‰IDã«å¤‰æ›\n",
    "        id_a = self.name_to_id.get(term_a, term_a)\n",
    "        id_b = self.name_to_id.get(term_b, term_b)\n",
    "\n",
    "        if id_a not in self.terms or id_b not in self.terms:\n",
    "            return None\n",
    "\n",
    "        # ä¸¡æ–¹ã®ç¥–å…ˆã‚’å–å¾—\n",
    "        ancestors_a = self.get_ancestors(id_a)\n",
    "        ancestors_a.add(id_a)  # è‡ªåˆ†è‡ªèº«ã‚‚å«ã‚ã‚‹\n",
    "\n",
    "        ancestors_b = self.get_ancestors(id_b)\n",
    "        ancestors_b.add(id_b)  # è‡ªåˆ†è‡ªèº«ã‚‚å«ã‚ã‚‹\n",
    "\n",
    "        # å…±é€šç¥–å…ˆ\n",
    "        common_ancestors = ancestors_a & ancestors_b\n",
    "\n",
    "        if not common_ancestors:\n",
    "            return None\n",
    "\n",
    "        # æœ€ã‚‚æ·±ã„ï¼ˆå…·ä½“çš„ãªï¼‰å…±é€šç¥–å…ˆã‚’é¸æŠ\n",
    "        lca = max(common_ancestors, key=lambda x: self.get_term_depth(x))\n",
    "\n",
    "        return lca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPOã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼ã‚’ãƒ‘ãƒ¼ã‚¹\n",
    "mpo_parser = MPOParser(\"data/ontology/mp.obo\")\n",
    "mpo_parser.parse()\n",
    "\n",
    "print(f\"Total terms: {len(mpo_parser.terms)}\")\n",
    "print(f\"\\nSample terms:\")\n",
    "for i, (term_id, term_data) in enumerate(mpo_parser.terms.items()):\n",
    "    if i < 5:\n",
    "        print(f\"  {term_id}: {term_data['name']}\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é »åº¦è¨ˆç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¢å­˜ã®TSUMUGIãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¡¨ç¾å‹é »åº¦ã‚’è¨ˆç®—\n",
    "def calculate_phenotype_frequencies(symbol_mptermname_path: str) -> Dict[str, int]:\n",
    "    \"\"\"éºä¼å­â†’è¡¨ç¾å‹ãƒãƒƒãƒ”ãƒ³ã‚°ã‹ã‚‰å„è¡¨ç¾å‹ã®å‡ºç¾é »åº¦ã‚’è¨ˆç®—\"\"\"\n",
    "\n",
    "    with open(symbol_mptermname_path, \"r\") as f:\n",
    "        symbol_mptermname = json.load(f)\n",
    "\n",
    "    # å…¨è¡¨ç¾å‹ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    phenotype_counts = Counter()\n",
    "\n",
    "    for gene, phenotypes in symbol_mptermname.items():\n",
    "        if phenotypes:  # ç©ºã§ãªã„å ´åˆ\n",
    "            for phenotype in phenotypes:\n",
    "                if phenotype.strip():  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆ\n",
    "                    phenotype_counts[phenotype.strip()] += 1\n",
    "\n",
    "    print(f\"Found {len(phenotype_counts)} unique phenotypes\")\n",
    "    print(f\"Total phenotype observations: {sum(phenotype_counts.values())}\")\n",
    "\n",
    "    return dict(phenotype_counts)\n",
    "\n",
    "\n",
    "# é »åº¦ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—\n",
    "phenotype_frequencies = calculate_phenotype_frequencies(\n",
    "    \"data/annotation/symbol_mptermname.json\"\n",
    ")\n",
    "\n",
    "# é »å‡ºè¡¨ç¾å‹TOP10ã‚’ç¢ºèª\n",
    "top_phenotypes = sorted(\n",
    "    phenotype_frequencies.items(), key=lambda x: x[1], reverse=True\n",
    ")[:10]\n",
    "print(\"\\nTop 10 most frequent phenotypes:\")\n",
    "for phenotype, count in top_phenotypes:\n",
    "    print(f\"  {count:4d}: {phenotype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information Contentè¨ˆç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_content(phenotype_frequencies: Dict[str, int], mpo_parser: MPOParser) -> Dict[str, float]:\n",
    "    \"\"\"å„è¡¨ç¾å‹ã®Information Content (IC)ã‚’è¨ˆç®—\"\"\"\n",
    "    \n",
    "    total_observations = sum(phenotype_frequencies.values())\n",
    "    ic_scores = {}\n",
    "    \n",
    "    for phenotype, frequency in phenotype_frequencies.items():\n",
    "        # ç¢ºç‡è¨ˆç®—\n",
    "        probability = frequency / total_observations\n",
    "        \n",
    "        # Information Content: IC = -log(probability)\n",
    "        ic_score = -math.log(probability) if probability > 0 else 0.0\n",
    "        \n",
    "        ic_scores[phenotype] = ic_score\n",
    "    \n",
    "    return ic_scores\n",
    "\n",
    "def extract_base_phenotype(phenotype: str) -> str:\n",
    "    \"\"\"ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãè¡¨ç¾å‹ã‹ã‚‰åŸºæœ¬è¡¨ç¾å‹ã‚’æŠ½å‡º\"\"\"\n",
    "    # æ‹¬å¼§ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ã‚’é™¤å»\n",
    "    pattern = r\"^(.+?)\\s*\\([^)]+\\)$\"\n",
    "    match = re.match(pattern, phenotype.strip())\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return phenotype.strip()\n",
    "\n",
    "# Information Contentã‚’è¨ˆç®—\n",
    "ic_scores = calculate_information_content(phenotype_frequencies, mpo_parser)\n",
    "\n",
    "print(f\"Calculated IC scores for {len(ic_scores)} phenotypes\")\n",
    "\n",
    "# å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­˜åœ¨ã™ã‚‹è¡¨ç¾å‹ã‚’ä½¿ç”¨\n",
    "sample_phenotypes = [\n",
    "    \"preweaning lethality, complete penetrance (Homo, Early)\",\n",
    "    \"abnormal skin morphology (Homo, Early)\",\n",
    "    \"abnormal kidney morphology (Homo, Early)\"\n",
    "]\n",
    "\n",
    "print(\"\\nSample IC scores:\")\n",
    "for phenotype in sample_phenotypes:\n",
    "    if phenotype in ic_scores:\n",
    "        freq = phenotype_frequencies[phenotype]\n",
    "        ic = ic_scores[phenotype]\n",
    "        print(f\"  {phenotype}\")\n",
    "        print(f\"    Frequency: {freq}, IC: {ic:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {phenotype} - NOT FOUND in data\")\n",
    "\n",
    "print(f\"\\nTop 5 phenotypes with highest IC (rarest):\")\n",
    "sorted_ic = sorted(ic_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for phenotype, ic in sorted_ic:\n",
    "    freq = phenotype_frequencies[phenotype]\n",
    "    print(f\"  IC: {ic:.3f}, Freq: {freq} - {phenotype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ‹¡å¼µResnik Similarityå®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedResnikSimilarity:\n",
    "    \"\"\"ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è€ƒæ…®å‹Resnik Similarityè¨ˆç®—ã‚¯ãƒ©ã‚¹ï¼ˆæ”¹è‰¯ç‰ˆï¼‰\"\"\"\n",
    "\n",
    "    def __init__(self, mpo_parser: MPOParser, ic_scores: Dict[str, float], phenotype_frequencies: Dict[str, int]):\n",
    "        self.mpo_parser = mpo_parser\n",
    "        self.ic_scores = ic_scores\n",
    "        self.phenotype_frequencies = phenotype_frequencies\n",
    "\n",
    "        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é¡ä¼¼åº¦ã®é‡ã¿ï¼ˆã•ã‚‰ã«å‰Šæ¸›ï¼‰\n",
    "        self.annotation_weights = {\"genotype\": 0.5, \"sex\": 0.3, \"life_stage\": 0.2}\n",
    "\n",
    "        # çµ±åˆæ™‚ã®é‡ã¿ï¼ˆã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®é‡ã¿ã‚’æ¥µé™ã¾ã§å‰Šæ¸›ï¼‰\n",
    "        self.base_weight = 0.98\n",
    "        self.annotation_weight = 0.02\n",
    "\n",
    "    def calculate_similarity(self, phenotype_a: str, phenotype_b: str) -> float:\n",
    "        \"\"\"æ‹¡å¼µResnik similarityã‚’è¨ˆç®—\"\"\"\n",
    "\n",
    "        # å®Œå…¨ä¸€è‡´ã®å ´åˆã¯è‡ªèº«ã®ICã‚¹ã‚³ã‚¢ã‚’è¿”ã™\n",
    "        if phenotype_a == phenotype_b:\n",
    "            return self.ic_scores.get(phenotype_a, 0.0)\n",
    "\n",
    "        # 1. è¡¨ç¾å‹ã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆ†é›¢\n",
    "        base_a, annotations_a = self.parse_phenotype_with_annotations(phenotype_a)\n",
    "        base_b, annotations_b = self.parse_phenotype_with_annotations(phenotype_b)\n",
    "\n",
    "        # 2. åŸºæœ¬è¡¨ç¾å‹ã®Resnik similarity\n",
    "        base_similarity = self.calculate_base_resnik_similarity(base_a, base_b)\n",
    "\n",
    "        # 3. ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é¡ä¼¼åº¦\n",
    "        annotation_similarity = self.calculate_annotation_similarity(\n",
    "            annotations_a, annotations_b\n",
    "        )\n",
    "\n",
    "        # 4. çµ±åˆé¡ä¼¼åº¦ - åŸºæœ¬è¡¨ç¾å‹ãŒç•°ãªã‚‹å ´åˆã¯ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã»ã¼ç„¡è¦–\n",
    "        if base_similarity < 0.01:  # åŸºæœ¬è¡¨ç¾å‹ãŒã»ã¼ç•°ãªã‚‹å ´åˆï¼ˆé–¾å€¤ã‚’ã‚ˆã‚Šå³ã—ãï¼‰\n",
    "            # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®é‡ã¿ã‚’æ¥µé™ã¾ã§å‰Šæ¸›\n",
    "            combined_similarity = 0.999 * base_similarity + 0.001 * annotation_similarity\n",
    "        elif base_similarity < 0.1:  # åŸºæœ¬è¡¨ç¾å‹ãŒå°‘ã—é¡ä¼¼ã™ã‚‹å ´åˆ\n",
    "            # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®é‡ã¿ã‚’ã‹ãªã‚Šå‰Šæ¸›\n",
    "            combined_similarity = 0.99 * base_similarity + 0.01 * annotation_similarity\n",
    "        else:\n",
    "            # é€šå¸¸ã®é‡ã¿ï¼ˆãã‚Œã§ã‚‚ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é‡è¦–ã‚’å‰Šæ¸›ï¼‰\n",
    "            combined_similarity = (\n",
    "                self.base_weight * base_similarity\n",
    "                + self.annotation_weight * annotation_similarity\n",
    "            )\n",
    "\n",
    "        return combined_similarity\n",
    "\n",
    "    def parse_phenotype_with_annotations(self, phenotype: str) -> Tuple[str, Dict]:\n",
    "        \"\"\"è¡¨ç¾å‹æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦åŸºæœ¬é …ç›®ã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«åˆ†é›¢\"\"\"\n",
    "        pattern = r\"^(.+?)\\s*\\(([^)]+)\\)$\"\n",
    "        match = re.match(pattern, phenotype.strip())\n",
    "\n",
    "        if not match:\n",
    "            return phenotype.strip(), {}\n",
    "\n",
    "        base_term = match.group(1).strip()\n",
    "        annotation_str = match.group(2).strip()\n",
    "\n",
    "        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ†é¡\n",
    "        annotations = {}\n",
    "\n",
    "        if annotation_str in [\"Homo\", \"Hetero\", \"Hemi\"]:\n",
    "            annotations[\"genotype\"] = annotation_str\n",
    "        elif annotation_str in [\"Male\", \"Female\"]:\n",
    "            annotations[\"sex\"] = annotation_str\n",
    "        elif annotation_str in [\"Early\", \"Late\", \"Embryo\", \"Interval\"]:\n",
    "            annotations[\"life_stage\"] = annotation_str\n",
    "        else:\n",
    "            # è¤‡æ•°ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®å ´åˆ\n",
    "            parts = [part.strip() for part in annotation_str.split(\",\")]\n",
    "            for part in parts:\n",
    "                if part in [\"Homo\", \"Hetero\", \"Hemi\"]:\n",
    "                    annotations[\"genotype\"] = part\n",
    "                elif part in [\"Male\", \"Female\"]:\n",
    "                    annotations[\"sex\"] = part\n",
    "                elif part in [\"Early\", \"Late\", \"Embryo\", \"Interval\"]:\n",
    "                    annotations[\"life_stage\"] = part\n",
    "\n",
    "        return base_term, annotations\n",
    "\n",
    "    def calculate_base_resnik_similarity(self, term_a: str, term_b: str) -> float:\n",
    "        \"\"\"åŸºæœ¬è¡¨ç¾å‹ã®Resnik similarityè¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰\"\"\"\n",
    "\n",
    "        # åŒä¸€é …ç›®ã®å ´åˆ\n",
    "        if term_a == term_b:\n",
    "            # åŸºæœ¬è¡¨ç¾å‹ã«å¯¾å¿œã™ã‚‹æœ€é«˜ICã‚¹ã‚³ã‚¢ã‚’å–å¾—\n",
    "            max_ic = 0.0\n",
    "            for phenotype, ic in self.ic_scores.items():\n",
    "                base_phenotype = extract_base_phenotype(phenotype)\n",
    "                if base_phenotype == term_a:\n",
    "                    max_ic = max(max_ic, ic)\n",
    "            return max_ic\n",
    "\n",
    "        # æ–‡å­—åˆ—é¡ä¼¼åº¦ã«ã‚ˆã‚‹ä»£æ›¿è¨ˆç®—ã‚’è¿½åŠ \n",
    "        string_similarity = self.calculate_string_similarity(term_a, term_b)\n",
    "        \n",
    "        # MPOã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼ã§LCAã‚’è¦‹ã¤ã‘ã‚‹\n",
    "        lca_id = self.mpo_parser.find_lowest_common_ancestor(term_a, term_b)\n",
    "        ontology_similarity = 0.0\n",
    "\n",
    "        if lca_id is not None:\n",
    "            # LCAã®åå‰ã‚’å–å¾—\n",
    "            lca_name = self.mpo_parser.terms[lca_id][\"name\"]\n",
    "\n",
    "            # LCAã«å¯¾å¿œã™ã‚‹æœ€é«˜ICã‚¹ã‚³ã‚¢ã‚’å–å¾—\n",
    "            max_ic = 0.0\n",
    "            for phenotype, ic in self.ic_scores.items():\n",
    "                base_phenotype = extract_base_phenotype(phenotype)\n",
    "                if base_phenotype == lca_name:\n",
    "                    max_ic = max(max_ic, ic)\n",
    "            ontology_similarity = max_ic\n",
    "\n",
    "        # ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼é¡ä¼¼åº¦ã¨æ–‡å­—åˆ—é¡ä¼¼åº¦ã®çµ„ã¿åˆã‚ã›\n",
    "        # ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯æ–‡å­—åˆ—é¡ä¼¼åº¦ã‚’ä½¿ç”¨\n",
    "        if ontology_similarity > 0:\n",
    "            return max(ontology_similarity, string_similarity)\n",
    "        else:\n",
    "            return string_similarity\n",
    "\n",
    "    def calculate_string_similarity(self, term_a: str, term_b: str) -> float:\n",
    "        \"\"\"æ–‡å­—åˆ—é¡ä¼¼åº¦ã«ã‚ˆã‚‹è£œå®Œè¨ˆç®—\"\"\"\n",
    "        \n",
    "        # å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º\n",
    "        if self.has_common_morphology_pattern(term_a, term_b):\n",
    "            # abnormal XXX morphology ã®ã‚ˆã†ãªå…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹å ´åˆ\n",
    "            return 1.0  # ä¸­ç¨‹åº¦ã®é¡ä¼¼åº¦\n",
    "        \n",
    "        # å…±é€šã®å˜èªæ•°ã«ã‚ˆã‚‹é¡ä¼¼åº¦\n",
    "        words_a = set(term_a.lower().split())\n",
    "        words_b = set(term_b.lower().split())\n",
    "        \n",
    "        if not words_a or not words_b:\n",
    "            return 0.0\n",
    "            \n",
    "        common_words = words_a & words_b\n",
    "        total_words = words_a | words_b\n",
    "        \n",
    "        jaccard_sim = len(common_words) / len(total_words)\n",
    "        \n",
    "        # é‡è¦ãªå˜èªï¼ˆabnormal, morphologyç­‰ï¼‰ã«é‡ã¿ã‚’ä»˜ã‘ã‚‹\n",
    "        important_words = {'abnormal', 'morphology', 'increased', 'decreased', 'phenotype'}\n",
    "        important_common = common_words & important_words\n",
    "        \n",
    "        if important_common:\n",
    "            bonus = len(important_common) * 0.3\n",
    "            jaccard_sim += bonus\n",
    "        \n",
    "        # IC scoreã§é‡ã¿ä»˜ã‘\n",
    "        max_ic_a = 0.0\n",
    "        max_ic_b = 0.0\n",
    "        \n",
    "        for phenotype, ic in self.ic_scores.items():\n",
    "            base_phenotype = extract_base_phenotype(phenotype)\n",
    "            if base_phenotype == term_a:\n",
    "                max_ic_a = max(max_ic_a, ic)\n",
    "            if base_phenotype == term_b:\n",
    "                max_ic_b = max(max_ic_b, ic)\n",
    "        \n",
    "        # å¹³å‡IC scoreã§èª¿æ•´\n",
    "        avg_ic = (max_ic_a + max_ic_b) / 2 if (max_ic_a > 0 and max_ic_b > 0) else 0\n",
    "        ic_factor = min(avg_ic / 10.0, 1.0)  # IC scoreã‚’0-1ã«ã‚¹ã‚±ãƒ¼ãƒ«\n",
    "        \n",
    "        final_similarity = jaccard_sim * ic_factor\n",
    "        \n",
    "        return min(final_similarity, 3.0)  # æœ€å¤§3.0ã«åˆ¶é™\n",
    "\n",
    "    def has_common_morphology_pattern(self, term_a: str, term_b: str) -> bool:\n",
    "        \"\"\"å…±é€šã®å½¢æ…‹å­¦çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º\"\"\"\n",
    "        \n",
    "        # abnormal XXX morphology ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        pattern_abnormal_morphology = r'^abnormal\\s+\\w+\\s+morphology$'\n",
    "        \n",
    "        if (re.match(pattern_abnormal_morphology, term_a) and \n",
    "            re.match(pattern_abnormal_morphology, term_b)):\n",
    "            return True\n",
    "        \n",
    "        # ãã®ä»–ã®å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ å¯èƒ½\n",
    "        # increased/decreased XXX level ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        pattern_level = r'^(increased|decreased)\\s+.*\\s+level$'\n",
    "        if (re.match(pattern_level, term_a) and \n",
    "            re.match(pattern_level, term_b)):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "    def calculate_annotation_similarity(\n",
    "        self, annotations_a: Dict, annotations_b: Dict\n",
    "    ) -> float:\n",
    "        \"\"\"ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–“ã®é¡ä¼¼åº¦è¨ˆç®—ï¼ˆã‚ˆã‚Šå³ã—ãï¼‰\"\"\"\n",
    "\n",
    "        total_weight = 0\n",
    "        similarity_sum = 0\n",
    "\n",
    "        # å„ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã«ã¤ã„ã¦é¡ä¼¼åº¦è¨ˆç®—\n",
    "        for annotation_type in [\"genotype\", \"sex\", \"life_stage\"]:\n",
    "            if annotation_type in annotations_a or annotation_type in annotations_b:\n",
    "                weight = self.annotation_weights[annotation_type]\n",
    "                total_weight += weight\n",
    "\n",
    "                if (\n",
    "                    annotation_type in annotations_a\n",
    "                    and annotation_type in annotations_b\n",
    "                ):\n",
    "                    # ä¸¡æ–¹ã«ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨\n",
    "                    if annotations_a[annotation_type] == annotations_b[annotation_type]:\n",
    "                        similarity_sum += weight * 1.0  # å®Œå…¨ä¸€è‡´\n",
    "                    else:\n",
    "                        # éƒ¨åˆ†çš„é¡ä¼¼åº¦\n",
    "                        partial_sim = self.get_annotation_partial_similarity(\n",
    "                            annotation_type,\n",
    "                            annotations_a[annotation_type],\n",
    "                            annotations_b[annotation_type],\n",
    "                        )\n",
    "                        similarity_sum += weight * partial_sim\n",
    "                elif (\n",
    "                    annotation_type in annotations_a or annotation_type in annotations_b\n",
    "                ):\n",
    "                    # ç‰‡æ–¹ã®ã¿ã«ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å­˜åœ¨ - ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ã•ã‚‰ã«å¼·åŒ–\n",
    "                    similarity_sum += weight * 0.01  # 0.1â†’0.01ã«å‰Šæ¸›\n",
    "\n",
    "        if total_weight == 0:\n",
    "            return 1.0  # ä¸¡æ–¹ã¨ã‚‚ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã—\n",
    "\n",
    "        return similarity_sum / total_weight\n",
    "\n",
    "    def get_annotation_partial_similarity(\n",
    "        self, annotation_type: str, value_a: str, value_b: str\n",
    "    ) -> float:\n",
    "        \"\"\"ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å€¤é–“ã®éƒ¨åˆ†çš„é¡ä¼¼åº¦ï¼ˆã•ã‚‰ã«å‰Šæ¸›ï¼‰\"\"\"\n",
    "\n",
    "        if annotation_type == \"genotype\":\n",
    "            genotype_similarity = {\n",
    "                (\"Hetero\", \"Homo\"): 0.1,  # 0.3â†’0.1ã«å‰Šæ¸›\n",
    "                (\"Hemi\", \"Homo\"): 0.05,   # 0.2â†’0.05ã«å‰Šæ¸›  \n",
    "                (\"Hemi\", \"Hetero\"): 0.05, # 0.2â†’0.05ã«å‰Šæ¸›\n",
    "            }\n",
    "            key = tuple(sorted([value_a, value_b]))\n",
    "            return genotype_similarity.get(key, 0.0)\n",
    "\n",
    "        elif annotation_type == \"sex\":\n",
    "            return 0.0  # Male vs Female: å®Œå…¨ã«ç•°ãªã‚‹\n",
    "\n",
    "        elif annotation_type == \"life_stage\":\n",
    "            life_stage_similarity = {\n",
    "                (\"Early\", \"Interval\"): 0.1,  # 0.3â†’0.1ã«å‰Šæ¸›\n",
    "                (\"Early\", \"Late\"): 0.02,     # 0.1â†’0.02ã«å‰Šæ¸›\n",
    "                (\"Early\", \"Embryo\"): 0.02,   # 0.1â†’0.02ã«å‰Šæ¸›\n",
    "                (\"Interval\", \"Late\"): 0.2,   # 0.4â†’0.2ã«å‰Šæ¸›\n",
    "                (\"Interval\", \"Embryo\"): 0.02, # 0.1â†’0.02ã«å‰Šæ¸›\n",
    "                (\"Late\", \"Embryo\"): 0.02,     # 0.1â†’0.02ã«å‰Šæ¸›\n",
    "            }\n",
    "            key = tuple(sorted([value_a, value_b]))\n",
    "            return life_stage_similarity.get(key, 0.0)\n",
    "\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ”¹è‰¯ç‰ˆExtendedResnikSimilarityã‚¯ãƒ©ã‚¹ã§å†åˆæœŸåŒ–\n",
    "extended_resnik = ExtendedResnikSimilarity(mpo_parser, ic_scores, phenotype_frequencies)\n",
    "\n",
    "# å•é¡Œã¨ãªã£ã¦ã„ãŸãƒšã‚¢ã‚’ãƒ†ã‚¹ãƒˆ\n",
    "problem_pairs = [\n",
    "    (\"preweaning lethality, complete penetrance (Homo, Early)\", \"abnormal heart morphology (Homo, Early)\"),\n",
    "    (\"abnormal skin morphology (Homo, Early)\", \"abnormal kidney morphology (Homo, Early)\"),\n",
    "    (\"abnormal skin morphology (Homo, Early)\", \"abnormal spleen morphology (Homo, Early)\"),\n",
    "]\n",
    "\n",
    "print(\"æ”¹è‰¯ç‰ˆ Extended Resnik Similarity ãƒ†ã‚¹ãƒˆ:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for phenotype_a, phenotype_b in problem_pairs:\n",
    "    # è©³ç´°åˆ†æ\n",
    "    base_a, ann_a = extended_resnik.parse_phenotype_with_annotations(phenotype_a)\n",
    "    base_b, ann_b = extended_resnik.parse_phenotype_with_annotations(phenotype_b)\n",
    "    \n",
    "    # å„ç¨®é¡ä¼¼åº¦ã‚’å€‹åˆ¥ã«è¨ˆç®—\n",
    "    base_sim = extended_resnik.calculate_base_resnik_similarity(base_a, base_b)\n",
    "    string_sim = extended_resnik.calculate_string_similarity(base_a, base_b)\n",
    "    ann_sim = extended_resnik.calculate_annotation_similarity(ann_a, ann_b)\n",
    "    \n",
    "    # æœ€çµ‚é¡ä¼¼åº¦è¨ˆç®—\n",
    "    similarity = extended_resnik.calculate_similarity(phenotype_a, phenotype_b)\n",
    "    \n",
    "    print(f\"\\nğŸ“ {phenotype_a}\")\n",
    "    print(f\"   vs\")\n",
    "    print(f\"   {phenotype_b}\")\n",
    "    print(f\"   ğŸ“Š çµæœ:\")\n",
    "    print(f\"      åŸºæœ¬é¡ä¼¼åº¦: {base_sim:.6f}\")\n",
    "    print(f\"      æ–‡å­—åˆ—é¡ä¼¼åº¦: {string_sim:.6f}\")\n",
    "    print(f\"      ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é¡ä¼¼åº¦: {ann_sim:.6f}\")\n",
    "    print(f\"      æœ€çµ‚é¡ä¼¼åº¦: {similarity:.6f}\")\n",
    "    \n",
    "    # å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯\n",
    "    has_pattern = extended_resnik.has_common_morphology_pattern(base_a, base_b)\n",
    "    print(f\"      å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³: {'Yes' if has_pattern else 'No'}\")\n",
    "    \n",
    "    # è¨ˆç®—è©³ç´°\n",
    "    if base_sim < 0.01:\n",
    "        expected_sim = 0.999 * base_sim + 0.001 * ann_sim\n",
    "        print(f\"      è¨ˆç®—å¼: 0.999 Ã— {base_sim:.6f} + 0.001 Ã— {ann_sim:.6f} = {expected_sim:.6f} (æ¥µé™å‰Šæ¸›)\")\n",
    "    elif base_sim < 0.1:\n",
    "        expected_sim = 0.99 * base_sim + 0.01 * ann_sim\n",
    "        print(f\"      è¨ˆç®—å¼: 0.99 Ã— {base_sim:.6f} + 0.01 Ã— {ann_sim:.6f} = {expected_sim:.6f} (å¼·å‰Šæ¸›)\")\n",
    "    else:\n",
    "        expected_sim = 0.98 * base_sim + 0.02 * ann_sim\n",
    "        print(f\"      è¨ˆç®—å¼: 0.98 Ã— {base_sim:.6f} + 0.02 Ã— {ann_sim:.6f} = {expected_sim:.6f} (é€šå¸¸)\")\n",
    "    \n",
    "    # é »åº¦æƒ…å ±\n",
    "    freq_a = phenotype_frequencies.get(phenotype_a, 0)\n",
    "    freq_b = phenotype_frequencies.get(phenotype_b, 0)\n",
    "    print(f\"      é »åº¦: {freq_a} vs {freq_b}\")\n",
    "    \n",
    "    # Jaccardã¨ã®æ¯”è¼ƒï¼ˆå‚è€ƒï¼‰\n",
    "    jaccard_sim = 1.0 if phenotype_a == phenotype_b else 0.0\n",
    "    print(f\"      Jaccardå‚è€ƒ: {jaccard_sim:.6f}\")\n",
    "    \n",
    "    # åˆ†æ\n",
    "    if phenotype_a != phenotype_b:\n",
    "        if 'abnormal' in base_a and 'morphology' in base_a and 'abnormal' in base_b and 'morphology' in base_b:\n",
    "            print(f\"      ğŸ’¡ morphologyç³»ã®è¡¨ç¾å‹ãƒšã‚¢ -> é¡ä¼¼åº¦å‘ä¸ŠæœŸå¾…\")\n",
    "        elif 'preweaning lethality' in base_a or 'preweaning lethality' in base_b:\n",
    "            print(f\"      ğŸ’¡ preweaning lethality vs ä»– -> æ¥µä½é¡ä¼¼åº¦æœŸå¾…\")\n",
    "\n",
    "print(f\"\\nâš™ï¸ æ”¹è‰¯ç‚¹:\")\n",
    "print(f\"   - æ–‡å­—åˆ—é¡ä¼¼åº¦ã«ã‚ˆã‚‹è£œå®Œæ©Ÿèƒ½ã‚’è¿½åŠ \")\n",
    "print(f\"   - å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºæ©Ÿèƒ½ã‚’è¿½åŠ \")\n",
    "print(f\"   - abnormal XXX morphology ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹åˆ¥æ‰±ã„\")\n",
    "print(f\"   - ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼ãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ä»£æ›¿æ‰‹æ®µã‚’æä¾›\")\n",
    "\n",
    "print(f\"\\nğŸ¯ æœŸå¾…ã•ã‚Œã‚‹çµæœ:\")\n",
    "print(f\"   1. preweaning lethality vs abnormal heart morphology: ~0.001 (æ¥µä½)\")\n",
    "print(f\"   2. abnormal skin morphology vs abnormal kidney morphology: >0.1 (ä¸­ç¨‹åº¦)\")\n",
    "print(f\"   3. å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤è¡¨ç¾å‹é–“ã§ã®é©åˆ‡ãªé¡ä¼¼åº¦å‘ä¸Š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ€§èƒ½æ¯”è¼ƒï¼šJaccard vs Resnik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard vs Extended Resnik ã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "comparison_pairs = [\n",
    "    (\"preweaning lethality, complete penetrance (Homo, Early)\", \"abnormal heart morphology (Homo, Early)\"),\n",
    "    (\"abnormal skin morphology (Homo, Early)\", \"abnormal kidney morphology (Homo, Early)\"),\n",
    "    (\"abnormal spleen morphology (Homo, Early)\", \"abnormal kidney morphology (Homo, Early)\"),\n",
    "]\n",
    "\n",
    "def jaccard_similarity(phenotype_a: str, phenotype_b: str) -> float:\n",
    "    \"\"\"å¾“æ¥ã®Jaccard similarityï¼ˆå‚è€ƒç”¨ï¼‰\"\"\"\n",
    "    if phenotype_a == phenotype_b:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "print(\"\\nä¿®æ­£å¾Œã®æ¯”è¼ƒ: Jaccard vs Extended Resnik Similarity\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for phenotype_a, phenotype_b in comparison_pairs:\n",
    "    jaccard_sim = jaccard_similarity(phenotype_a, phenotype_b)\n",
    "    resnik_sim = extended_resnik.calculate_similarity(phenotype_a, phenotype_b)\n",
    "\n",
    "    print(f\"\\nğŸ“ {phenotype_a}\")\n",
    "    print(f\"   vs\")\n",
    "    print(f\"   {phenotype_b}\")\n",
    "    print(f\"   ğŸ“Š Jaccard: {jaccard_sim:.4f}  |  Extended Resnik: {resnik_sim:.4f}\")\n",
    "\n",
    "    # è©³ç´°åˆ†æ\n",
    "    base_a, ann_a = extended_resnik.parse_phenotype_with_annotations(phenotype_a)\n",
    "    base_b, ann_b = extended_resnik.parse_phenotype_with_annotations(phenotype_b)\n",
    "    base_sim = extended_resnik.calculate_base_resnik_similarity(base_a, base_b)\n",
    "    ann_sim = extended_resnik.calculate_annotation_similarity(ann_a, ann_b)\n",
    "    \n",
    "    print(f\"   ğŸ” è©³ç´°:\")\n",
    "    print(f\"      åŸºæœ¬è¡¨ç¾å‹: '{base_a}' vs '{base_b}' (é¡ä¼¼åº¦: {base_sim:.4f})\")\n",
    "    print(f\"      ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: {ann_a} vs {ann_b} (é¡ä¼¼åº¦: {ann_sim:.4f})\")\n",
    "    \n",
    "    # é »åº¦æƒ…å ±ã¨ICæƒ…å ±ã‚‚è¡¨ç¤º\n",
    "    freq_a = phenotype_frequencies.get(phenotype_a, 0)\n",
    "    freq_b = phenotype_frequencies.get(phenotype_b, 0)\n",
    "    ic_a = ic_scores.get(phenotype_a, 0)\n",
    "    ic_b = ic_scores.get(phenotype_b, 0)\n",
    "    print(f\"      é »åº¦: {freq_a} vs {freq_b}\")\n",
    "    print(f\"      IC scores: {ic_a:.3f} vs {ic_b:.3f}\")\n",
    "    \n",
    "    # æ”¹å–„åŠ¹æœã‚’è©•ä¾¡\n",
    "    if jaccard_sim == 0.0 and resnik_sim < 0.1:\n",
    "        print(f\"      âœ… æ”¹å–„æˆåŠŸ: éé–¢é€£è¡¨ç¾å‹ã®é¡ä¼¼åº¦ãŒé©åˆ‡ã«ä½ã„ ({resnik_sim:.4f})\")\n",
    "    elif jaccard_sim == 0.0 and resnik_sim >= 0.1:\n",
    "        print(f\"      âš ï¸  è¦æ”¹å–„: éé–¢é€£è¡¨ç¾å‹ã®é¡ä¼¼åº¦ãŒã¾ã é«˜ã„ ({resnik_sim:.4f})\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Extended Resnik Similarity ã®åˆ©ç‚¹:\")\n",
    "print(f\"   1. é »å‡ºè¡¨ç¾å‹ã®åã‚Šã‚’è»½æ¸›\")\n",
    "print(f\"   2. ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼éšå±¤ã‚’è€ƒæ…®ã—ãŸæ„å‘³çš„é¡ä¼¼åº¦\")\n",
    "print(f\"   3. ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’é©åˆ‡ã«è€ƒæ…®\")\n",
    "print(f\"   4. éé–¢é€£è¡¨ç¾å‹é–“ã®é¡ä¼¼åº¦ã‚’é©åˆ‡ã«ä½ãæŠ‘åˆ¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ãƒ‡ãƒ¼ã‚¿ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—çµæœã‚’ä¿å­˜\n",
    "output_dir = Path(\"data/resnik_similarity\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. MPOãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä¿å­˜\n",
    "with open(output_dir / \"mpo_parser.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mpo_parser, f)\n",
    "\n",
    "# 2. IC scoresä¿å­˜\n",
    "with open(output_dir / \"ic_scores.json\", \"w\") as f:\n",
    "    json.dump(ic_scores, f, indent=2)\n",
    "\n",
    "# 3. é »åº¦ãƒ‡ãƒ¼ã‚¿ä¿å­˜\n",
    "with open(output_dir / \"phenotype_frequencies.json\", \"w\") as f:\n",
    "    json.dump(phenotype_frequencies, f, indent=2)\n",
    "\n",
    "# 4. ExtendedResnikSimilarityã‚¯ãƒ©ã‚¹ä¿å­˜\n",
    "with open(output_dir / \"extended_resnik_similarity.pkl\", \"wb\") as f:\n",
    "    pickle.dump(extended_resnik, f)\n",
    "\n",
    "print(f\"Resnik similarity data saved to {output_dir}/\")\n",
    "print(f\"Files saved:\")\n",
    "for file_path in output_dir.glob(\"*\"):\n",
    "    print(f\"  - {file_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ… Extended Resnik Similarity implementation completed!\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- Parsed {len(mpo_parser.terms)} MPO terms\")\n",
    "print(f\"- Calculated IC scores for {len(ic_scores)} phenotypes\")\n",
    "print(f\"- Implemented annotation-aware similarity calculation\")\n",
    "print(f\"- Results saved to data/resnik_similarity/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tsumugi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
