{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnik Similarity for Phenotype Analysis\n",
    "\n",
    "このノートブックでは、Jaccard indexの代わりにResnik similarityを使用して表現型類似度を計算します。\n",
    "頻出表現型（preweaning lethalityなど）による偏りを軽減し、アノテーション（Genotype, Sex, Life stage）を考慮した拡張版を実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up to top directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "while not Path(\"LICENSE\").exists():\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MPO Ontology Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPOParser:\n",
    "    \"\"\"MPO (Mammalian Phenotype Ontology) OBOファイルパーサー\"\"\"\n",
    "\n",
    "    def __init__(self, obo_file_path: str):\n",
    "        self.obo_file_path = obo_file_path\n",
    "        self.terms = {}  # MP_ID -> {name, is_a, ...}\n",
    "        self.name_to_id = {}  # name -> MP_ID\n",
    "        self.hierarchy = defaultdict(set)  # child_id -> {parent_ids}\n",
    "        self.children = defaultdict(set)  # parent_id -> {child_ids}\n",
    "\n",
    "    def parse(self):\n",
    "        \"\"\"OBOファイルをパースしてオントロジー構造を構築\"\"\"\n",
    "        print(f\"Parsing MPO ontology from {self.obo_file_path}...\")\n",
    "\n",
    "        with open(self.obo_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Termブロックに分割\n",
    "        term_blocks = content.split(\"[Term]\")[1:]  # 最初の空要素を除く\n",
    "\n",
    "        for block in term_blocks:\n",
    "            term_data = self._parse_term_block(block)\n",
    "            if term_data and not term_data.get(\"is_obsolete\", False):\n",
    "                self.terms[term_data[\"id\"]] = term_data\n",
    "                self.name_to_id[term_data[\"name\"]] = term_data[\"id\"]\n",
    "\n",
    "        # 階層関係を構築\n",
    "        self._build_hierarchy()\n",
    "\n",
    "        print(f\"Parsed {len(self.terms)} MP terms\")\n",
    "        return self\n",
    "\n",
    "    def _parse_term_block(self, block: str) -> Optional[Dict]:\n",
    "        \"\"\"個別のTermブロックをパース\"\"\"\n",
    "        lines = [line.strip() for line in block.strip().split(\"\\n\") if line.strip()]\n",
    "\n",
    "        term_data = {\"is_a\": [], \"synonyms\": [], \"is_obsolete\": False}\n",
    "\n",
    "        for line in lines:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "\n",
    "            key, value = line.split(\":\", 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if key == \"id\":\n",
    "                term_data[\"id\"] = value\n",
    "            elif key == \"name\":\n",
    "                term_data[\"name\"] = value\n",
    "            elif key == \"def\":\n",
    "                # \"definition text\" [references] の形式\n",
    "                match = re.match(r'\"([^\"]+)\"', value)\n",
    "                if match:\n",
    "                    term_data[\"def\"] = match.group(1)\n",
    "            elif key == \"is_a\":\n",
    "                # MP:0000001 ! parent term name の形式\n",
    "                parent_id = value.split(\" !\")[0].strip()\n",
    "                term_data[\"is_a\"].append(parent_id)\n",
    "            elif key == \"synonym\":\n",
    "                # \"synonym text\" EXACT [] の形式\n",
    "                match = re.match(r'\"([^\"]+)\"', value)\n",
    "                if match:\n",
    "                    term_data[\"synonyms\"].append(match.group(1))\n",
    "            elif key == \"is_obsolete\":\n",
    "                term_data[\"is_obsolete\"] = value.lower() == \"true\"\n",
    "\n",
    "        return term_data if \"id\" in term_data and \"name\" in term_data else None\n",
    "\n",
    "    def _build_hierarchy(self):\n",
    "        \"\"\"親子関係から階層構造を構築\"\"\"\n",
    "        for term_id, term_data in self.terms.items():\n",
    "            for parent_id in term_data.get(\"is_a\", []):\n",
    "                if parent_id in self.terms:\n",
    "                    self.hierarchy[term_id].add(parent_id)\n",
    "                    self.children[parent_id].add(term_id)\n",
    "\n",
    "    def get_ancestors(self, term_id: str) -> Set[str]:\n",
    "        \"\"\"指定されたtermの全祖先を取得\"\"\"\n",
    "        ancestors = set()\n",
    "        stack = [term_id]\n",
    "\n",
    "        while stack:\n",
    "            current = stack.pop()\n",
    "            for parent in self.hierarchy.get(current, set()):\n",
    "                if parent not in ancestors:\n",
    "                    ancestors.add(parent)\n",
    "                    stack.append(parent)\n",
    "\n",
    "        return ancestors\n",
    "\n",
    "    def get_term_depth(self, term_id: str) -> int:\n",
    "        \"\"\"ルートからの深さを計算\"\"\"\n",
    "        if not self.hierarchy.get(term_id):\n",
    "            return 0  # ルートノード\n",
    "\n",
    "        max_depth = 0\n",
    "        for parent in self.hierarchy[term_id]:\n",
    "            max_depth = max(max_depth, self.get_term_depth(parent) + 1)\n",
    "\n",
    "        return max_depth\n",
    "\n",
    "    def find_lowest_common_ancestor(self, term_a: str, term_b: str) -> Optional[str]:\n",
    "        \"\"\"2つのtermの最下位共通祖先（LCA）を見つける\"\"\"\n",
    "        # term名からIDに変換\n",
    "        id_a = self.name_to_id.get(term_a, term_a)\n",
    "        id_b = self.name_to_id.get(term_b, term_b)\n",
    "\n",
    "        if id_a not in self.terms or id_b not in self.terms:\n",
    "            return None\n",
    "\n",
    "        # 両方の祖先を取得\n",
    "        ancestors_a = self.get_ancestors(id_a)\n",
    "        ancestors_a.add(id_a)  # 自分自身も含める\n",
    "\n",
    "        ancestors_b = self.get_ancestors(id_b)\n",
    "        ancestors_b.add(id_b)  # 自分自身も含める\n",
    "\n",
    "        # 共通祖先\n",
    "        common_ancestors = ancestors_a & ancestors_b\n",
    "\n",
    "        if not common_ancestors:\n",
    "            return None\n",
    "\n",
    "        # 最も深い（具体的な）共通祖先を選択\n",
    "        lca = max(common_ancestors, key=lambda x: self.get_term_depth(x))\n",
    "\n",
    "        return lca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPOオントロジーをパース\n",
    "mpo_parser = MPOParser(\"data/ontology/mp.obo\")\n",
    "mpo_parser.parse()\n",
    "\n",
    "print(f\"Total terms: {len(mpo_parser.terms)}\")\n",
    "print(f\"\\nSample terms:\")\n",
    "for i, (term_id, term_data) in enumerate(mpo_parser.terms.items()):\n",
    "    if i < 5:\n",
    "        print(f\"  {term_id}: {term_data['name']}\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 既存データから頻度計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 既存のTSUMUGIデータから表現型頻度を計算\n",
    "def calculate_phenotype_frequencies(symbol_mptermname_path: str) -> Dict[str, int]:\n",
    "    \"\"\"遺伝子→表現型マッピングから各表現型の出現頻度を計算\"\"\"\n",
    "\n",
    "    with open(symbol_mptermname_path, \"r\") as f:\n",
    "        symbol_mptermname = json.load(f)\n",
    "\n",
    "    # 全表現型の出現回数をカウント\n",
    "    phenotype_counts = Counter()\n",
    "\n",
    "    for gene, phenotypes in symbol_mptermname.items():\n",
    "        if phenotypes:  # 空でない場合\n",
    "            for phenotype in phenotypes:\n",
    "                if phenotype.strip():  # 空文字列でない場合\n",
    "                    phenotype_counts[phenotype.strip()] += 1\n",
    "\n",
    "    print(f\"Found {len(phenotype_counts)} unique phenotypes\")\n",
    "    print(f\"Total phenotype observations: {sum(phenotype_counts.values())}\")\n",
    "\n",
    "    return dict(phenotype_counts)\n",
    "\n",
    "\n",
    "# 頻度データを計算\n",
    "phenotype_frequencies = calculate_phenotype_frequencies(\n",
    "    \"data/annotation/symbol_mptermname.json\"\n",
    ")\n",
    "\n",
    "# 頻出表現型TOP10を確認\n",
    "top_phenotypes = sorted(\n",
    "    phenotype_frequencies.items(), key=lambda x: x[1], reverse=True\n",
    ")[:10]\n",
    "print(\"\\nTop 10 most frequent phenotypes:\")\n",
    "for phenotype, count in top_phenotypes:\n",
    "    print(f\"  {count:4d}: {phenotype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information Content計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_content(phenotype_frequencies: Dict[str, int], mpo_parser: MPOParser) -> Dict[str, float]:\n",
    "    \"\"\"各表現型のInformation Content (IC)を計算\"\"\"\n",
    "    \n",
    "    total_observations = sum(phenotype_frequencies.values())\n",
    "    ic_scores = {}\n",
    "    \n",
    "    for phenotype, frequency in phenotype_frequencies.items():\n",
    "        # 確率計算\n",
    "        probability = frequency / total_observations\n",
    "        \n",
    "        # Information Content: IC = -log(probability)\n",
    "        ic_score = -math.log(probability) if probability > 0 else 0.0\n",
    "        \n",
    "        ic_scores[phenotype] = ic_score\n",
    "    \n",
    "    return ic_scores\n",
    "\n",
    "def extract_base_phenotype(phenotype: str) -> str:\n",
    "    \"\"\"アノテーション付き表現型から基本表現型を抽出\"\"\"\n",
    "    # 括弧で囲まれた部分を除去\n",
    "    pattern = r\"^(.+?)\\s*\\([^)]+\\)$\"\n",
    "    match = re.match(pattern, phenotype.strip())\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return phenotype.strip()\n",
    "\n",
    "# Information Contentを計算\n",
    "ic_scores = calculate_information_content(phenotype_frequencies, mpo_parser)\n",
    "\n",
    "print(f\"Calculated IC scores for {len(ic_scores)} phenotypes\")\n",
    "\n",
    "# 実際のデータから存在する表現型を使用\n",
    "sample_phenotypes = [\n",
    "    \"preweaning lethality, complete penetrance (Homo, Early)\",\n",
    "    \"abnormal skin morphology (Homo, Early)\",\n",
    "    \"abnormal kidney morphology (Homo, Early)\"\n",
    "]\n",
    "\n",
    "print(\"\\nSample IC scores:\")\n",
    "for phenotype in sample_phenotypes:\n",
    "    if phenotype in ic_scores:\n",
    "        freq = phenotype_frequencies[phenotype]\n",
    "        ic = ic_scores[phenotype]\n",
    "        print(f\"  {phenotype}\")\n",
    "        print(f\"    Frequency: {freq}, IC: {ic:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {phenotype} - NOT FOUND in data\")\n",
    "\n",
    "print(f\"\\nTop 5 phenotypes with highest IC (rarest):\")\n",
    "sorted_ic = sorted(ic_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for phenotype, ic in sorted_ic:\n",
    "    freq = phenotype_frequencies[phenotype]\n",
    "    print(f\"  IC: {ic:.3f}, Freq: {freq} - {phenotype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 拡張Resnik Similarity実装"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedResnikSimilarity:\n",
    "    \"\"\"アノテーション考慮型Resnik Similarity計算クラス（改良版）\"\"\"\n",
    "\n",
    "    def __init__(self, mpo_parser: MPOParser, ic_scores: Dict[str, float], phenotype_frequencies: Dict[str, int]):\n",
    "        self.mpo_parser = mpo_parser\n",
    "        self.ic_scores = ic_scores\n",
    "        self.phenotype_frequencies = phenotype_frequencies\n",
    "\n",
    "        # アノテーション類似度の重み（さらに削減）\n",
    "        self.annotation_weights = {\"genotype\": 0.5, \"sex\": 0.3, \"life_stage\": 0.2}\n",
    "\n",
    "        # 統合時の重み（アノテーションの重みを極限まで削減）\n",
    "        self.base_weight = 0.98\n",
    "        self.annotation_weight = 0.02\n",
    "\n",
    "    def calculate_similarity(self, phenotype_a: str, phenotype_b: str) -> float:\n",
    "        \"\"\"拡張Resnik similarityを計算\"\"\"\n",
    "\n",
    "        # 完全一致の場合は自身のICスコアを返す\n",
    "        if phenotype_a == phenotype_b:\n",
    "            return self.ic_scores.get(phenotype_a, 0.0)\n",
    "\n",
    "        # 1. 表現型とアノテーションを分離\n",
    "        base_a, annotations_a = self.parse_phenotype_with_annotations(phenotype_a)\n",
    "        base_b, annotations_b = self.parse_phenotype_with_annotations(phenotype_b)\n",
    "\n",
    "        # 2. 基本表現型のResnik similarity\n",
    "        base_similarity = self.calculate_base_resnik_similarity(base_a, base_b)\n",
    "\n",
    "        # 3. アノテーション類似度\n",
    "        annotation_similarity = self.calculate_annotation_similarity(\n",
    "            annotations_a, annotations_b\n",
    "        )\n",
    "\n",
    "        # 4. 統合類似度 - 基本表現型が異なる場合はアノテーションをほぼ無視\n",
    "        if base_similarity < 0.01:  # 基本表現型がほぼ異なる場合（閾値をより厳しく）\n",
    "            # アノテーションの重みを極限まで削減\n",
    "            combined_similarity = 0.999 * base_similarity + 0.001 * annotation_similarity\n",
    "        elif base_similarity < 0.1:  # 基本表現型が少し類似する場合\n",
    "            # アノテーションの重みをかなり削減\n",
    "            combined_similarity = 0.99 * base_similarity + 0.01 * annotation_similarity\n",
    "        else:\n",
    "            # 通常の重み（それでもアノテーション重視を削減）\n",
    "            combined_similarity = (\n",
    "                self.base_weight * base_similarity\n",
    "                + self.annotation_weight * annotation_similarity\n",
    "            )\n",
    "\n",
    "        return combined_similarity\n",
    "\n",
    "    def parse_phenotype_with_annotations(self, phenotype: str) -> Tuple[str, Dict]:\n",
    "        \"\"\"表現型文字列をパースして基本項目とアノテーションに分離\"\"\"\n",
    "        pattern = r\"^(.+?)\\s*\\(([^)]+)\\)$\"\n",
    "        match = re.match(pattern, phenotype.strip())\n",
    "\n",
    "        if not match:\n",
    "            return phenotype.strip(), {}\n",
    "\n",
    "        base_term = match.group(1).strip()\n",
    "        annotation_str = match.group(2).strip()\n",
    "\n",
    "        # アノテーションの分類\n",
    "        annotations = {}\n",
    "\n",
    "        if annotation_str in [\"Homo\", \"Hetero\", \"Hemi\"]:\n",
    "            annotations[\"genotype\"] = annotation_str\n",
    "        elif annotation_str in [\"Male\", \"Female\"]:\n",
    "            annotations[\"sex\"] = annotation_str\n",
    "        elif annotation_str in [\"Early\", \"Late\", \"Embryo\", \"Interval\"]:\n",
    "            annotations[\"life_stage\"] = annotation_str\n",
    "        else:\n",
    "            # 複数アノテーションの場合\n",
    "            parts = [part.strip() for part in annotation_str.split(\",\")]\n",
    "            for part in parts:\n",
    "                if part in [\"Homo\", \"Hetero\", \"Hemi\"]:\n",
    "                    annotations[\"genotype\"] = part\n",
    "                elif part in [\"Male\", \"Female\"]:\n",
    "                    annotations[\"sex\"] = part\n",
    "                elif part in [\"Early\", \"Late\", \"Embryo\", \"Interval\"]:\n",
    "                    annotations[\"life_stage\"] = part\n",
    "\n",
    "        return base_term, annotations\n",
    "\n",
    "    def calculate_base_resnik_similarity(self, term_a: str, term_b: str) -> float:\n",
    "        \"\"\"基本表現型のResnik similarity計算（改良版）\"\"\"\n",
    "\n",
    "        # 同一項目の場合\n",
    "        if term_a == term_b:\n",
    "            # 基本表現型に対応する最高ICスコアを取得\n",
    "            max_ic = 0.0\n",
    "            for phenotype, ic in self.ic_scores.items():\n",
    "                base_phenotype = extract_base_phenotype(phenotype)\n",
    "                if base_phenotype == term_a:\n",
    "                    max_ic = max(max_ic, ic)\n",
    "            return max_ic\n",
    "\n",
    "        # 文字列類似度による代替計算を追加\n",
    "        string_similarity = self.calculate_string_similarity(term_a, term_b)\n",
    "        \n",
    "        # MPOオントロジーでLCAを見つける\n",
    "        lca_id = self.mpo_parser.find_lowest_common_ancestor(term_a, term_b)\n",
    "        ontology_similarity = 0.0\n",
    "\n",
    "        if lca_id is not None:\n",
    "            # LCAの名前を取得\n",
    "            lca_name = self.mpo_parser.terms[lca_id][\"name\"]\n",
    "\n",
    "            # LCAに対応する最高ICスコアを取得\n",
    "            max_ic = 0.0\n",
    "            for phenotype, ic in self.ic_scores.items():\n",
    "                base_phenotype = extract_base_phenotype(phenotype)\n",
    "                if base_phenotype == lca_name:\n",
    "                    max_ic = max(max_ic, ic)\n",
    "            ontology_similarity = max_ic\n",
    "\n",
    "        # オントロジー類似度と文字列類似度の組み合わせ\n",
    "        # オントロジーが利用できない場合は文字列類似度を使用\n",
    "        if ontology_similarity > 0:\n",
    "            return max(ontology_similarity, string_similarity)\n",
    "        else:\n",
    "            return string_similarity\n",
    "\n",
    "    def calculate_string_similarity(self, term_a: str, term_b: str) -> float:\n",
    "        \"\"\"文字列類似度による補完計算\"\"\"\n",
    "        \n",
    "        # 共通パターンを検出\n",
    "        if self.has_common_morphology_pattern(term_a, term_b):\n",
    "            # abnormal XXX morphology のような共通パターンがある場合\n",
    "            return 1.0  # 中程度の類似度\n",
    "        \n",
    "        # 共通の単語数による類似度\n",
    "        words_a = set(term_a.lower().split())\n",
    "        words_b = set(term_b.lower().split())\n",
    "        \n",
    "        if not words_a or not words_b:\n",
    "            return 0.0\n",
    "            \n",
    "        common_words = words_a & words_b\n",
    "        total_words = words_a | words_b\n",
    "        \n",
    "        jaccard_sim = len(common_words) / len(total_words)\n",
    "        \n",
    "        # 重要な単語（abnormal, morphology等）に重みを付ける\n",
    "        important_words = {'abnormal', 'morphology', 'increased', 'decreased', 'phenotype'}\n",
    "        important_common = common_words & important_words\n",
    "        \n",
    "        if important_common:\n",
    "            bonus = len(important_common) * 0.3\n",
    "            jaccard_sim += bonus\n",
    "        \n",
    "        # IC scoreで重み付け\n",
    "        max_ic_a = 0.0\n",
    "        max_ic_b = 0.0\n",
    "        \n",
    "        for phenotype, ic in self.ic_scores.items():\n",
    "            base_phenotype = extract_base_phenotype(phenotype)\n",
    "            if base_phenotype == term_a:\n",
    "                max_ic_a = max(max_ic_a, ic)\n",
    "            if base_phenotype == term_b:\n",
    "                max_ic_b = max(max_ic_b, ic)\n",
    "        \n",
    "        # 平均IC scoreで調整\n",
    "        avg_ic = (max_ic_a + max_ic_b) / 2 if (max_ic_a > 0 and max_ic_b > 0) else 0\n",
    "        ic_factor = min(avg_ic / 10.0, 1.0)  # IC scoreを0-1にスケール\n",
    "        \n",
    "        final_similarity = jaccard_sim * ic_factor\n",
    "        \n",
    "        return min(final_similarity, 3.0)  # 最大3.0に制限\n",
    "\n",
    "    def has_common_morphology_pattern(self, term_a: str, term_b: str) -> bool:\n",
    "        \"\"\"共通の形態学的パターンを検出\"\"\"\n",
    "        \n",
    "        # abnormal XXX morphology パターン\n",
    "        pattern_abnormal_morphology = r'^abnormal\\s+\\w+\\s+morphology$'\n",
    "        \n",
    "        if (re.match(pattern_abnormal_morphology, term_a) and \n",
    "            re.match(pattern_abnormal_morphology, term_b)):\n",
    "            return True\n",
    "        \n",
    "        # その他の共通パターンを追加可能\n",
    "        # increased/decreased XXX level パターン\n",
    "        pattern_level = r'^(increased|decreased)\\s+.*\\s+level$'\n",
    "        if (re.match(pattern_level, term_a) and \n",
    "            re.match(pattern_level, term_b)):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "    def calculate_annotation_similarity(\n",
    "        self, annotations_a: Dict, annotations_b: Dict\n",
    "    ) -> float:\n",
    "        \"\"\"アノテーション間の類似度計算（より厳しく）\"\"\"\n",
    "\n",
    "        total_weight = 0\n",
    "        similarity_sum = 0\n",
    "\n",
    "        # 各アノテーションタイプについて類似度計算\n",
    "        for annotation_type in [\"genotype\", \"sex\", \"life_stage\"]:\n",
    "            if annotation_type in annotations_a or annotation_type in annotations_b:\n",
    "                weight = self.annotation_weights[annotation_type]\n",
    "                total_weight += weight\n",
    "\n",
    "                if (\n",
    "                    annotation_type in annotations_a\n",
    "                    and annotation_type in annotations_b\n",
    "                ):\n",
    "                    # 両方にアノテーションが存在\n",
    "                    if annotations_a[annotation_type] == annotations_b[annotation_type]:\n",
    "                        similarity_sum += weight * 1.0  # 完全一致\n",
    "                    else:\n",
    "                        # 部分的類似度\n",
    "                        partial_sim = self.get_annotation_partial_similarity(\n",
    "                            annotation_type,\n",
    "                            annotations_a[annotation_type],\n",
    "                            annotations_b[annotation_type],\n",
    "                        )\n",
    "                        similarity_sum += weight * partial_sim\n",
    "                elif (\n",
    "                    annotation_type in annotations_a or annotation_type in annotations_b\n",
    "                ):\n",
    "                    # 片方のみにアノテーション存在 - ペナルティをさらに強化\n",
    "                    similarity_sum += weight * 0.01  # 0.1→0.01に削減\n",
    "\n",
    "        if total_weight == 0:\n",
    "            return 1.0  # 両方ともアノテーションなし\n",
    "\n",
    "        return similarity_sum / total_weight\n",
    "\n",
    "    def get_annotation_partial_similarity(\n",
    "        self, annotation_type: str, value_a: str, value_b: str\n",
    "    ) -> float:\n",
    "        \"\"\"アノテーション値間の部分的類似度（さらに削減）\"\"\"\n",
    "\n",
    "        if annotation_type == \"genotype\":\n",
    "            genotype_similarity = {\n",
    "                (\"Hetero\", \"Homo\"): 0.1,  # 0.3→0.1に削減\n",
    "                (\"Hemi\", \"Homo\"): 0.05,   # 0.2→0.05に削減  \n",
    "                (\"Hemi\", \"Hetero\"): 0.05, # 0.2→0.05に削減\n",
    "            }\n",
    "            key = tuple(sorted([value_a, value_b]))\n",
    "            return genotype_similarity.get(key, 0.0)\n",
    "\n",
    "        elif annotation_type == \"sex\":\n",
    "            return 0.0  # Male vs Female: 完全に異なる\n",
    "\n",
    "        elif annotation_type == \"life_stage\":\n",
    "            life_stage_similarity = {\n",
    "                (\"Early\", \"Interval\"): 0.1,  # 0.3→0.1に削減\n",
    "                (\"Early\", \"Late\"): 0.02,     # 0.1→0.02に削減\n",
    "                (\"Early\", \"Embryo\"): 0.02,   # 0.1→0.02に削減\n",
    "                (\"Interval\", \"Late\"): 0.2,   # 0.4→0.2に削減\n",
    "                (\"Interval\", \"Embryo\"): 0.02, # 0.1→0.02に削減\n",
    "                (\"Late\", \"Embryo\"): 0.02,     # 0.1→0.02に削減\n",
    "            }\n",
    "            key = tuple(sorted([value_a, value_b]))\n",
    "            return life_stage_similarity.get(key, 0.0)\n",
    "\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. テスト実行"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改良版ExtendedResnikSimilarityクラスで再初期化\n",
    "extended_resnik = ExtendedResnikSimilarity(mpo_parser, ic_scores, phenotype_frequencies)\n",
    "\n",
    "# 問題となっていたペアをテスト\n",
    "problem_pairs = [\n",
    "    (\"preweaning lethality, complete penetrance (Homo, Early)\", \"abnormal heart morphology (Homo, Early)\"),\n",
    "    (\"abnormal skin morphology (Homo, Early)\", \"abnormal kidney morphology (Homo, Early)\"),\n",
    "    (\"abnormal skin morphology (Homo, Early)\", \"abnormal spleen morphology (Homo, Early)\"),\n",
    "]\n",
    "\n",
    "print(\"改良版 Extended Resnik Similarity テスト:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for phenotype_a, phenotype_b in problem_pairs:\n",
    "    # 詳細分析\n",
    "    base_a, ann_a = extended_resnik.parse_phenotype_with_annotations(phenotype_a)\n",
    "    base_b, ann_b = extended_resnik.parse_phenotype_with_annotations(phenotype_b)\n",
    "    \n",
    "    # 各種類似度を個別に計算\n",
    "    base_sim = extended_resnik.calculate_base_resnik_similarity(base_a, base_b)\n",
    "    string_sim = extended_resnik.calculate_string_similarity(base_a, base_b)\n",
    "    ann_sim = extended_resnik.calculate_annotation_similarity(ann_a, ann_b)\n",
    "    \n",
    "    # 最終類似度計算\n",
    "    similarity = extended_resnik.calculate_similarity(phenotype_a, phenotype_b)\n",
    "    \n",
    "    print(f\"\\n📍 {phenotype_a}\")\n",
    "    print(f\"   vs\")\n",
    "    print(f\"   {phenotype_b}\")\n",
    "    print(f\"   📊 結果:\")\n",
    "    print(f\"      基本類似度: {base_sim:.6f}\")\n",
    "    print(f\"      文字列類似度: {string_sim:.6f}\")\n",
    "    print(f\"      アノテーション類似度: {ann_sim:.6f}\")\n",
    "    print(f\"      最終類似度: {similarity:.6f}\")\n",
    "    \n",
    "    # 共通パターンチェック\n",
    "    has_pattern = extended_resnik.has_common_morphology_pattern(base_a, base_b)\n",
    "    print(f\"      共通パターン: {'Yes' if has_pattern else 'No'}\")\n",
    "    \n",
    "    # 計算詳細\n",
    "    if base_sim < 0.01:\n",
    "        expected_sim = 0.999 * base_sim + 0.001 * ann_sim\n",
    "        print(f\"      計算式: 0.999 × {base_sim:.6f} + 0.001 × {ann_sim:.6f} = {expected_sim:.6f} (極限削減)\")\n",
    "    elif base_sim < 0.1:\n",
    "        expected_sim = 0.99 * base_sim + 0.01 * ann_sim\n",
    "        print(f\"      計算式: 0.99 × {base_sim:.6f} + 0.01 × {ann_sim:.6f} = {expected_sim:.6f} (強削減)\")\n",
    "    else:\n",
    "        expected_sim = 0.98 * base_sim + 0.02 * ann_sim\n",
    "        print(f\"      計算式: 0.98 × {base_sim:.6f} + 0.02 × {ann_sim:.6f} = {expected_sim:.6f} (通常)\")\n",
    "    \n",
    "    # 頻度情報\n",
    "    freq_a = phenotype_frequencies.get(phenotype_a, 0)\n",
    "    freq_b = phenotype_frequencies.get(phenotype_b, 0)\n",
    "    print(f\"      頻度: {freq_a} vs {freq_b}\")\n",
    "    \n",
    "    # Jaccardとの比較（参考）\n",
    "    jaccard_sim = 1.0 if phenotype_a == phenotype_b else 0.0\n",
    "    print(f\"      Jaccard参考: {jaccard_sim:.6f}\")\n",
    "    \n",
    "    # 分析\n",
    "    if phenotype_a != phenotype_b:\n",
    "        if 'abnormal' in base_a and 'morphology' in base_a and 'abnormal' in base_b and 'morphology' in base_b:\n",
    "            print(f\"      💡 morphology系の表現型ペア -> 類似度向上期待\")\n",
    "        elif 'preweaning lethality' in base_a or 'preweaning lethality' in base_b:\n",
    "            print(f\"      💡 preweaning lethality vs 他 -> 極低類似度期待\")\n",
    "\n",
    "print(f\"\\n⚙️ 改良点:\")\n",
    "print(f\"   - 文字列類似度による補完機能を追加\")\n",
    "print(f\"   - 共通パターン検出機能を追加\")\n",
    "print(f\"   - abnormal XXX morphology パターンを特別扱い\")\n",
    "print(f\"   - オントロジーが利用できない場合の代替手段を提供\")\n",
    "\n",
    "print(f\"\\n🎯 期待される結果:\")\n",
    "print(f\"   1. preweaning lethality vs abnormal heart morphology: ~0.001 (極低)\")\n",
    "print(f\"   2. abnormal skin morphology vs abnormal kidney morphology: >0.1 (中程度)\")\n",
    "print(f\"   3. 共通パターンを持つ表現型間での適切な類似度向上\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 性能比較：Jaccard vs Resnik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard vs Extended Resnik の比較テスト（修正版）\n",
    "comparison_pairs = [\n",
    "    (\"preweaning lethality, complete penetrance (Homo, Early)\", \"abnormal heart morphology (Homo, Early)\"),\n",
    "    (\"abnormal skin morphology (Homo, Early)\", \"abnormal kidney morphology (Homo, Early)\"),\n",
    "    (\"abnormal spleen morphology (Homo, Early)\", \"abnormal kidney morphology (Homo, Early)\"),\n",
    "]\n",
    "\n",
    "def jaccard_similarity(phenotype_a: str, phenotype_b: str) -> float:\n",
    "    \"\"\"従来のJaccard similarity（参考用）\"\"\"\n",
    "    if phenotype_a == phenotype_b:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "print(\"\\n修正後の比較: Jaccard vs Extended Resnik Similarity\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for phenotype_a, phenotype_b in comparison_pairs:\n",
    "    jaccard_sim = jaccard_similarity(phenotype_a, phenotype_b)\n",
    "    resnik_sim = extended_resnik.calculate_similarity(phenotype_a, phenotype_b)\n",
    "\n",
    "    print(f\"\\n📍 {phenotype_a}\")\n",
    "    print(f\"   vs\")\n",
    "    print(f\"   {phenotype_b}\")\n",
    "    print(f\"   📊 Jaccard: {jaccard_sim:.4f}  |  Extended Resnik: {resnik_sim:.4f}\")\n",
    "\n",
    "    # 詳細分析\n",
    "    base_a, ann_a = extended_resnik.parse_phenotype_with_annotations(phenotype_a)\n",
    "    base_b, ann_b = extended_resnik.parse_phenotype_with_annotations(phenotype_b)\n",
    "    base_sim = extended_resnik.calculate_base_resnik_similarity(base_a, base_b)\n",
    "    ann_sim = extended_resnik.calculate_annotation_similarity(ann_a, ann_b)\n",
    "    \n",
    "    print(f\"   🔍 詳細:\")\n",
    "    print(f\"      基本表現型: '{base_a}' vs '{base_b}' (類似度: {base_sim:.4f})\")\n",
    "    print(f\"      アノテーション: {ann_a} vs {ann_b} (類似度: {ann_sim:.4f})\")\n",
    "    \n",
    "    # 頻度情報とIC情報も表示\n",
    "    freq_a = phenotype_frequencies.get(phenotype_a, 0)\n",
    "    freq_b = phenotype_frequencies.get(phenotype_b, 0)\n",
    "    ic_a = ic_scores.get(phenotype_a, 0)\n",
    "    ic_b = ic_scores.get(phenotype_b, 0)\n",
    "    print(f\"      頻度: {freq_a} vs {freq_b}\")\n",
    "    print(f\"      IC scores: {ic_a:.3f} vs {ic_b:.3f}\")\n",
    "    \n",
    "    # 改善効果を評価\n",
    "    if jaccard_sim == 0.0 and resnik_sim < 0.1:\n",
    "        print(f\"      ✅ 改善成功: 非関連表現型の類似度が適切に低い ({resnik_sim:.4f})\")\n",
    "    elif jaccard_sim == 0.0 and resnik_sim >= 0.1:\n",
    "        print(f\"      ⚠️  要改善: 非関連表現型の類似度がまだ高い ({resnik_sim:.4f})\")\n",
    "\n",
    "print(f\"\\n🎯 Extended Resnik Similarity の利点:\")\n",
    "print(f\"   1. 頻出表現型の偏りを軽減\")\n",
    "print(f\"   2. オントロジー階層を考慮した意味的類似度\")\n",
    "print(f\"   3. アノテーション情報を適切に考慮\")\n",
    "print(f\"   4. 非関連表現型間の類似度を適切に低く抑制\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. データ保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算結果を保存\n",
    "output_dir = Path(\"data/resnik_similarity\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. MPOパーサーを保存\n",
    "with open(output_dir / \"mpo_parser.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mpo_parser, f)\n",
    "\n",
    "# 2. IC scores保存\n",
    "with open(output_dir / \"ic_scores.json\", \"w\") as f:\n",
    "    json.dump(ic_scores, f, indent=2)\n",
    "\n",
    "# 3. 頻度データ保存\n",
    "with open(output_dir / \"phenotype_frequencies.json\", \"w\") as f:\n",
    "    json.dump(phenotype_frequencies, f, indent=2)\n",
    "\n",
    "# 4. ExtendedResnikSimilarityクラス保存\n",
    "with open(output_dir / \"extended_resnik_similarity.pkl\", \"wb\") as f:\n",
    "    pickle.dump(extended_resnik, f)\n",
    "\n",
    "print(f\"Resnik similarity data saved to {output_dir}/\")\n",
    "print(f\"Files saved:\")\n",
    "for file_path in output_dir.glob(\"*\"):\n",
    "    print(f\"  - {file_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Extended Resnik Similarity implementation completed!\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- Parsed {len(mpo_parser.terms)} MPO terms\")\n",
    "print(f\"- Calculated IC scores for {len(ic_scores)} phenotypes\")\n",
    "print(f\"- Implemented annotation-aware similarity calculation\")\n",
    "print(f\"- Results saved to data/resnik_similarity/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tsumugi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
