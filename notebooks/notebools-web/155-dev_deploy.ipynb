{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run allですべてのデータを準備する\n",
    "\n",
    "* URL: https://ftp.ebi.ac.uk/pub/databases/impc/all-data-releases/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEASE = 23.0\n",
    "\n",
    "columns = [\"marker_symbol\", \"marker_accession_id\", \"mp_term_name\", \"mp_term_id\", \"p_value\", \"effect_size\",\n",
    "           \"female_ko_effect_p_value\", \"male_ko_effect_p_value\", \"female_ko_parameter_estimate\",\"sex_effect_p_value\", \"male_ko_parameter_estimate\", # sex differences\n",
    "           \"genotype_effect_p_value\", \"genotype_effect_parameter_estimate\",\n",
    "           \"zygosity\", # zygosity\n",
    "           \"pipeline_name\", \"procedure_name\", # life-stage\n",
    "           \"allele_symbol\", # map to Phendigm\n",
    "           ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download IMPC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = print\n",
    "from pprint import pprint as PP\n",
    "from collections import Counter as C\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import shutil\n",
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import networkx as nx\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import hashlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up to top directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "while not Path(\"LICENSE\").exists():\n",
    "    os.chdir('../')\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phenodigm dataが存在していない場合には、ダウンロードを促す\n",
    "\n",
    "if not Path(\"data\", \"phenodigm\", \"impc_phenodigm.csv\").exists():\n",
    "    raise FileNotFoundError(\"Please download impc phenodigm data from https://diseasemodels.research.its.qmul.ac.uk/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パスの設定\n",
    "data_dir = Path(\"data/impc\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = data_dir / f\"statistical-results-ALL-{RELEASE}.csv\"\n",
    "\n",
    "# ファイルが存在しない場合にダウンロードして解凍\n",
    "if not csv_path.exists():\n",
    "    # ダウンロード URL\n",
    "    url = f\"https://ftp.ebi.ac.uk/pub/databases/impc/all-data-releases/release-{RELEASE}/results/statistical-results-ALL.csv.gz\"\n",
    "\n",
    "    print(f\"Downloading and extracting: {url}\")\n",
    "\n",
    "    # URL からファイルサイズ取得（tqdmのため）\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        total_size = int(response.info().get(\"Content-Length\", -1))\n",
    "        with tqdm.wrapattr(response, \"read\", total=total_size, desc=\"Downloading\", unit=\"B\", unit_scale=True) as r:\n",
    "            with gzip.GzipFile(fileobj=r) as uncompressed:\n",
    "                with open(csv_path, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(uncompressed, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# wc -l data/impc/statistical-results*.csv\n",
    "# Release 22.1: 3165335\n",
    "# Release 23.0: 2159931\n",
    "\n",
    "# 1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter dataset by P value < 0.0001 (10^-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_statistical_all = Path(\"data\", \"impc\", f\"statistical-results-ALL-{RELEASE}.csv\")\n",
    "df_statistical_all = pd.read_csv(path_statistical_all)\n",
    "df_statistical_all = df_statistical_all[columns]\n",
    "# 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(data))\n",
    "# Release 21.1: 2062772\n",
    "# Release 22.0: 3165334\n",
    "# Release 23.0: 2159930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by p_value < 0.0001\n",
    "threshold = 0.0001\n",
    "filter_pvalue = df_statistical_all[\"p_value\"] < threshold\n",
    "filter_female_ko_pvalue = df_statistical_all[\"female_ko_effect_p_value\"] < threshold\n",
    "filter_male_ko_pvalue = df_statistical_all[\"male_ko_effect_p_value\"] < threshold\n",
    "\n",
    "df_statistical_filtered = df_statistical_all[filter_pvalue | filter_female_ko_pvalue | filter_male_ko_pvalue] #! 要確認！！！\n",
    "\n",
    "# Filter by mp_term_id and mp_term_name are not NaN\n",
    "df_statistical_filtered = df_statistical_filtered.dropna(subset=[\"mp_term_id\"])\n",
    "df_statistical_filtered = df_statistical_filtered.dropna(subset=[\"mp_term_name\"])\n",
    "\n",
    "# Filter by effect_size is not NaN\n",
    "df_statistical_filtered = df_statistical_filtered.dropna(subset=[\"effect_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_statistical_filtered))\n",
    "# Release 22.0: 54059 rows\n",
    "# Release 22.1: 54059 rows\n",
    "# Release 23.0: 49299 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistical_filtered.to_csv(f\"data/statistical_filtered-{RELEASE}.csv\", index=False) # 2 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data by mp_term_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistical_filtered = pd.read_csv(f\"data/statistical_filtered-{RELEASE}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/mp_term_nameを作成\n",
    "\n",
    "output_path = Path(\"data\", \"mp_term_name\")\n",
    "if output_path.exists():\n",
    "    shutil.rmtree(output_path)\n",
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名前をクリーンにする関数を定義\n",
    "def clean_name(name):\n",
    "    return name.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "# mp_term_nameをクリーニングし、ユニークな値を取得\n",
    "unique_mp_term_names = df_statistical_filtered['mp_term_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニークなmp_term_nameごとにフィルタリングしてCSVに保存: 5 sec\n",
    "for mp_term_name in unique_mp_term_names:\n",
    "    df_mp_term = df_statistical_filtered[df_statistical_filtered['mp_term_name'] == mp_term_name]\n",
    "    clean_mp_term_name = clean_name(mp_term_name)\n",
    "    df_mp_term.to_csv(f\"data/mp_term_name/{clean_mp_term_name}.csv\", index=False)\n",
    "# 5 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TSUMUGIに必要なアノテーション情報を整理する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistical_filtered = pd.read_csv(f\"data/statistical_filtered-{RELEASE}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate life stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# life_stageの初期割り当て\n",
    "def assign_life_stage(pipeline_name):\n",
    "    if pd.isna(pipeline_name):\n",
    "        return \"Early\"\n",
    "    if \"Interval\" in pipeline_name or \"interval\" in pipeline_name:\n",
    "        return \"Interval\"\n",
    "    elif \"Late\" in pipeline_name or \"late\" in pipeline_name:\n",
    "        return \"Late\"\n",
    "    else:\n",
    "        return \"Early\"\n",
    "\n",
    "df_statistical_filtered[\"life_stage\"] = df_statistical_filtered[\"pipeline_name\"].apply(assign_life_stage)\n",
    "\n",
    "# Embryo 表現型に該当する procedure_name の一覧\n",
    "embryo_phenotyping = [\n",
    "    \"Gross Morphology Embryo E9.5\",\n",
    "    \"Viability E9.5 Secondary Screen\",\n",
    "    \"OPT E9.5\",\n",
    "    \"MicroCT E9.5\",\n",
    "    \"Gross Morphology Placenta E9.5\",\n",
    "    \"Gross Morphology Embryo E12.5\",\n",
    "    \"Embryo LacZ\",\n",
    "    \"Gross Morphology Placenta E12.5\",\n",
    "    \"Viability E12.5 Secondary Screen\",\n",
    "    \"Viability E14.5-E15.5 Secondary Screen\",\n",
    "    \"Gross Morphology Placenta E14.5-E15.5\",\n",
    "    \"MicroCT E14.5-E15.5\",\n",
    "    \"Gross Morphology Embryo E14.5-E15.5\",\n",
    "    \"Viability E18.5 Secondary Screen\",\n",
    "    \"MicroCT E18.5\",\n",
    "    \"Gross Morphology Embryo E18.5\",\n",
    "    \"Gross Morphology Placenta E18.5\"\n",
    "]\n",
    "\n",
    "# life_stageをEmbryoに上書き\n",
    "df_statistical_filtered.loc[df_statistical_filtered[\"procedure_name\"].isin(embryo_phenotyping), \"life_stage\"] = \"Embryo\"\n",
    "df_annotated = df_statistical_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_annotated))\n",
    "print(df_annotated[\"life_stage\"].value_counts())\n",
    "# 54059\n",
    "# life_stage\n",
    "# Early       45724\n",
    "# Embryo       4253\n",
    "# Late         4024\n",
    "# Interval       58\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Sex differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.0001\n",
    "\n",
    "# 条件リスト\n",
    "conditions = [\n",
    "    (df_annotated[\"sex_effect_p_value\"] < threshold) & (df_annotated[\"female_ko_effect_p_value\"] < threshold) & (df_annotated[\"male_ko_effect_p_value\"] > threshold),\n",
    "    (df_annotated[\"sex_effect_p_value\"] < threshold) & (df_annotated[\"male_ko_effect_p_value\"] < threshold) & (df_annotated[\"female_ko_effect_p_value\"] > threshold)\n",
    "]\n",
    "\n",
    "# 条件に対応する値\n",
    "choices = [\"female\", \"male\"]\n",
    "\n",
    "# np.selectで列を設定\n",
    "df_annotated[\"sexdual_dimorphism\"] = np.select(conditions, choices, default=None)\n",
    "df_annotated = df_annotated.reset_index(drop=True)\n",
    "\n",
    "# 結果を確認\n",
    "print(RELEASE)\n",
    "print(df_annotated[\"sexdual_dimorphism\"].value_counts())\n",
    "\n",
    "# RELEASE 22.1\n",
    "# male      4915\n",
    "# female    4146\n",
    "\n",
    "# RELEASE 23.0\n",
    "# male      5026\n",
    "# female    4344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認\n",
    "df_annotated.dropna(subset=[\"sexdual_dimorphism\"])[[\"p_value\", \"sexdual_dimorphism\", \"effect_size\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遺伝型、性差、ライフステージのアノテーションを統合する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_annotated[\"zygosity\"].value_counts())\n",
    "# RELEASE 22.1\n",
    "# zygosity\n",
    "# homozygote      41444\n",
    "# heterozygote    11921\n",
    "# hemizygote        694\n",
    "\n",
    "# RELEASE 23.0\n",
    "# homozygote      48037\n",
    "# heterozygote    14706\n",
    "# hemizygote        902\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colmuns = [\"marker_symbol\", \"marker_accession_id\", \"mp_term_name\", \"mp_term_id\", \"p_value\", \"female_ko_effect_p_value\", \"male_ko_effect_p_value\", \"sexdual_dimorphism\", \"zygosity\", \"life_stage\", \"effect_size\",]\n",
    "\n",
    "# df_annotated = df_annotated[colmuns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_term = \"increased fasting circulating glucose level\".strip()\n",
    "# df_annotated[\n",
    "#     (df_annotated[\"marker_symbol\"] == \"Dnase1l2\") &\n",
    "#     (df_annotated[\"mp_term_name\"] == mp_term)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アノテーション列を追加（inplace）\n",
    "def make_annotation(row) -> list[str]:\n",
    "    # 遺伝型\n",
    "    if row['zygosity'] == 'homozygote':\n",
    "        annotate = \"Homo\"\n",
    "    elif row['zygosity'] == 'heterozygote':\n",
    "        annotate = \"Hetero\"\n",
    "    else:\n",
    "        annotate = \"Hemi\"\n",
    "\n",
    "    # 性別\n",
    "    if row['sexdual_dimorphism'] == \"female\":\n",
    "        annotate += \", Female\"\n",
    "    elif row['sexdual_dimorphism'] == \"male\":\n",
    "        annotate += \", Male\"\n",
    "\n",
    "    # life stage\n",
    "    if row['life_stage'] in {\"Embryo\", \"Early\", \"Interval\", \"Late\"}:\n",
    "        annotate += f\", {row['life_stage']}\"\n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    annotations.append(f\"{row['mp_term_name']} ({annotate})\")\n",
    "\n",
    "    return annotations\n",
    "\n",
    "df_annotated[\"annotation\"] = df_annotated.apply(make_annotation, axis=1)\n",
    "\n",
    "df_exploded = df_annotated.explode(\"annotation\").reset_index(drop=True)\n",
    "\n",
    "# marker_symbol ごとに annotation をリスト化＆ソート\n",
    "marker_annotation_map = (\n",
    "    df_exploded\n",
    "    .groupby(\"marker_symbol\")[\"annotation\"]\n",
    "    .apply(lambda x: sorted(set(x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：Rhdの注釈を表示\n",
    "print(marker_annotation_map[\"Rhd\"])\n",
    "# 例：Amtの注釈を表示 (Embryo)\n",
    "print(marker_annotation_map[\"Amt\"])\n",
    "# 例：Spag4の注釈を表示 (重複が削除されているか)\n",
    "print(marker_annotation_map[\"Spag4\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data/annotation\").mkdir(exist_ok=True, parents=True)\n",
    "file_path = \"data/annotation/symbol_mptermname.json\"\n",
    "marker_annotation_map.to_json(file_path, indent=4)\n",
    "\n",
    "# json.dump(marker_annotation_map, open(file_path, \"w\"), indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "grep -c \"Male\" data/annotation/symbol_mptermname.json | sed \"s|^|Male: |\"\n",
    "grep -c \"Female\" data/annotation/symbol_mptermname.json | sed \"s|^|Feale: |\"\n",
    "\n",
    "grep -c \"Homo\" data/annotation/symbol_mptermname.json | sed \"s|^|Homo: |\"\n",
    "grep -c \"Hetero\" data/annotation/symbol_mptermname.json | sed \"s|^|Hetero: |\"\n",
    "grep -c \"Hemi\" data/annotation/symbol_mptermname.json | sed \"s|^|Hemi: |\"\n",
    "\n",
    "grep -c \"Embryo\" data/annotation/symbol_mptermname.json | sed \"s|^|Embryo: |\"\n",
    "grep -c \"Early\" data/annotation/symbol_mptermname.json | sed \"s|^|Early: |\"\n",
    "grep -c \"Interval\" data/annotation/symbol_mptermname.json | sed \"s|^|Interval: |\"\n",
    "grep -c \"Late\" data/annotation/symbol_mptermname.json | sed \"s|^|Late: |\"\n",
    "\n",
    "# RELEASE 22.1\n",
    "# Male: 4915\n",
    "# Feale: 4146\n",
    "# Homo: 41444\n",
    "# Hetero: 11921\n",
    "# Hemi: 694\n",
    "# Embryo: 4253\n",
    "# Early: 45724\n",
    "# Interval: 58\n",
    "# Late: 4024\n",
    "\n",
    "# RELEASE 23.0\n",
    "# Male: 4480\n",
    "# Feale: 3557\n",
    "# Homo: 30977\n",
    "# Hetero: 9625\n",
    "# Hemi: 492\n",
    "# Embryo: 4207\n",
    "# Early: 34324\n",
    "# Interval: 54\n",
    "# Late: 2509"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Phenodigmを用いたヒト疾患情報を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phenodigm = pd.read_csv(Path(\"data\", \"phenodigm\", \"impc_phenodigm.csv\"))\n",
    "P(len(df_phenodigm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各行について空白の数をカウント\n",
    "space_counts = df_phenodigm['Mouse model description'].str.count(' ')\n",
    "\n",
    "# 空白の数が2でない行を抽出（== split して3つにならない行）\n",
    "invalid_rows = df_phenodigm[space_counts != 2]\n",
    "\n",
    "# 結果表示\n",
    "print(f\"全体の件数: {len(df_phenodigm)}\")\n",
    "print(f\"空白がちょうど2つでない行数: {len(invalid_rows)}\")\n",
    "print(invalid_rows.head())\n",
    "# -> たった2つしかなく、`Phex<not yet available>`なので、この2つは無視する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phenodigm = df_phenodigm[space_counts == 2]\n",
    "P(len(df_phenodigm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phenodigm[['allele_symbol', 'zygosity', 'life_stage']] = df_phenodigm['Mouse model description'].str.split(' ', n=2, expand=True)\n",
    "df_phenodigm = df_phenodigm.drop(columns=['Mouse model description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P(df_phenodigm.columns)\n",
    "P(df_phenodigm[\"allele_symbol\"].head(3))\n",
    "P(df_phenodigm[\"zygosity\"].head(3))\n",
    "P(df_phenodigm[\"life_stage\"].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phenodigmの表記とimpcデータの表記を揃える\n",
    "\n",
    "df_phenodigm = df_phenodigm.replace({'zygosity': {'hom': 'homozygote', 'het': 'heterozygote','hem': 'hemizygote'}})\n",
    "df_phenodigm['life_stage'] = df_phenodigm['life_stage'].str.capitalize()\n",
    "print(df_phenodigm[\"zygosity\"].value_counts())\n",
    "print(df_phenodigm[\"life_stage\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotated_phenodigm = \\\n",
    "    df_annotated.set_index(['allele_symbol','life_stage','zygosity']) \\\n",
    "    .join(df_phenodigm.set_index(['allele_symbol','life_stage','zygosity']), how='left', rsuffix='_phenodigm') \\\n",
    "    .reset_index()\n",
    "print(len(df_annotated_phenodigm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    \"marker_symbol\", \"Disorder name\", \"life_stage\", \"zygosity\"\n",
    "    ]\n",
    "df_annotated_phenodigm = df_annotated_phenodigm[columns_to_keep].dropna(subset=[\"Disorder name\"]).reset_index(drop=True)\n",
    "df_annotated_phenodigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アノテーション列を追加（inplace）\n",
    "def make_annotation(row) -> list[str]:\n",
    "    # 遺伝型\n",
    "    if row['zygosity'] == 'homozygote':\n",
    "        annotate = \"Homo\"\n",
    "    elif row['zygosity'] == 'heterozygote':\n",
    "        annotate = \"Hetero\"\n",
    "    else:\n",
    "        annotate = \"Hemi\"\n",
    "\n",
    "    # life stage\n",
    "    if row['life_stage'] in {\"Embryo\", \"Early\", \"Interval\", \"Late\"}:\n",
    "        annotate += f\", {row['life_stage']}\"\n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    annotations.append(f\"{row['Disorder name']} ({annotate})\")\n",
    "\n",
    "    return annotations\n",
    "\n",
    "df_annotated_phenodigm[\"annotation\"] = df_annotated_phenodigm.apply(make_annotation, axis=1)\n",
    "\n",
    "df_exploded = df_annotated_phenodigm.explode(\"annotation\").reset_index(drop=True)\n",
    "\n",
    "# marker_symbol ごとに annotation をリスト化＆ソート\n",
    "marker_annotation_map = (\n",
    "    df_exploded\n",
    "    .groupby(\"marker_symbol\")[\"annotation\"]\n",
    "    .apply(lambda x: sorted(set(x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：Phenodigmの注釈を表示 (Embryo)\n",
    "print(marker_annotation_map[\"Arhgap31\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data/annotation\").mkdir(exist_ok=True, parents=True)\n",
    "file_path = \"data/annotation/symbol_disordername.json\"\n",
    "marker_annotation_map.to_json(file_path, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mp term nameとIMPCのPhenotype URLを紐付ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_statistical_filtered[['mp_term_id', 'mp_term_name']].drop_duplicates()\n",
    "# df_select = data[['marker_symbol', 'marker_accession_id', 'mp_term_name', 'mp_term_id']].drop_duplicates()\n",
    "df_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_phenotype_url = dict()\n",
    "for index, row in df_select.iterrows():\n",
    "    mp_tern_id = row['mp_term_id']\n",
    "    impc_url = f\"https://www.mousephenotype.org/data/phenotypes/{mp_tern_id}\"\n",
    "    mp_term_name = row['mp_term_name']\n",
    "    dict_phenotype_url[mp_term_name] = impc_url\n",
    "\n",
    "print(dict_phenotype_url[\"small lymph nodes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotation/mptermname_phenotypeurl.tsv', 'w') as f:\n",
    "    for term, url in dict_phenotype_url.items():\n",
    "        f.write(f\"{term}\\t{url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "head -n 3 data/annotation/mptermname_phenotypeurl.tsv\n",
    "wc -l data/annotation/mptermname_phenotypeurl.tsv\n",
    "# Release 22.0: 664\n",
    "# Release 23.0: 659"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### marker symbolとMGI accession idを紐付ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_statistical_filtered[['marker_symbol', 'marker_accession_id']].drop_duplicates()\n",
    "# df_select = data[['marker_symbol', 'marker_accession_id', 'mp_term_name', 'mp_term_id']].drop_duplicates()\n",
    "df_select\n",
    "# Release 22.1: 7746 rows\n",
    "# Release 23.0: 7934 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_symbol_id = dict()\n",
    "for index, row in df_select.iterrows():\n",
    "    dict_symbol_id[row['marker_symbol']] = row['marker_accession_id']\n",
    "print(dict_symbol_id[\"Ncam1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dict_symbol_id, open(\"data/annotation/symbol_mgiid.json\", \"w\"), indent=4, sort_keys=True)\n",
    "Path(\"data/annotation/symbol_mgiid.tsv\").write_text(\"\\n\".join([f\"{k}\\t{v}\" for k, v in dict_symbol_id.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n 3 data/annotation/symbol_mgiid.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 表現型の類似度を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"data\", \"annotation\", \"symbol_mptermname.json\")\n",
    "\n",
    "symbol_mptermname = json.load(open(file_path))\n",
    "print(symbol_mptermname[\"Dpf2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 空リストを持つ要素を除外しつつ、値を list → set に変換して重複を削除する\n",
    "symbol_mptermname = {k: set(v) for k, v in symbol_mptermname.items() if v}\n",
    "print(symbol_mptermname[\"Dpf2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard係数で集合の類似度を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gene_pair_mp_similarity = []\n",
    "\n",
    "for a, b in combinations(symbol_mptermname, 2):\n",
    "    shared_mp = sorted(symbol_mptermname[a] & symbol_mptermname[b])\n",
    "    shared_mp_number = len(shared_mp)\n",
    "    union_mp_number = len(symbol_mptermname[a] | symbol_mptermname[b])\n",
    "    overlap_ratio = shared_mp_number / union_mp_number\n",
    "\n",
    "    gene_pair_mp_similarity.append([a, b, round(overlap_ratio, 3), shared_mp_number, shared_mp])\n",
    "\n",
    "## 46s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gene_pair_mp_similarity [:3])\n",
    "print(len(gene_pair_mp_similarity))\n",
    "# Release 22.0: 29996385\n",
    "# Release 22.1: 29996385\n",
    "# Release 23.0: 31470211"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重複する表現型が閾値以上のものを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.2\n",
    "num_shared_mp = 3\n",
    "\n",
    "gene_pair_mp_similarity_filtered = []\n",
    "for record in gene_pair_mp_similarity:\n",
    "    if record[2] >= similarity_threshold or record[3] >= num_shared_mp:\n",
    "        gene_pair_mp_similarity_filtered.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gene_pair_mp_similarity_filtered[:3])\n",
    "print(len(gene_pair_mp_similarity_filtered))\n",
    "# Release 21.1: 134880\n",
    "# Release 22.0: 133281 <- Homo/Hetero/Hemiおよび♂・♀の完全一致を考慮するようになったため、減少\n",
    "# Release 22.1: 133281\n",
    "# v0.3.0: 261216 <- Similarity_threshodのor条件をつけたため、増加\n",
    "# Release 23.0 TSUMUGI v0.3.2: 241,645 (thres >= 0.5 or num >= 3)\n",
    "# Release 23.0 TSUMUGI v0.3.2: 866,361 (thres >= 0.2 or num >= 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data\", \"overlap\").mkdir(exist_ok=True, parents=True)\n",
    "pickle.dump(gene_pair_mp_similarity, open(\"data/overlap/gene_pair_mp_similarity.pkl\", \"wb\"))\n",
    "pickle.dump(gene_pair_mp_similarity_filtered, open(\"data/overlap/gene_pair_mp_similarity_filtered.pkl\", \"wb\"))\n",
    "\n",
    "# 18 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生データをCSV形式で出力 （ダウンロード用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity = pd.DataFrame(gene_pair_mp_similarity)\n",
    "df_similarity.columns = [\"Gene1\", \"Gene2\", \"Jaccard Similarity\", \"Number of shared phenotype\", \"List of shared phenotypes\"]\n",
    "df_similarity.reindex(\n",
    "    columns=[\"Gene1\", \"Gene2\", \"Number of shared phenotype\", \"Jaccard Similarity\", \"List of shared phenotypes\"]\n",
    ")\n",
    "df_similarity[\"List of shared phenotypes\"] = df_similarity[\"List of shared phenotypes\"].apply(json.dumps)\n",
    "df_similarity\n",
    "# 30 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"data/TSUMUGI_raw_data.csv.gz\")\n",
    "\n",
    "def get_head1000_hash(df: pd.DataFrame) -> str:\n",
    "    # head(1000)だけを対象にハッシュ化\n",
    "    csv_bytes = df.head(1000).to_csv(index=False, lineterminator='\\n').encode('utf-8')\n",
    "    return hashlib.md5(csv_bytes).hexdigest()\n",
    "\n",
    "def file_head1000_hash(path: Path) -> str | None:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        lines = [next(f) for _ in range(1001)]  # 1行目がヘッダー\n",
    "        csv_content = ''.join(lines).encode('utf-8')\n",
    "        return hashlib.md5(csv_content).hexdigest()\n",
    "\n",
    "# 比較\n",
    "new_hash = get_head1000_hash(df_similarity)\n",
    "existing_hash = file_head1000_hash(output_path)\n",
    "\n",
    "if new_hash != existing_hash:\n",
    "    df_similarity.to_csv(output_path, index=False, compression=\"gzip\", lineterminator='\\n')\n",
    "    print(\"🔄 ファイルを更新しました\") # 3 min\n",
    "else:\n",
    "    print(\"✅ 内容に変更がないためスキップしました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_overlap_filtered = pd.DataFrame(shared_ratios_filtered)\n",
    "# df_overlap_filtered.columns = [\"Gene1\", \"Gene2\", \"Jaccard Similarity\", \"Number of shared phenotype\", \"List of shared phenotypes\"]\n",
    "# df_overlap_filtered.reindex(\n",
    "#     columns=[\"Gene1\", \"Gene2\", \"Number of shared phenotype\", \"Jaccard Similarity\", \"List of shared phenotypes\"]\n",
    "# )\n",
    "# df_overlap_filtered[\"List of shared phenotypes\"] = df_overlap_filtered[\"List of shared phenotypes\"].apply(json.dumps)\n",
    "\n",
    "# df_overlap_filtered.to_csv(\"data/TSUMUGI_filtered_data.csv.gz\", index=False, compression=\"gzip\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 表現型ごとのネットワークを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_pair_mp_similarity_filtered = pickle.load(open(\"data/overlap/gene_pair_mp_similarity_filtered.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity = pd.DataFrame(\n",
    "    gene_pair_mp_similarity_filtered, columns=[\"marker1\", \"marker2\", \"phenotype_similarity\", \"shared_mp_number\", \"shared_mp\"]\n",
    ")\n",
    "df_similarity\n",
    "# version 0.2.2: 133281  rows × 5 columns\n",
    "# version 0.3.0: 261216  rows × 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_mp = json.load(open(\"data/annotation/symbol_mptermname.json\"))\n",
    "marker_mp = pd.DataFrame(marker_mp.items(), columns=[\"marker_symbol\", \"mp_term_name\"])\n",
    "marker_mp\n",
    "# version 0.2.2: 7626 rows × 2 columns\n",
    "# version 0.3.0: 7746 rows × 2 columns\n",
    "# version 0.3.1: 7746 rows × 2 columns\n",
    "# RELEASE 23.0: 7954 rows × 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_disease = json.load(open(\"data/annotation/symbol_disordername.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"data/network/mp_term_name\")\n",
    "# remove network directory\n",
    "if output_dir.exists():\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mp_terms = list(Path(\"data\", \"mp_term_name\").glob(\"*.csv\"))\n",
    "# print(path_mp_terms[:3])\n",
    "# print(len(path_mp_terms))\n",
    "# path_mp_term = Path(\"data\", \"mp_term_name\", \"increasing_circulating_glucose_level.csv\")\n",
    "\n",
    "\"\"\"\n",
    "ノードが多すぎるとWebページが描画できない問題を回避するため、\n",
    "ノード数を200以下にするために最適なphenotype_similarityを求める\n",
    "\"\"\"\n",
    "number_of_nodes = 200\n",
    "tolerance = 25\n",
    "\n",
    "for path_mp_term in path_mp_terms:\n",
    "\n",
    "    df_marker_effect = pd.read_csv(path_mp_term).dropna(subset=[\"effect_size\"])\n",
    "\n",
    "    # Absolute value of effect size\n",
    "    df_marker_effect[\"effect_size\"] = df_marker_effect[\"effect_size\"].abs()\n",
    "\n",
    "    # * effect sizeの絶対値が最大の行を取得 (Homo/Heteroで異なる効果量がある場合に、ひとまず最大値を採用する← 今後の考慮事項)\n",
    "    df_marker_effect = (\n",
    "        df_marker_effect[[\"marker_symbol\", \"effect_size\"]]  # 必要な列だけ抽出\n",
    "        .loc[df_marker_effect.groupby(\"marker_symbol\")[\"effect_size\"].idxmax()]  # 各 marker_symbol ごとの effect_size 最大値の行を取得\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    mp_term = path_mp_term.stem\n",
    "    target_mp = mp_term.replace(\"_\", \" \")\n",
    "    valid_markers = df_marker_effect['marker_symbol']\n",
    "\n",
    "    df_filtered = df_similarity[\n",
    "        df_similarity['marker1'].isin(valid_markers) &\n",
    "        df_similarity['marker2'].isin(valid_markers) &\n",
    "        df_similarity['shared_mp'].apply(lambda lst: any(target_mp in term for term in lst))\n",
    "    ]\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Nodeを作成する\n",
    "    # ----------------------------------------------------\n",
    "    df_marker1 = df_filtered[[\"marker1\"]]\n",
    "    df_marker2 = df_filtered[[\"marker2\"]]\n",
    "    df_node_marker1 = pd.merge(df_marker1, df_marker_effect, left_on='marker1', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "    df_node_marker2 = pd.merge(df_marker2, df_marker_effect, left_on='marker2', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "\n",
    "    df_node = pd.concat([df_node_marker1, df_node_marker2], axis=0).drop_duplicates()\n",
    "    df_node = pd.merge(df_node, marker_mp, how='inner', on='marker_symbol')\n",
    "    df_node = pd.merge(df_node, df_marker_effect, how='inner', on='marker_symbol')\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # ノード数を200以下にするために最適なphenotype_similarityを求める\n",
    "    # ----------------------------------------------------\n",
    "    if len(df_node) > number_of_nodes:\n",
    "\n",
    "        low, high = df_filtered[\"phenotype_similarity\"].min(), df_filtered[\"phenotype_similarity\"].max()\n",
    "\n",
    "        best_phenotype_similarity = 0\n",
    "        closest_diff = float('inf')  # 最も近いノード数の差を記録\n",
    "\n",
    "        while low <= high:\n",
    "            mid = (low + high) / 2\n",
    "\n",
    "            df_mid = df_filtered[df_filtered[\"phenotype_similarity\"] >= mid]\n",
    "\n",
    "            # Nodeを作成する\n",
    "            df_marker1 = df_mid[[\"marker1\"]]\n",
    "            df_marker2 = df_mid[[\"marker2\"]]\n",
    "            df_node_marker1 = pd.merge(df_marker1, df_marker_effect, left_on='marker1', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "            df_node_marker2 = pd.merge(df_marker2, df_marker_effect, left_on='marker2', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "            df_node = pd.concat([df_node_marker1, df_node_marker2], axis=0).drop_duplicates()\n",
    "            df_node = pd.merge(df_node, marker_mp, how='inner', on='marker_symbol')\n",
    "            df_node = pd.merge(df_node, df_marker_effect, how='inner', on='marker_symbol')\n",
    "\n",
    "            node_count = len(df_node)\n",
    "\n",
    "            # ターゲットノード数に近い場合、best_phenotype_similarityを更新\n",
    "            current_diff = abs(node_count - number_of_nodes)\n",
    "            if current_diff < closest_diff:\n",
    "                best_phenotype_similarity = mid\n",
    "                closest_diff = current_diff \n",
    "\n",
    "            if number_of_nodes - tolerance < node_count < number_of_nodes + tolerance:\n",
    "                break\n",
    "            elif node_count > number_of_nodes:\n",
    "                # ノード数が多い場合、範囲を上げる\n",
    "                low = mid + 1e-6\n",
    "            else:\n",
    "                # ノード数が少ない場合、範囲を下げる\n",
    "                high = mid - 1e-6\n",
    "\n",
    "        df_filtered = df_filtered[df_filtered[\"phenotype_similarity\"] >= best_phenotype_similarity]\n",
    "\n",
    "    # NodeをJSON形式に変換\n",
    "    node_json = []\n",
    "    for _, row in df_node.iterrows():\n",
    "        node_json.append({\n",
    "            \"data\": {\n",
    "                \"id\": row['marker_symbol'],\n",
    "                \"label\": row['marker_symbol'],\n",
    "                \"phenotype\": row['mp_term_name'],\n",
    "                \"disease\": marker_disease[row['marker_symbol']] if row['marker_symbol'] in marker_disease else \"\",\n",
    "                \"node_color\": row['effect_size']\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Edgeを作成する\n",
    "    # ----------------------------------------------------\n",
    "    df_edge = df_filtered[[\"marker1\", \"marker2\", \"phenotype_similarity\", \"shared_mp\"]]\n",
    "    # EdgeをJSON形式に変換\n",
    "    edge_json = []\n",
    "    for _, row in df_edge.iterrows():\n",
    "        edge_json.append({\n",
    "            \"data\": {\n",
    "                \"source\": row['marker1'],\n",
    "                \"target\": row['marker2'],\n",
    "                \"phenotype\": row['shared_mp'],\n",
    "                \"edge_size\": row['phenotype_similarity']\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # EdgeとNodeを統合して、出力\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    network_json = node_json + edge_json\n",
    "\n",
    "    # Output as JSON\n",
    "    if network_json:\n",
    "        output_json = output_dir / f\"{mp_term}.json.gz\"\n",
    "        with gzip.open(output_json, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(network_json, f, indent=4)\n",
    "\n",
    "# 1m30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -lhS data/network/mp_term_name/ | head -n 5\n",
    "\n",
    "# version 0.2.2: total 5.3M\n",
    "# version 0.3.0: total 5.5M\n",
    "# version 0.3.1: total 5.1M <- 該当の表現型を含むネットワークのみを表示 （Issue: #54）\n",
    "# version 0.3.2: total 5.1M <- Similarity 0.5 or 3 phenotypes\n",
    "# version 0.3.3: total 9.8M <- Similarity 0.2 or 3 phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 200前後のノード数となっているか確認\n",
    "\n",
    "zcat data/network/mp_term_name/preweaning_lethality,_complete_penetrance.json.gz | grep -c \"node_color\"\n",
    "# preweaning_lethality,_complete_penetranceは似ているノードが多すぎて、200を超えてしまう\n",
    "\n",
    "zcat data/network/mp_term_name/edema.json.gz | grep -c \"node_color\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path_mp_term = Path(\"data/mp_term_name/preweaning_lethality,_complete_penetrance.csv\")\n",
    "\n",
    "# number_of_nodes = 200\n",
    "# tolerance = 25\n",
    "# df_marker_effect = pd.read_csv(path_mp_term).dropna(subset=[\"effect_size\"])\n",
    "\n",
    "# # Absolute value of effect size\n",
    "# df_marker_effect[\"effect_size\"] = df_marker_effect[\"effect_size\"].abs()\n",
    "\n",
    "# # * effect sizeの絶対値が最大の行を取得 (Homo/Heteroで異なる効果量がある場合に、ひとまず最大値を採用する← 今後の考慮事項)\n",
    "# df_marker_effect = (\n",
    "#     df_marker_effect[[\"marker_symbol\", \"effect_size\"]]  # 必要な列だけ抽出\n",
    "#     .loc[df_marker_effect.groupby(\"marker_symbol\")[\"effect_size\"].idxmax()]  # 各 marker_symbol ごとの effect_size 最大値の行を取得\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# mp_term = path_mp_term.stem\n",
    "# target_mp = mp_term.replace(\"_\", \" \")\n",
    "# valid_markers = df_marker_effect['marker_symbol']\n",
    "\n",
    "# df_filtered = df_similarity[\n",
    "#     df_similarity['marker1'].isin(valid_markers) &\n",
    "#     df_similarity['marker2'].isin(valid_markers) &\n",
    "#     df_similarity['shared_mp'].apply(lambda lst: any(target_mp in term for term in lst))\n",
    "# ]\n",
    "\n",
    "# # ----------------------------------------------------\n",
    "# # Nodeを作成する\n",
    "# # ----------------------------------------------------\n",
    "# df_marker1 = df_filtered[[\"marker1\"]]\n",
    "# df_marker2 = df_filtered[[\"marker2\"]]\n",
    "# df_node_marker1 = pd.merge(df_marker1, df_marker_effect, left_on='marker1', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "# df_node_marker2 = pd.merge(df_marker2, df_marker_effect, left_on='marker2', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "\n",
    "# df_node = pd.concat([df_node_marker1, df_node_marker2], axis=0).drop_duplicates()\n",
    "# df_node = pd.merge(df_node, marker_mp, how='inner', on='marker_symbol')\n",
    "# df_node = pd.merge(df_node, df_marker_effect, how='inner', on='marker_symbol')\n",
    "\n",
    "# best_phenotype_similarity = None\n",
    "# best_under_node_count = -1\n",
    "# best_under_mid = None\n",
    "\n",
    "# print(len(df_node))\n",
    "# # ----------------------------------------------------\n",
    "# # ノード数を200以下にするために最適なphenotype_similarityを求める\n",
    "# # ----------------------------------------------------\n",
    "# if len(df_node) > number_of_nodes:\n",
    "\n",
    "#     low, high = df_filtered[\"phenotype_similarity\"].min(), df_filtered[\"phenotype_similarity\"].max()\n",
    "#     print(low, high)\n",
    "#     while low <= high:\n",
    "#         mid = (low + high) / 2\n",
    "\n",
    "#         df_mid = df_filtered[df_filtered[\"phenotype_similarity\"] >= mid]\n",
    "\n",
    "#         # Nodeを作成する\n",
    "#         df_marker1 = df_mid[[\"marker1\"]]\n",
    "#         df_marker2 = df_mid[[\"marker2\"]]\n",
    "#         df_node_marker1 = pd.merge(df_marker1, df_marker_effect, left_on='marker1', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "#         df_node_marker2 = pd.merge(df_marker2, df_marker_effect, left_on='marker2', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "#         df_node = pd.concat([df_node_marker1, df_node_marker2], axis=0).drop_duplicates()\n",
    "#         df_node = pd.merge(df_node, marker_mp, how='inner', on='marker_symbol')\n",
    "#         df_node = pd.merge(df_node, df_marker_effect, how='inner', on='marker_symbol')\n",
    "\n",
    "#         node_count = len(df_node)\n",
    "\n",
    "#         if number_of_nodes - tolerance < node_count < number_of_nodes + tolerance:\n",
    "#             best_phenotype_similarity = mid\n",
    "#             break\n",
    "#         elif node_count > number_of_nodes:\n",
    "#             # ノード数が多い場合、範囲を上げる\n",
    "#             low = mid + 1e-6\n",
    "#         else:\n",
    "#             # ノード数が少ない場合、範囲を下げる\n",
    "#             best_under_node_count = node_count\n",
    "#             best_under_mid = mid\n",
    "#             high = mid - 1e-6\n",
    "\n",
    "#     if best_phenotype_similarity is None:\n",
    "#         best_phenotype_similarity = best_under_mid\n",
    "\n",
    "#     df_optimized = df_filtered[df_filtered[\"phenotype_similarity\"] >= best_phenotype_similarity]\n",
    "#     G = nx.from_pandas_edgelist(df_optimized, \"marker1\", \"marker2\")\n",
    "#     print(low, high)\n",
    "#     print(len(G.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遺伝子ごとのネットワークを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_mp_dict = dict(zip(marker_mp.marker_symbol, marker_mp.mp_term_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_symbols = df_similarity.marker1.unique().tolist()\n",
    "gene_symbols += df_similarity.marker2.unique().tolist()\n",
    "gene_symbols = list(set(gene_symbols))\n",
    "gene_symbols.sort()  # 以下のfor文で、どこまで遺伝子が処理されたのか途中経過を見積もるためのソート\n",
    "P(gene_symbols[:3])\n",
    "P(len(gene_symbols))  # 6003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"data\", \"network\", \"gene_symbol\")\n",
    "# remove network directory\n",
    "if output_dir.exists():\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_nodes = 200\n",
    "tolerance = 25  # tolerance for the number of nodes\n",
    "\n",
    "for i, gene_symbol in enumerate(gene_symbols):\n",
    "    \"\"\"\n",
    "    ノードが多すぎるとWebページが描画できない問題を回避するため、\n",
    "    ノード数を200以下にするために最適なphenotype_similarityを求める\n",
    "    \"\"\"\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing {int(i+1)}/{len(gene_symbols)}: {gene_symbol}\")\n",
    "\n",
    "    df_filtered = df_similarity[(df_similarity[\"marker1\"] == gene_symbol) | (df_similarity[\"marker2\"] == gene_symbol)]\n",
    "    G = nx.from_pandas_edgelist(df_filtered, \"marker1\", \"marker2\")\n",
    "\n",
    "    # ノードAと直接つながっているノードのみを取得\n",
    "    neighbors = list(G.neighbors(gene_symbol))\n",
    "    subgraph_nodes = [gene_symbol] + neighbors\n",
    "    subgraph = G.subgraph(subgraph_nodes)\n",
    "\n",
    "    if len(subgraph.nodes) > number_of_nodes:\n",
    "\n",
    "        low, high = df_filtered[\"phenotype_similarity\"].min(), df_filtered[\"phenotype_similarity\"].max()\n",
    "\n",
    "        best_phenotype_similarity = 0\n",
    "        closest_diff = float('inf')  # 最も近いノード数の差を記録\n",
    "\n",
    "        while low <= high:\n",
    "            mid = (low + high) / 2\n",
    "\n",
    "            # phenotype_similarity >= mid のデータをフィルタリング\n",
    "            df_mid = df_filtered[df_filtered[\"phenotype_similarity\"] >= mid]\n",
    "\n",
    "            G = nx.from_pandas_edgelist(df_mid, \"marker1\", \"marker2\")\n",
    "            # ノードAと直接つながっているノードのみを取得\n",
    "            if gene_symbol in G:\n",
    "                neighbors = list(G.neighbors(gene_symbol))\n",
    "            else:\n",
    "            # mid が大きすぎてノードが除外されすぎた（＝subgraph が小さくなりすぎた）と考えて、探索範囲の上限 (high) を下げる\n",
    "                high = mid - 1e-6\n",
    "                continue\n",
    "\n",
    "            subgraph_nodes = [gene_symbol] + neighbors\n",
    "            subgraph = G.subgraph(subgraph_nodes)\n",
    "\n",
    "            node_count = len(subgraph.nodes)\n",
    "\n",
    "            # ターゲットノード数に近い場合、best_phenotype_similarityを更新\n",
    "            current_diff = abs(node_count - number_of_nodes)\n",
    "            if current_diff < closest_diff:\n",
    "                best_phenotype_similarity = mid\n",
    "                closest_diff = current_diff \n",
    "\n",
    "            if number_of_nodes - tolerance < node_count < number_of_nodes + tolerance:\n",
    "                break\n",
    "            elif node_count > number_of_nodes:\n",
    "                # ノード数が多い場合、範囲を上げる\n",
    "                low = mid + 1e-6\n",
    "            else:\n",
    "                # ノード数が少ない場合、範囲を下げる\n",
    "                high = mid - 1e-6\n",
    "\n",
    "        # 最適なphenotype_similarityでフィルタリング\n",
    "        df_optimized = df_filtered[df_filtered[\"phenotype_similarity\"] >= best_phenotype_similarity]\n",
    "        G = nx.from_pandas_edgelist(df_optimized, \"marker1\", \"marker2\")\n",
    "\n",
    "        # ノードAと直接つながっているノードのみを取得\n",
    "        neighbors = list(G.neighbors(gene_symbol))\n",
    "        subgraph_nodes = [gene_symbol] + neighbors\n",
    "        subgraph = G.subgraph(subgraph_nodes)\n",
    "        \n",
    "    # nodesを用意\n",
    "    node_json = []\n",
    "    for node in subgraph.nodes():\n",
    "        annotation = marker_mp_dict[node]\n",
    "        node_color = 1 if node == gene_symbol else 0\n",
    "        node_json.append({\n",
    "            \"data\": {\n",
    "                \"id\": node,\n",
    "                \"label\": node,\n",
    "                \"node_color\": node_color,\n",
    "                \"phenotype\": annotation,\n",
    "                \"disease\": marker_disease[node] if node in marker_disease else \"\",\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # edgesを用意\n",
    "    df_edge = df_similarity[\n",
    "        (df_similarity[\"marker1\"].isin(subgraph.nodes())) & (df_similarity[\"marker2\"].isin(subgraph.nodes()))\n",
    "    ]\n",
    "\n",
    "    edge_json = []\n",
    "    for edge in df_edge.itertuples():\n",
    "        edge_json.append(\n",
    "            {\n",
    "                \"data\": {\n",
    "                    \"source\": edge.marker1,\n",
    "                    \"target\": edge.marker2,\n",
    "                    \"edge_size\": edge.phenotype_similarity,\n",
    "                    \"phenotype\": edge.shared_mp,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    network_json = node_json + edge_json\n",
    "\n",
    "    # Output as JSON\n",
    "    if network_json:\n",
    "        output_json = output_dir / f\"{gene_symbol}.json.gz\"\n",
    "        with gzip.open(output_json, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(network_json, f, indent=4)\n",
    "\n",
    "# 10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -lhS data/network/gene_symbol/ | head -n 5\n",
    "# version 0.3.0: total 170M\n",
    "# version 0.3.1: total 168M\n",
    "# version 0.3.2: total 145M\n",
    "\n",
    "# total 287M\n",
    "# -rwxrwxrwx 1 kuno kuno 578K Jun 11 05:35 Ttbk2.json.gz\n",
    "# -rwxrwxrwx 1 kuno kuno 558K Jun 11 05:22 Furin.json.gz\n",
    "# -rwxrwxrwx 1 kuno kuno 558K Jun 11 05:35 Trpm7.json.gz\n",
    "# -rwxrwxrwx 1 kuno kuno 555K Jun 11 05:29 Plpp3.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 200前後のノード数となっているか確認\n",
    "zcat data/network/gene_symbol/Crls1.json.gz | grep -c \"node_color\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! DEBUG TEST\n",
    "# number_of_nodes = 200\n",
    "# tolerance = 25  # tolerance for the number of nodes\n",
    "# gene_symbol = \"Map2k1\"\n",
    "# print(gene_symbol)\n",
    "# # 今の処理\n",
    "# df_filtered = df_similarity[(df_similarity[\"marker1\"] == gene_symbol) | (df_similarity[\"marker2\"] == gene_symbol)]\n",
    "\n",
    "# G = nx.from_pandas_edgelist(df_filtered, \"marker1\", \"marker2\")\n",
    "\n",
    "# # ノードAと直接つながっているノードのみを取得\n",
    "# neighbors = list(G.neighbors(gene_symbol))\n",
    "# subgraph_nodes = [gene_symbol] + neighbors\n",
    "# subgraph = G.subgraph(subgraph_nodes)\n",
    "\n",
    "# P(len(subgraph.nodes))  # 200\n",
    "\n",
    "# if len(subgraph.nodes) > number_of_nodes:\n",
    "#     # 二分探索の範囲\n",
    "#     low, high = df_filtered[\"phenotype_similarity\"].min(), df_filtered[\"phenotype_similarity\"].max()\n",
    "\n",
    "#     # 最適なものが見つからなかった場合に備えて保持\n",
    "#     best_under_node_count = -1\n",
    "#     best_under_mid = 0\n",
    "#     best_phenotype_similarity = 0\n",
    "#     closest_diff = float('inf')  # 最も近いノード数の差を記録\n",
    "\n",
    "#     while low <= high:\n",
    "#         mid = (low + high) / 2\n",
    "\n",
    "#         # phenotype_similarity >= mid のデータをフィルタリング\n",
    "#         df_mid = df_filtered[df_filtered[\"phenotype_similarity\"] >= mid]\n",
    "\n",
    "#         G = nx.from_pandas_edgelist(df_mid, \"marker1\", \"marker2\")\n",
    "#         # ノードAと直接つながっているノードのみを取得\n",
    "#         if gene_symbol in G:\n",
    "#             neighbors = list(G.neighbors(gene_symbol))\n",
    "#         else:\n",
    "#         # mid が大きすぎてノードが除外されすぎた（＝subgraph が小さくなりすぎた）と考えて、探索範囲の上限 (high) を下げる\n",
    "#             high = mid - 1e-6\n",
    "#             continue\n",
    "\n",
    "#         subgraph_nodes = [gene_symbol] + neighbors\n",
    "#         subgraph = G.subgraph(subgraph_nodes)\n",
    "\n",
    "#         node_count = len(subgraph.nodes)\n",
    "\n",
    "#         # ターゲットノード数に近い場合、best_phenotype_similarityを更新\n",
    "#         current_diff = abs(node_count - number_of_nodes)\n",
    "#         if current_diff < closest_diff:\n",
    "#             best_phenotype_similarity = mid\n",
    "#             closest_diff = current_diff \n",
    "\n",
    "#         if number_of_nodes - tolerance < node_count < number_of_nodes + tolerance:\n",
    "#             break\n",
    "#         elif node_count > number_of_nodes:\n",
    "#             # ノード数が多い場合、範囲を上げる\n",
    "#             low = mid + 1e-6\n",
    "#         else:\n",
    "#             # ノード数が少ない場合、候補として記録\n",
    "#             if node_count > best_under_node_count:\n",
    "#                 best_under_node_count = node_count\n",
    "#                 best_under_mid = mid\n",
    "#             # ノード数が少ない場合、範囲を下げる\n",
    "#             high = mid - 1e-6\n",
    "\n",
    "#     # 最適なphenotype_similarityでフィルタリング\n",
    "#     df_optimized = df_filtered[df_filtered[\"phenotype_similarity\"] >= best_phenotype_similarity]\n",
    "#     G = nx.from_pandas_edgelist(df_optimized, \"marker1\", \"marker2\")\n",
    "\n",
    "#     # ノードAと直接つながっているノードのみを取得\n",
    "#     neighbors = list(G.neighbors(gene_symbol))\n",
    "#     subgraph_nodes = [gene_symbol] + neighbors\n",
    "#     subgraph = G.subgraph(subgraph_nodes)\n",
    "    \n",
    "# print(node_count, low, high, mid, best_phenotype_similarity, best_under_mid)\n",
    "# P(len(subgraph.nodes))  # 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[\"phenotype_similarity\"].plot(kind=\"hist\", bins=100, title=f\"Phenotype Similarity Distribution for {gene_symbol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data/overlap/available_gene_symbols.txt\").write_text(\"\\n\".join(gene_symbols) + \"\\n\")\n",
    "print(len(gene_symbols))  # 4416 -> 4244 → 6003 → 4139\n",
    "# version 0.2.2: 4139\n",
    "# version 0.3.0: 6812 (Life stageを考慮 + 類似度を追加)\n",
    "# version 0.3.1: 6812\n",
    "# version 0.3.2: 6904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "uname -a # OS name\n",
    "date +\"%Y/%m/%d %H:%M:%S\" # Last update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tsumugi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
