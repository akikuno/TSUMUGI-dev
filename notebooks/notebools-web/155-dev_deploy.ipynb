{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run allã§ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹\n",
    "\n",
    "* URL: https://ftp.ebi.ac.uk/pub/databases/impc/all-data-releases/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEASE = 23.0\n",
    "\n",
    "columns = [\"marker_symbol\", \"marker_accession_id\", \"mp_term_name\", \"mp_term_id\", \"p_value\", \"effect_size\",\n",
    "           \"female_ko_effect_p_value\", \"male_ko_effect_p_value\", \"female_ko_parameter_estimate\",\"sex_effect_p_value\", \"male_ko_parameter_estimate\", # sex differences\n",
    "           \"genotype_effect_p_value\", \"genotype_effect_parameter_estimate\",\n",
    "           \"zygosity\", # zygosity\n",
    "           \"pipeline_name\", \"procedure_name\", # life-stage\n",
    "           \"allele_symbol\", # map to Phendigm\n",
    "           ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download IMPC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = print\n",
    "from pprint import pprint as PP\n",
    "from collections import Counter as C\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import shutil\n",
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import networkx as nx\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import hashlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up to top directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "while not Path(\"LICENSE\").exists():\n",
    "    os.chdir('../')\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phenodigm dataãŒå­˜åœ¨ã—ã¦ã„ãªã„å ´åˆã«ã¯ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ä¿ƒã™\n",
    "\n",
    "if not Path(\"data\", \"phenodigm\", \"impc_phenodigm.csv\").exists():\n",
    "    raise FileNotFoundError(\"Please download impc phenodigm data from https://diseasemodels.research.its.qmul.ac.uk/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‘ã‚¹ã®è¨­å®š\n",
    "data_dir = Path(\"data/impc\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = data_dir / f\"statistical-results-ALL-{RELEASE}.csv\"\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£å‡\n",
    "if not csv_path.exists():\n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ URL\n",
    "    url = f\"https://ftp.ebi.ac.uk/pub/databases/impc/all-data-releases/release-{RELEASE}/results/statistical-results-ALL.csv.gz\"\n",
    "\n",
    "    print(f\"Downloading and extracting: {url}\")\n",
    "\n",
    "    # URL ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºå–å¾—ï¼ˆtqdmã®ãŸã‚ï¼‰\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        total_size = int(response.info().get(\"Content-Length\", -1))\n",
    "        with tqdm.wrapattr(response, \"read\", total=total_size, desc=\"Downloading\", unit=\"B\", unit_scale=True) as r:\n",
    "            with gzip.GzipFile(fileobj=r) as uncompressed:\n",
    "                with open(csv_path, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(uncompressed, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# wc -l data/impc/statistical-results*.csv\n",
    "# Release 22.1: 3165335\n",
    "# Release 23.0: 2159931\n",
    "\n",
    "# 1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter dataset by P value < 0.0001 (10^-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_statistical_all = Path(\"data\", \"impc\", f\"statistical-results-ALL-{RELEASE}.csv\")\n",
    "df_statistical_all = pd.read_csv(path_statistical_all)\n",
    "df_statistical_all = df_statistical_all[columns]\n",
    "# 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(data))\n",
    "# Release 21.1: 2062772\n",
    "# Release 22.0: 3165334\n",
    "# Release 23.0: 2159930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by p_value < 0.0001\n",
    "threshold = 0.0001\n",
    "filter_pvalue = df_statistical_all[\"p_value\"] < threshold\n",
    "filter_female_ko_pvalue = df_statistical_all[\"female_ko_effect_p_value\"] < threshold\n",
    "filter_male_ko_pvalue = df_statistical_all[\"male_ko_effect_p_value\"] < threshold\n",
    "\n",
    "df_statistical_filtered = df_statistical_all[filter_pvalue | filter_female_ko_pvalue | filter_male_ko_pvalue] #! è¦ç¢ºèªï¼ï¼ï¼\n",
    "\n",
    "# Filter by mp_term_id and mp_term_name are not NaN\n",
    "df_statistical_filtered = df_statistical_filtered.dropna(subset=[\"mp_term_id\"])\n",
    "df_statistical_filtered = df_statistical_filtered.dropna(subset=[\"mp_term_name\"])\n",
    "\n",
    "# Filter by effect_size is not NaN\n",
    "df_statistical_filtered = df_statistical_filtered.dropna(subset=[\"effect_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_statistical_filtered))\n",
    "# Release 22.0: 54059 rows\n",
    "# Release 22.1: 54059 rows\n",
    "# Release 23.0: 49299 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistical_filtered.to_csv(f\"data/statistical_filtered-{RELEASE}.csv\", index=False) # 2 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data by mp_term_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistical_filtered = pd.read_csv(f\"data/statistical_filtered-{RELEASE}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/mp_term_nameã‚’ä½œæˆ\n",
    "\n",
    "output_path = Path(\"data\", \"mp_term_name\")\n",
    "if output_path.exists():\n",
    "    shutil.rmtree(output_path)\n",
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åå‰ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã«ã™ã‚‹é–¢æ•°ã‚’å®šç¾©\n",
    "def clean_name(name):\n",
    "    return name.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "# mp_term_nameã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã‚’å–å¾—\n",
    "unique_mp_term_names = df_statistical_filtered['mp_term_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªmp_term_nameã”ã¨ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦CSVã«ä¿å­˜: 5 sec\n",
    "for mp_term_name in unique_mp_term_names:\n",
    "    df_mp_term = df_statistical_filtered[df_statistical_filtered['mp_term_name'] == mp_term_name]\n",
    "    clean_mp_term_name = clean_name(mp_term_name)\n",
    "    df_mp_term.to_csv(f\"data/mp_term_name/{clean_mp_term_name}.csv\", index=False)\n",
    "# 5 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TSUMUGIã«å¿…è¦ãªã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æ•´ç†ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistical_filtered = pd.read_csv(f\"data/statistical_filtered-{RELEASE}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate life stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# life_stageã®åˆæœŸå‰²ã‚Šå½“ã¦\n",
    "def assign_life_stage(pipeline_name):\n",
    "    if pd.isna(pipeline_name):\n",
    "        return \"Early\"\n",
    "    if \"Interval\" in pipeline_name or \"interval\" in pipeline_name:\n",
    "        return \"Interval\"\n",
    "    elif \"Late\" in pipeline_name or \"late\" in pipeline_name:\n",
    "        return \"Late\"\n",
    "    else:\n",
    "        return \"Early\"\n",
    "\n",
    "df_statistical_filtered[\"life_stage\"] = df_statistical_filtered[\"pipeline_name\"].apply(assign_life_stage)\n",
    "\n",
    "# Embryo è¡¨ç¾å‹ã«è©²å½“ã™ã‚‹ procedure_name ã®ä¸€è¦§\n",
    "embryo_phenotyping = [\n",
    "    \"Gross Morphology Embryo E9.5\",\n",
    "    \"Viability E9.5 Secondary Screen\",\n",
    "    \"OPT E9.5\",\n",
    "    \"MicroCT E9.5\",\n",
    "    \"Gross Morphology Placenta E9.5\",\n",
    "    \"Gross Morphology Embryo E12.5\",\n",
    "    \"Embryo LacZ\",\n",
    "    \"Gross Morphology Placenta E12.5\",\n",
    "    \"Viability E12.5 Secondary Screen\",\n",
    "    \"Viability E14.5-E15.5 Secondary Screen\",\n",
    "    \"Gross Morphology Placenta E14.5-E15.5\",\n",
    "    \"MicroCT E14.5-E15.5\",\n",
    "    \"Gross Morphology Embryo E14.5-E15.5\",\n",
    "    \"Viability E18.5 Secondary Screen\",\n",
    "    \"MicroCT E18.5\",\n",
    "    \"Gross Morphology Embryo E18.5\",\n",
    "    \"Gross Morphology Placenta E18.5\"\n",
    "]\n",
    "\n",
    "# life_stageã‚’Embryoã«ä¸Šæ›¸ã\n",
    "df_statistical_filtered.loc[df_statistical_filtered[\"procedure_name\"].isin(embryo_phenotyping), \"life_stage\"] = \"Embryo\"\n",
    "df_annotated = df_statistical_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_annotated))\n",
    "print(df_annotated[\"life_stage\"].value_counts())\n",
    "# 54059\n",
    "# life_stage\n",
    "# Early       45724\n",
    "# Embryo       4253\n",
    "# Late         4024\n",
    "# Interval       58\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Sex differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.0001\n",
    "\n",
    "# æ¡ä»¶ãƒªã‚¹ãƒˆ\n",
    "conditions = [\n",
    "    (df_annotated[\"sex_effect_p_value\"] < threshold) & (df_annotated[\"female_ko_effect_p_value\"] < threshold) & (df_annotated[\"male_ko_effect_p_value\"] > threshold),\n",
    "    (df_annotated[\"sex_effect_p_value\"] < threshold) & (df_annotated[\"male_ko_effect_p_value\"] < threshold) & (df_annotated[\"female_ko_effect_p_value\"] > threshold)\n",
    "]\n",
    "\n",
    "# æ¡ä»¶ã«å¯¾å¿œã™ã‚‹å€¤\n",
    "choices = [\"female\", \"male\"]\n",
    "\n",
    "# np.selectã§åˆ—ã‚’è¨­å®š\n",
    "df_annotated[\"sexdual_dimorphism\"] = np.select(conditions, choices, default=None)\n",
    "df_annotated = df_annotated.reset_index(drop=True)\n",
    "\n",
    "# çµæœã‚’ç¢ºèª\n",
    "print(RELEASE)\n",
    "print(df_annotated[\"sexdual_dimorphism\"].value_counts())\n",
    "\n",
    "# RELEASE 22.1\n",
    "# male      4915\n",
    "# female    4146\n",
    "\n",
    "# RELEASE 23.0\n",
    "# male      5026\n",
    "# female    4344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¢ºèª\n",
    "df_annotated.dropna(subset=[\"sexdual_dimorphism\"])[[\"p_value\", \"sexdual_dimorphism\", \"effect_size\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### éºä¼å‹ã€æ€§å·®ã€ãƒ©ã‚¤ãƒ•ã‚¹ãƒ†ãƒ¼ã‚¸ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ±åˆã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_annotated[\"zygosity\"].value_counts())\n",
    "# RELEASE 22.1\n",
    "# zygosity\n",
    "# homozygote      41444\n",
    "# heterozygote    11921\n",
    "# hemizygote        694\n",
    "\n",
    "# RELEASE 23.0\n",
    "# homozygote      48037\n",
    "# heterozygote    14706\n",
    "# hemizygote        902\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colmuns = [\"marker_symbol\", \"marker_accession_id\", \"mp_term_name\", \"mp_term_id\", \"p_value\", \"female_ko_effect_p_value\", \"male_ko_effect_p_value\", \"sexdual_dimorphism\", \"zygosity\", \"life_stage\", \"effect_size\",]\n",
    "\n",
    "# df_annotated = df_annotated[colmuns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_term = \"increased fasting circulating glucose level\".strip()\n",
    "# df_annotated[\n",
    "#     (df_annotated[\"marker_symbol\"] == \"Dnase1l2\") &\n",
    "#     (df_annotated[\"mp_term_name\"] == mp_term)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³åˆ—ã‚’è¿½åŠ ï¼ˆinplaceï¼‰\n",
    "def make_annotation(row) -> list[str]:\n",
    "    # éºä¼å‹\n",
    "    if row['zygosity'] == 'homozygote':\n",
    "        annotate = \"Homo\"\n",
    "    elif row['zygosity'] == 'heterozygote':\n",
    "        annotate = \"Hetero\"\n",
    "    else:\n",
    "        annotate = \"Hemi\"\n",
    "\n",
    "    # æ€§åˆ¥\n",
    "    if row['sexdual_dimorphism'] == \"female\":\n",
    "        annotate += \", Female\"\n",
    "    elif row['sexdual_dimorphism'] == \"male\":\n",
    "        annotate += \", Male\"\n",
    "\n",
    "    # life stage\n",
    "    if row['life_stage'] in {\"Embryo\", \"Early\", \"Interval\", \"Late\"}:\n",
    "        annotate += f\", {row['life_stage']}\"\n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    annotations.append(f\"{row['mp_term_name']} ({annotate})\")\n",
    "\n",
    "    return annotations\n",
    "\n",
    "df_annotated[\"annotation\"] = df_annotated.apply(make_annotation, axis=1)\n",
    "\n",
    "df_exploded = df_annotated.explode(\"annotation\").reset_index(drop=True)\n",
    "\n",
    "# marker_symbol ã”ã¨ã« annotation ã‚’ãƒªã‚¹ãƒˆåŒ–ï¼†ã‚½ãƒ¼ãƒˆ\n",
    "marker_annotation_map = (\n",
    "    df_exploded\n",
    "    .groupby(\"marker_symbol\")[\"annotation\"]\n",
    "    .apply(lambda x: sorted(set(x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¾‹ï¼šRhdã®æ³¨é‡ˆã‚’è¡¨ç¤º\n",
    "print(marker_annotation_map[\"Rhd\"])\n",
    "# ä¾‹ï¼šAmtã®æ³¨é‡ˆã‚’è¡¨ç¤º (Embryo)\n",
    "print(marker_annotation_map[\"Amt\"])\n",
    "# ä¾‹ï¼šSpag4ã®æ³¨é‡ˆã‚’è¡¨ç¤º (é‡è¤‡ãŒå‰Šé™¤ã•ã‚Œã¦ã„ã‚‹ã‹)\n",
    "print(marker_annotation_map[\"Spag4\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data/annotation\").mkdir(exist_ok=True, parents=True)\n",
    "file_path = \"data/annotation/symbol_mptermname.json\"\n",
    "marker_annotation_map.to_json(file_path, indent=4)\n",
    "\n",
    "# json.dump(marker_annotation_map, open(file_path, \"w\"), indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "grep -c \"Male\" data/annotation/symbol_mptermname.json | sed \"s|^|Male: |\"\n",
    "grep -c \"Female\" data/annotation/symbol_mptermname.json | sed \"s|^|Feale: |\"\n",
    "\n",
    "grep -c \"Homo\" data/annotation/symbol_mptermname.json | sed \"s|^|Homo: |\"\n",
    "grep -c \"Hetero\" data/annotation/symbol_mptermname.json | sed \"s|^|Hetero: |\"\n",
    "grep -c \"Hemi\" data/annotation/symbol_mptermname.json | sed \"s|^|Hemi: |\"\n",
    "\n",
    "grep -c \"Embryo\" data/annotation/symbol_mptermname.json | sed \"s|^|Embryo: |\"\n",
    "grep -c \"Early\" data/annotation/symbol_mptermname.json | sed \"s|^|Early: |\"\n",
    "grep -c \"Interval\" data/annotation/symbol_mptermname.json | sed \"s|^|Interval: |\"\n",
    "grep -c \"Late\" data/annotation/symbol_mptermname.json | sed \"s|^|Late: |\"\n",
    "\n",
    "# RELEASE 22.1\n",
    "# Male: 4915\n",
    "# Feale: 4146\n",
    "# Homo: 41444\n",
    "# Hetero: 11921\n",
    "# Hemi: 694\n",
    "# Embryo: 4253\n",
    "# Early: 45724\n",
    "# Interval: 58\n",
    "# Late: 4024\n",
    "\n",
    "# RELEASE 23.0\n",
    "# Male: 4480\n",
    "# Feale: 3557\n",
    "# Homo: 30977\n",
    "# Hetero: 9625\n",
    "# Hemi: 492\n",
    "# Embryo: 4207\n",
    "# Early: 34324\n",
    "# Interval: 54\n",
    "# Late: 2509"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Phenodigmã‚’ç”¨ã„ãŸãƒ’ãƒˆç–¾æ‚£æƒ…å ±ã‚’å–å¾—ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phenodigm = pd.read_csv(Path(\"data\", \"phenodigm\", \"impc_phenodigm.csv\"))\n",
    "P(len(df_phenodigm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„è¡Œã«ã¤ã„ã¦ç©ºç™½ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "space_counts = df_phenodigm['Mouse model description'].str.count(' ')\n",
    "\n",
    "# ç©ºç™½ã®æ•°ãŒ2ã§ãªã„è¡Œã‚’æŠ½å‡ºï¼ˆ== split ã—ã¦3ã¤ã«ãªã‚‰ãªã„è¡Œï¼‰\n",
    "invalid_rows = df_phenodigm[space_counts != 2]\n",
    "\n",
    "# çµæœè¡¨ç¤º\n",
    "print(f\"å…¨ä½“ã®ä»¶æ•°: {len(df_phenodigm)}\")\n",
    "print(f\"ç©ºç™½ãŒã¡ã‚‡ã†ã©2ã¤ã§ãªã„è¡Œæ•°: {len(invalid_rows)}\")\n",
    "print(invalid_rows.head())\n",
    "# -> ãŸã£ãŸ2ã¤ã—ã‹ãªãã€`Phex<not yet available>`ãªã®ã§ã€ã“ã®2ã¤ã¯ç„¡è¦–ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phenodigm = df_phenodigm[space_counts == 2]\n",
    "P(len(df_phenodigm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phenodigm[['allele_symbol', 'zygosity', 'life_stage']] = df_phenodigm['Mouse model description'].str.split(' ', n=2, expand=True)\n",
    "df_phenodigm = df_phenodigm.drop(columns=['Mouse model description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P(df_phenodigm.columns)\n",
    "P(df_phenodigm[\"allele_symbol\"].head(3))\n",
    "P(df_phenodigm[\"zygosity\"].head(3))\n",
    "P(df_phenodigm[\"life_stage\"].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phenodigmã®è¡¨è¨˜ã¨impcãƒ‡ãƒ¼ã‚¿ã®è¡¨è¨˜ã‚’æƒãˆã‚‹\n",
    "\n",
    "df_phenodigm = df_phenodigm.replace({'zygosity': {'hom': 'homozygote', 'het': 'heterozygote','hem': 'hemizygote'}})\n",
    "df_phenodigm['life_stage'] = df_phenodigm['life_stage'].str.capitalize()\n",
    "print(df_phenodigm[\"zygosity\"].value_counts())\n",
    "print(df_phenodigm[\"life_stage\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotated_phenodigm = \\\n",
    "    df_annotated.set_index(['allele_symbol','life_stage','zygosity']) \\\n",
    "    .join(df_phenodigm.set_index(['allele_symbol','life_stage','zygosity']), how='left', rsuffix='_phenodigm') \\\n",
    "    .reset_index()\n",
    "print(len(df_annotated_phenodigm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    \"marker_symbol\", \"Disorder name\", \"life_stage\", \"zygosity\"\n",
    "    ]\n",
    "df_annotated_phenodigm = df_annotated_phenodigm[columns_to_keep].dropna(subset=[\"Disorder name\"]).reset_index(drop=True)\n",
    "df_annotated_phenodigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³åˆ—ã‚’è¿½åŠ ï¼ˆinplaceï¼‰\n",
    "def make_annotation(row) -> list[str]:\n",
    "    # éºä¼å‹\n",
    "    if row['zygosity'] == 'homozygote':\n",
    "        annotate = \"Homo\"\n",
    "    elif row['zygosity'] == 'heterozygote':\n",
    "        annotate = \"Hetero\"\n",
    "    else:\n",
    "        annotate = \"Hemi\"\n",
    "\n",
    "    # life stage\n",
    "    if row['life_stage'] in {\"Embryo\", \"Early\", \"Interval\", \"Late\"}:\n",
    "        annotate += f\", {row['life_stage']}\"\n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    annotations.append(f\"{row['Disorder name']} ({annotate})\")\n",
    "\n",
    "    return annotations\n",
    "\n",
    "df_annotated_phenodigm[\"annotation\"] = df_annotated_phenodigm.apply(make_annotation, axis=1)\n",
    "\n",
    "df_exploded = df_annotated_phenodigm.explode(\"annotation\").reset_index(drop=True)\n",
    "\n",
    "# marker_symbol ã”ã¨ã« annotation ã‚’ãƒªã‚¹ãƒˆåŒ–ï¼†ã‚½ãƒ¼ãƒˆ\n",
    "marker_annotation_map = (\n",
    "    df_exploded\n",
    "    .groupby(\"marker_symbol\")[\"annotation\"]\n",
    "    .apply(lambda x: sorted(set(x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¾‹ï¼šPhenodigmã®æ³¨é‡ˆã‚’è¡¨ç¤º (Embryo)\n",
    "print(marker_annotation_map[\"Arhgap31\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data/annotation\").mkdir(exist_ok=True, parents=True)\n",
    "file_path = \"data/annotation/symbol_disordername.json\"\n",
    "marker_annotation_map.to_json(file_path, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mp term nameã¨IMPCã®Phenotype URLã‚’ç´ä»˜ã‘ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_statistical_filtered[['mp_term_id', 'mp_term_name']].drop_duplicates()\n",
    "# df_select = data[['marker_symbol', 'marker_accession_id', 'mp_term_name', 'mp_term_id']].drop_duplicates()\n",
    "df_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_phenotype_url = dict()\n",
    "for index, row in df_select.iterrows():\n",
    "    mp_tern_id = row['mp_term_id']\n",
    "    impc_url = f\"https://www.mousephenotype.org/data/phenotypes/{mp_tern_id}\"\n",
    "    mp_term_name = row['mp_term_name']\n",
    "    dict_phenotype_url[mp_term_name] = impc_url\n",
    "\n",
    "print(dict_phenotype_url[\"small lymph nodes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotation/mptermname_phenotypeurl.tsv', 'w') as f:\n",
    "    for term, url in dict_phenotype_url.items():\n",
    "        f.write(f\"{term}\\t{url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "head -n 3 data/annotation/mptermname_phenotypeurl.tsv\n",
    "wc -l data/annotation/mptermname_phenotypeurl.tsv\n",
    "# Release 22.0: 664\n",
    "# Release 23.0: 659"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### marker symbolã¨MGI accession idã‚’ç´ä»˜ã‘ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_statistical_filtered[['marker_symbol', 'marker_accession_id']].drop_duplicates()\n",
    "# df_select = data[['marker_symbol', 'marker_accession_id', 'mp_term_name', 'mp_term_id']].drop_duplicates()\n",
    "df_select\n",
    "# Release 22.1: 7746 rows\n",
    "# Release 23.0: 7934 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_symbol_id = dict()\n",
    "for index, row in df_select.iterrows():\n",
    "    dict_symbol_id[row['marker_symbol']] = row['marker_accession_id']\n",
    "print(dict_symbol_id[\"Ncam1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dict_symbol_id, open(\"data/annotation/symbol_mgiid.json\", \"w\"), indent=4, sort_keys=True)\n",
    "Path(\"data/annotation/symbol_mgiid.tsv\").write_text(\"\\n\".join([f\"{k}\\t{v}\" for k, v in dict_symbol_id.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n 3 data/annotation/symbol_mgiid.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è¡¨ç¾å‹ã®é¡ä¼¼åº¦ã‚’æ±‚ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"data\", \"annotation\", \"symbol_mptermname.json\")\n",
    "\n",
    "symbol_mptermname = json.load(open(file_path))\n",
    "print(symbol_mptermname[\"Dpf2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç©ºãƒªã‚¹ãƒˆã‚’æŒã¤è¦ç´ ã‚’é™¤å¤–ã—ã¤ã¤ã€å€¤ã‚’ list â†’ set ã«å¤‰æ›ã—ã¦é‡è¤‡ã‚’å‰Šé™¤ã™ã‚‹\n",
    "symbol_mptermname = {k: set(v) for k, v in symbol_mptermname.items() if v}\n",
    "print(symbol_mptermname[\"Dpf2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccardä¿‚æ•°ã§é›†åˆã®é¡ä¼¼åº¦ã‚’è¨ˆç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gene_pair_mp_similarity = []\n",
    "\n",
    "for a, b in combinations(symbol_mptermname, 2):\n",
    "    shared_mp = sorted(symbol_mptermname[a] & symbol_mptermname[b])\n",
    "    shared_mp_number = len(shared_mp)\n",
    "    union_mp_number = len(symbol_mptermname[a] | symbol_mptermname[b])\n",
    "    overlap_ratio = shared_mp_number / union_mp_number\n",
    "\n",
    "    gene_pair_mp_similarity.append([a, b, round(overlap_ratio, 3), shared_mp_number, shared_mp])\n",
    "\n",
    "## 46s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gene_pair_mp_similarity [:3])\n",
    "print(len(gene_pair_mp_similarity))\n",
    "# Release 22.0: 29996385\n",
    "# Release 22.1: 29996385\n",
    "# Release 23.0: 31470211"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é‡è¤‡ã™ã‚‹è¡¨ç¾å‹ãŒé–¾å€¤ä»¥ä¸Šã®ã‚‚ã®ã‚’æŠ½å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.2\n",
    "num_shared_mp = 3\n",
    "\n",
    "gene_pair_mp_similarity_filtered = []\n",
    "for record in gene_pair_mp_similarity:\n",
    "    if record[2] >= similarity_threshold or record[3] >= num_shared_mp:\n",
    "        gene_pair_mp_similarity_filtered.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gene_pair_mp_similarity_filtered[:3])\n",
    "print(len(gene_pair_mp_similarity_filtered))\n",
    "# Release 21.1: 134880\n",
    "# Release 22.0: 133281 <- Homo/Hetero/HemiãŠã‚ˆã³â™‚ãƒ»â™€ã®å®Œå…¨ä¸€è‡´ã‚’è€ƒæ…®ã™ã‚‹ã‚ˆã†ã«ãªã£ãŸãŸã‚ã€æ¸›å°‘\n",
    "# Release 22.1: 133281\n",
    "# v0.3.0: 261216 <- Similarity_threshodã®oræ¡ä»¶ã‚’ã¤ã‘ãŸãŸã‚ã€å¢—åŠ \n",
    "# Release 23.0 TSUMUGI v0.3.2: 241,645 (thres >= 0.5 or num >= 3)\n",
    "# Release 23.0 TSUMUGI v0.3.2: 866,361 (thres >= 0.2 or num >= 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data\", \"overlap\").mkdir(exist_ok=True, parents=True)\n",
    "pickle.dump(gene_pair_mp_similarity, open(\"data/overlap/gene_pair_mp_similarity.pkl\", \"wb\"))\n",
    "pickle.dump(gene_pair_mp_similarity_filtered, open(\"data/overlap/gene_pair_mp_similarity_filtered.pkl\", \"wb\"))\n",
    "\n",
    "# 18 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§å‡ºåŠ› ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity = pd.DataFrame(gene_pair_mp_similarity)\n",
    "df_similarity.columns = [\"Gene1\", \"Gene2\", \"Jaccard Similarity\", \"Number of shared phenotype\", \"List of shared phenotypes\"]\n",
    "df_similarity.reindex(\n",
    "    columns=[\"Gene1\", \"Gene2\", \"Number of shared phenotype\", \"Jaccard Similarity\", \"List of shared phenotypes\"]\n",
    ")\n",
    "df_similarity[\"List of shared phenotypes\"] = df_similarity[\"List of shared phenotypes\"].apply(json.dumps)\n",
    "df_similarity\n",
    "# 30 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"data/TSUMUGI_raw_data.csv.gz\")\n",
    "\n",
    "def get_head1000_hash(df: pd.DataFrame) -> str:\n",
    "    # head(1000)ã ã‘ã‚’å¯¾è±¡ã«ãƒãƒƒã‚·ãƒ¥åŒ–\n",
    "    csv_bytes = df.head(1000).to_csv(index=False, lineterminator='\\n').encode('utf-8')\n",
    "    return hashlib.md5(csv_bytes).hexdigest()\n",
    "\n",
    "def file_head1000_hash(path: Path) -> str | None:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        lines = [next(f) for _ in range(1001)]  # 1è¡Œç›®ãŒãƒ˜ãƒƒãƒ€ãƒ¼\n",
    "        csv_content = ''.join(lines).encode('utf-8')\n",
    "        return hashlib.md5(csv_content).hexdigest()\n",
    "\n",
    "# æ¯”è¼ƒ\n",
    "new_hash = get_head1000_hash(df_similarity)\n",
    "existing_hash = file_head1000_hash(output_path)\n",
    "\n",
    "if new_hash != existing_hash:\n",
    "    df_similarity.to_csv(output_path, index=False, compression=\"gzip\", lineterminator='\\n')\n",
    "    print(\"ğŸ”„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ\") # 3 min\n",
    "else:\n",
    "    print(\"âœ… å†…å®¹ã«å¤‰æ›´ãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_overlap_filtered = pd.DataFrame(shared_ratios_filtered)\n",
    "# df_overlap_filtered.columns = [\"Gene1\", \"Gene2\", \"Jaccard Similarity\", \"Number of shared phenotype\", \"List of shared phenotypes\"]\n",
    "# df_overlap_filtered.reindex(\n",
    "#     columns=[\"Gene1\", \"Gene2\", \"Number of shared phenotype\", \"Jaccard Similarity\", \"List of shared phenotypes\"]\n",
    "# )\n",
    "# df_overlap_filtered[\"List of shared phenotypes\"] = df_overlap_filtered[\"List of shared phenotypes\"].apply(json.dumps)\n",
    "\n",
    "# df_overlap_filtered.to_csv(\"data/TSUMUGI_filtered_data.csv.gz\", index=False, compression=\"gzip\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¡¨ç¾å‹ã”ã¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å‡ºåŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_pair_mp_similarity_filtered = pickle.load(open(\"data/overlap/gene_pair_mp_similarity_filtered.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity = pd.DataFrame(\n",
    "    gene_pair_mp_similarity_filtered, columns=[\"marker1\", \"marker2\", \"phenotype_similarity\", \"shared_mp_number\", \"shared_mp\"]\n",
    ")\n",
    "df_similarity\n",
    "# version 0.2.2: 133281  rows Ã— 5 columns\n",
    "# version 0.3.0: 261216  rows Ã— 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_mp = json.load(open(\"data/annotation/symbol_mptermname.json\"))\n",
    "marker_mp = pd.DataFrame(marker_mp.items(), columns=[\"marker_symbol\", \"mp_term_name\"])\n",
    "marker_mp\n",
    "# version 0.2.2: 7626 rows Ã— 2 columns\n",
    "# version 0.3.0: 7746 rows Ã— 2 columns\n",
    "# version 0.3.1: 7746 rows Ã— 2 columns\n",
    "# RELEASE 23.0: 7954 rows Ã— 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_disease = json.load(open(\"data/annotation/symbol_disordername.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"data/network/mp_term_name\")\n",
    "# remove network directory\n",
    "if output_dir.exists():\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mp_terms = list(Path(\"data\", \"mp_term_name\").glob(\"*.csv\"))\n",
    "# print(path_mp_terms[:3])\n",
    "# print(len(path_mp_terms))\n",
    "# path_mp_term = Path(\"data\", \"mp_term_name\", \"increasing_circulating_glucose_level.csv\")\n",
    "\n",
    "\"\"\"\n",
    "ãƒãƒ¼ãƒ‰ãŒå¤šã™ãã‚‹ã¨Webãƒšãƒ¼ã‚¸ãŒæç”»ã§ããªã„å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€\n",
    "ãƒãƒ¼ãƒ‰æ•°ã‚’200ä»¥ä¸‹ã«ã™ã‚‹ãŸã‚ã«æœ€é©ãªphenotype_similarityã‚’æ±‚ã‚ã‚‹\n",
    "\"\"\"\n",
    "number_of_nodes = 200\n",
    "tolerance = 25\n",
    "\n",
    "for path_mp_term in path_mp_terms:\n",
    "\n",
    "    df_marker_effect = pd.read_csv(path_mp_term).dropna(subset=[\"effect_size\"])\n",
    "\n",
    "    # Absolute value of effect size\n",
    "    df_marker_effect[\"effect_size\"] = df_marker_effect[\"effect_size\"].abs()\n",
    "\n",
    "    # * effect sizeã®çµ¶å¯¾å€¤ãŒæœ€å¤§ã®è¡Œã‚’å–å¾— (Homo/Heteroã§ç•°ãªã‚‹åŠ¹æœé‡ãŒã‚ã‚‹å ´åˆã«ã€ã²ã¨ã¾ãšæœ€å¤§å€¤ã‚’æ¡ç”¨ã™ã‚‹â† ä»Šå¾Œã®è€ƒæ…®äº‹é …)\n",
    "    df_marker_effect = (\n",
    "        df_marker_effect[[\"marker_symbol\", \"effect_size\"]]  # å¿…è¦ãªåˆ—ã ã‘æŠ½å‡º\n",
    "        .loc[df_marker_effect.groupby(\"marker_symbol\")[\"effect_size\"].idxmax()]  # å„ marker_symbol ã”ã¨ã® effect_size æœ€å¤§å€¤ã®è¡Œã‚’å–å¾—\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    mp_term = path_mp_term.stem\n",
    "    target_mp = mp_term.replace(\"_\", \" \")\n",
    "    valid_markers = df_marker_effect['marker_symbol']\n",
    "\n",
    "    df_filtered = df_similarity[\n",
    "        df_similarity['marker1'].isin(valid_markers) &\n",
    "        df_similarity['marker2'].isin(valid_markers) &\n",
    "        df_similarity['shared_mp'].apply(lambda lst: any(target_mp in term for term in lst))\n",
    "    ]\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Nodeã‚’ä½œæˆã™ã‚‹\n",
    "    # ----------------------------------------------------\n",
    "    df_marker1 = df_filtered[[\"marker1\"]]\n",
    "    df_marker2 = df_filtered[[\"marker2\"]]\n",
    "    df_node_marker1 = pd.merge(df_marker1, df_marker_effect, left_on='marker1', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "    df_node_marker2 = pd.merge(df_marker2, df_marker_effect, left_on='marker2', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "\n",
    "    df_node = pd.concat([df_node_marker1, df_node_marker2], axis=0).drop_duplicates()\n",
    "    df_node = pd.merge(df_node, marker_mp, how='inner', on='marker_symbol')\n",
    "    df_node = pd.merge(df_node, df_marker_effect, how='inner', on='marker_symbol')\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # ãƒãƒ¼ãƒ‰æ•°ã‚’200ä»¥ä¸‹ã«ã™ã‚‹ãŸã‚ã«æœ€é©ãªphenotype_similarityã‚’æ±‚ã‚ã‚‹\n",
    "    # ----------------------------------------------------\n",
    "    if len(df_node) > number_of_nodes:\n",
    "\n",
    "        low, high = df_filtered[\"phenotype_similarity\"].min(), df_filtered[\"phenotype_similarity\"].max()\n",
    "\n",
    "        best_phenotype_similarity = 0\n",
    "        closest_diff = float('inf')  # æœ€ã‚‚è¿‘ã„ãƒãƒ¼ãƒ‰æ•°ã®å·®ã‚’è¨˜éŒ²\n",
    "\n",
    "        while low <= high:\n",
    "            mid = (low + high) / 2\n",
    "\n",
    "            df_mid = df_filtered[df_filtered[\"phenotype_similarity\"] >= mid]\n",
    "\n",
    "            # Nodeã‚’ä½œæˆã™ã‚‹\n",
    "            df_marker1 = df_mid[[\"marker1\"]]\n",
    "            df_marker2 = df_mid[[\"marker2\"]]\n",
    "            df_node_marker1 = pd.merge(df_marker1, df_marker_effect, left_on='marker1', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "            df_node_marker2 = pd.merge(df_marker2, df_marker_effect, left_on='marker2', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "            df_node = pd.concat([df_node_marker1, df_node_marker2], axis=0).drop_duplicates()\n",
    "            df_node = pd.merge(df_node, marker_mp, how='inner', on='marker_symbol')\n",
    "            df_node = pd.merge(df_node, df_marker_effect, how='inner', on='marker_symbol')\n",
    "\n",
    "            node_count = len(df_node)\n",
    "\n",
    "            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒ¼ãƒ‰æ•°ã«è¿‘ã„å ´åˆã€best_phenotype_similarityã‚’æ›´æ–°\n",
    "            current_diff = abs(node_count - number_of_nodes)\n",
    "            if current_diff < closest_diff:\n",
    "                best_phenotype_similarity = mid\n",
    "                closest_diff = current_diff \n",
    "\n",
    "            if number_of_nodes - tolerance < node_count < number_of_nodes + tolerance:\n",
    "                break\n",
    "            elif node_count > number_of_nodes:\n",
    "                # ãƒãƒ¼ãƒ‰æ•°ãŒå¤šã„å ´åˆã€ç¯„å›²ã‚’ä¸Šã’ã‚‹\n",
    "                low = mid + 1e-6\n",
    "            else:\n",
    "                # ãƒãƒ¼ãƒ‰æ•°ãŒå°‘ãªã„å ´åˆã€ç¯„å›²ã‚’ä¸‹ã’ã‚‹\n",
    "                high = mid - 1e-6\n",
    "\n",
    "        df_filtered = df_filtered[df_filtered[\"phenotype_similarity\"] >= best_phenotype_similarity]\n",
    "\n",
    "    # Nodeã‚’JSONå½¢å¼ã«å¤‰æ›\n",
    "    node_json = []\n",
    "    for _, row in df_node.iterrows():\n",
    "        node_json.append({\n",
    "            \"data\": {\n",
    "                \"id\": row['marker_symbol'],\n",
    "                \"label\": row['marker_symbol'],\n",
    "                \"phenotype\": row['mp_term_name'],\n",
    "                \"disease\": marker_disease[row['marker_symbol']] if row['marker_symbol'] in marker_disease else \"\",\n",
    "                \"node_color\": row['effect_size']\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Edgeã‚’ä½œæˆã™ã‚‹\n",
    "    # ----------------------------------------------------\n",
    "    df_edge = df_filtered[[\"marker1\", \"marker2\", \"phenotype_similarity\", \"shared_mp\"]]\n",
    "    # Edgeã‚’JSONå½¢å¼ã«å¤‰æ›\n",
    "    edge_json = []\n",
    "    for _, row in df_edge.iterrows():\n",
    "        edge_json.append({\n",
    "            \"data\": {\n",
    "                \"source\": row['marker1'],\n",
    "                \"target\": row['marker2'],\n",
    "                \"phenotype\": row['shared_mp'],\n",
    "                \"edge_size\": row['phenotype_similarity']\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Edgeã¨Nodeã‚’çµ±åˆã—ã¦ã€å‡ºåŠ›\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    network_json = node_json + edge_json\n",
    "\n",
    "    # Output as JSON\n",
    "    if network_json:\n",
    "        output_json = output_dir / f\"{mp_term}.json.gz\"\n",
    "        with gzip.open(output_json, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(network_json, f, indent=4)\n",
    "\n",
    "# 1m30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -lhS data/network/mp_term_name/ | head -n 5\n",
    "\n",
    "# version 0.2.2: total 5.3M\n",
    "# version 0.3.0: total 5.5M\n",
    "# version 0.3.1: total 5.1M <- è©²å½“ã®è¡¨ç¾å‹ã‚’å«ã‚€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã¿ã‚’è¡¨ç¤º ï¼ˆIssue: #54ï¼‰\n",
    "# version 0.3.2: total 5.1M <- Similarity 0.5 or 3 phenotypes\n",
    "# version 0.3.3: total 9.8M <- Similarity 0.2 or 3 phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 200å‰å¾Œã®ãƒãƒ¼ãƒ‰æ•°ã¨ãªã£ã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "\n",
    "zcat data/network/mp_term_name/preweaning_lethality,_complete_penetrance.json.gz | grep -c \"node_color\"\n",
    "# preweaning_lethality,_complete_penetranceã¯ä¼¼ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ãŒå¤šã™ãã¦ã€200ã‚’è¶…ãˆã¦ã—ã¾ã†\n",
    "\n",
    "zcat data/network/mp_term_name/edema.json.gz | grep -c \"node_color\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path_mp_term = Path(\"data/mp_term_name/preweaning_lethality,_complete_penetrance.csv\")\n",
    "\n",
    "# number_of_nodes = 200\n",
    "# tolerance = 25\n",
    "# df_marker_effect = pd.read_csv(path_mp_term).dropna(subset=[\"effect_size\"])\n",
    "\n",
    "# # Absolute value of effect size\n",
    "# df_marker_effect[\"effect_size\"] = df_marker_effect[\"effect_size\"].abs()\n",
    "\n",
    "# # * effect sizeã®çµ¶å¯¾å€¤ãŒæœ€å¤§ã®è¡Œã‚’å–å¾— (Homo/Heteroã§ç•°ãªã‚‹åŠ¹æœé‡ãŒã‚ã‚‹å ´åˆã«ã€ã²ã¨ã¾ãšæœ€å¤§å€¤ã‚’æ¡ç”¨ã™ã‚‹â† ä»Šå¾Œã®è€ƒæ…®äº‹é …)\n",
    "# df_marker_effect = (\n",
    "#     df_marker_effect[[\"marker_symbol\", \"effect_size\"]]  # å¿…è¦ãªåˆ—ã ã‘æŠ½å‡º\n",
    "#     .loc[df_marker_effect.groupby(\"marker_symbol\")[\"effect_size\"].idxmax()]  # å„ marker_symbol ã”ã¨ã® effect_size æœ€å¤§å€¤ã®è¡Œã‚’å–å¾—\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# mp_term = path_mp_term.stem\n",
    "# target_mp = mp_term.replace(\"_\", \" \")\n",
    "# valid_markers = df_marker_effect['marker_symbol']\n",
    "\n",
    "# df_filtered = df_similarity[\n",
    "#     df_similarity['marker1'].isin(valid_markers) &\n",
    "#     df_similarity['marker2'].isin(valid_markers) &\n",
    "#     df_similarity['shared_mp'].apply(lambda lst: any(target_mp in term for term in lst))\n",
    "# ]\n",
    "\n",
    "# # ----------------------------------------------------\n",
    "# # Nodeã‚’ä½œæˆã™ã‚‹\n",
    "# # ----------------------------------------------------\n",
    "# df_marker1 = df_filtered[[\"marker1\"]]\n",
    "# df_marker2 = df_filtered[[\"marker2\"]]\n",
    "# df_node_marker1 = pd.merge(df_marker1, df_marker_effect, left_on='marker1', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "# df_node_marker2 = pd.merge(df_marker2, df_marker_effect, left_on='marker2', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "\n",
    "# df_node = pd.concat([df_node_marker1, df_node_marker2], axis=0).drop_duplicates()\n",
    "# df_node = pd.merge(df_node, marker_mp, how='inner', on='marker_symbol')\n",
    "# df_node = pd.merge(df_node, df_marker_effect, how='inner', on='marker_symbol')\n",
    "\n",
    "# best_phenotype_similarity = None\n",
    "# best_under_node_count = -1\n",
    "# best_under_mid = None\n",
    "\n",
    "# print(len(df_node))\n",
    "# # ----------------------------------------------------\n",
    "# # ãƒãƒ¼ãƒ‰æ•°ã‚’200ä»¥ä¸‹ã«ã™ã‚‹ãŸã‚ã«æœ€é©ãªphenotype_similarityã‚’æ±‚ã‚ã‚‹\n",
    "# # ----------------------------------------------------\n",
    "# if len(df_node) > number_of_nodes:\n",
    "\n",
    "#     low, high = df_filtered[\"phenotype_similarity\"].min(), df_filtered[\"phenotype_similarity\"].max()\n",
    "#     print(low, high)\n",
    "#     while low <= high:\n",
    "#         mid = (low + high) / 2\n",
    "\n",
    "#         df_mid = df_filtered[df_filtered[\"phenotype_similarity\"] >= mid]\n",
    "\n",
    "#         # Nodeã‚’ä½œæˆã™ã‚‹\n",
    "#         df_marker1 = df_mid[[\"marker1\"]]\n",
    "#         df_marker2 = df_mid[[\"marker2\"]]\n",
    "#         df_node_marker1 = pd.merge(df_marker1, df_marker_effect, left_on='marker1', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "#         df_node_marker2 = pd.merge(df_marker2, df_marker_effect, left_on='marker2', right_on='marker_symbol', how='inner')[[\"marker_symbol\"]]\n",
    "#         df_node = pd.concat([df_node_marker1, df_node_marker2], axis=0).drop_duplicates()\n",
    "#         df_node = pd.merge(df_node, marker_mp, how='inner', on='marker_symbol')\n",
    "#         df_node = pd.merge(df_node, df_marker_effect, how='inner', on='marker_symbol')\n",
    "\n",
    "#         node_count = len(df_node)\n",
    "\n",
    "#         if number_of_nodes - tolerance < node_count < number_of_nodes + tolerance:\n",
    "#             best_phenotype_similarity = mid\n",
    "#             break\n",
    "#         elif node_count > number_of_nodes:\n",
    "#             # ãƒãƒ¼ãƒ‰æ•°ãŒå¤šã„å ´åˆã€ç¯„å›²ã‚’ä¸Šã’ã‚‹\n",
    "#             low = mid + 1e-6\n",
    "#         else:\n",
    "#             # ãƒãƒ¼ãƒ‰æ•°ãŒå°‘ãªã„å ´åˆã€ç¯„å›²ã‚’ä¸‹ã’ã‚‹\n",
    "#             best_under_node_count = node_count\n",
    "#             best_under_mid = mid\n",
    "#             high = mid - 1e-6\n",
    "\n",
    "#     if best_phenotype_similarity is None:\n",
    "#         best_phenotype_similarity = best_under_mid\n",
    "\n",
    "#     df_optimized = df_filtered[df_filtered[\"phenotype_similarity\"] >= best_phenotype_similarity]\n",
    "#     G = nx.from_pandas_edgelist(df_optimized, \"marker1\", \"marker2\")\n",
    "#     print(low, high)\n",
    "#     print(len(G.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## éºä¼å­ã”ã¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å‡ºåŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_mp_dict = dict(zip(marker_mp.marker_symbol, marker_mp.mp_term_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_symbols = df_similarity.marker1.unique().tolist()\n",
    "gene_symbols += df_similarity.marker2.unique().tolist()\n",
    "gene_symbols = list(set(gene_symbols))\n",
    "gene_symbols.sort()  # ä»¥ä¸‹ã®foræ–‡ã§ã€ã©ã“ã¾ã§éºä¼å­ãŒå‡¦ç†ã•ã‚ŒãŸã®ã‹é€”ä¸­çµŒéã‚’è¦‹ç©ã‚‚ã‚‹ãŸã‚ã®ã‚½ãƒ¼ãƒˆ\n",
    "P(gene_symbols[:3])\n",
    "P(len(gene_symbols))  # 6003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"data\", \"network\", \"gene_symbol\")\n",
    "# remove network directory\n",
    "if output_dir.exists():\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_nodes = 200\n",
    "tolerance = 25  # tolerance for the number of nodes\n",
    "\n",
    "for i, gene_symbol in enumerate(gene_symbols):\n",
    "    \"\"\"\n",
    "    ãƒãƒ¼ãƒ‰ãŒå¤šã™ãã‚‹ã¨Webãƒšãƒ¼ã‚¸ãŒæç”»ã§ããªã„å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€\n",
    "    ãƒãƒ¼ãƒ‰æ•°ã‚’200ä»¥ä¸‹ã«ã™ã‚‹ãŸã‚ã«æœ€é©ãªphenotype_similarityã‚’æ±‚ã‚ã‚‹\n",
    "    \"\"\"\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing {int(i+1)}/{len(gene_symbols)}: {gene_symbol}\")\n",
    "\n",
    "    df_filtered = df_similarity[(df_similarity[\"marker1\"] == gene_symbol) | (df_similarity[\"marker2\"] == gene_symbol)]\n",
    "    G = nx.from_pandas_edgelist(df_filtered, \"marker1\", \"marker2\")\n",
    "\n",
    "    # ãƒãƒ¼ãƒ‰Aã¨ç›´æ¥ã¤ãªãŒã£ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã®ã¿ã‚’å–å¾—\n",
    "    neighbors = list(G.neighbors(gene_symbol))\n",
    "    subgraph_nodes = [gene_symbol] + neighbors\n",
    "    subgraph = G.subgraph(subgraph_nodes)\n",
    "\n",
    "    if len(subgraph.nodes) > number_of_nodes:\n",
    "\n",
    "        low, high = df_filtered[\"phenotype_similarity\"].min(), df_filtered[\"phenotype_similarity\"].max()\n",
    "\n",
    "        best_phenotype_similarity = 0\n",
    "        closest_diff = float('inf')  # æœ€ã‚‚è¿‘ã„ãƒãƒ¼ãƒ‰æ•°ã®å·®ã‚’è¨˜éŒ²\n",
    "\n",
    "        while low <= high:\n",
    "            mid = (low + high) / 2\n",
    "\n",
    "            # phenotype_similarity >= mid ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "            df_mid = df_filtered[df_filtered[\"phenotype_similarity\"] >= mid]\n",
    "\n",
    "            G = nx.from_pandas_edgelist(df_mid, \"marker1\", \"marker2\")\n",
    "            # ãƒãƒ¼ãƒ‰Aã¨ç›´æ¥ã¤ãªãŒã£ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã®ã¿ã‚’å–å¾—\n",
    "            if gene_symbol in G:\n",
    "                neighbors = list(G.neighbors(gene_symbol))\n",
    "            else:\n",
    "            # mid ãŒå¤§ãã™ãã¦ãƒãƒ¼ãƒ‰ãŒé™¤å¤–ã•ã‚Œã™ããŸï¼ˆï¼subgraph ãŒå°ã•ããªã‚Šã™ããŸï¼‰ã¨è€ƒãˆã¦ã€æ¢ç´¢ç¯„å›²ã®ä¸Šé™ (high) ã‚’ä¸‹ã’ã‚‹\n",
    "                high = mid - 1e-6\n",
    "                continue\n",
    "\n",
    "            subgraph_nodes = [gene_symbol] + neighbors\n",
    "            subgraph = G.subgraph(subgraph_nodes)\n",
    "\n",
    "            node_count = len(subgraph.nodes)\n",
    "\n",
    "            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒ¼ãƒ‰æ•°ã«è¿‘ã„å ´åˆã€best_phenotype_similarityã‚’æ›´æ–°\n",
    "            current_diff = abs(node_count - number_of_nodes)\n",
    "            if current_diff < closest_diff:\n",
    "                best_phenotype_similarity = mid\n",
    "                closest_diff = current_diff \n",
    "\n",
    "            if number_of_nodes - tolerance < node_count < number_of_nodes + tolerance:\n",
    "                break\n",
    "            elif node_count > number_of_nodes:\n",
    "                # ãƒãƒ¼ãƒ‰æ•°ãŒå¤šã„å ´åˆã€ç¯„å›²ã‚’ä¸Šã’ã‚‹\n",
    "                low = mid + 1e-6\n",
    "            else:\n",
    "                # ãƒãƒ¼ãƒ‰æ•°ãŒå°‘ãªã„å ´åˆã€ç¯„å›²ã‚’ä¸‹ã’ã‚‹\n",
    "                high = mid - 1e-6\n",
    "\n",
    "        # æœ€é©ãªphenotype_similarityã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        df_optimized = df_filtered[df_filtered[\"phenotype_similarity\"] >= best_phenotype_similarity]\n",
    "        G = nx.from_pandas_edgelist(df_optimized, \"marker1\", \"marker2\")\n",
    "\n",
    "        # ãƒãƒ¼ãƒ‰Aã¨ç›´æ¥ã¤ãªãŒã£ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã®ã¿ã‚’å–å¾—\n",
    "        neighbors = list(G.neighbors(gene_symbol))\n",
    "        subgraph_nodes = [gene_symbol] + neighbors\n",
    "        subgraph = G.subgraph(subgraph_nodes)\n",
    "        \n",
    "    # nodesã‚’ç”¨æ„\n",
    "    node_json = []\n",
    "    for node in subgraph.nodes():\n",
    "        annotation = marker_mp_dict[node]\n",
    "        node_color = 1 if node == gene_symbol else 0\n",
    "        node_json.append({\n",
    "            \"data\": {\n",
    "                \"id\": node,\n",
    "                \"label\": node,\n",
    "                \"node_color\": node_color,\n",
    "                \"phenotype\": annotation,\n",
    "                \"disease\": marker_disease[node] if node in marker_disease else \"\",\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # edgesã‚’ç”¨æ„\n",
    "    df_edge = df_similarity[\n",
    "        (df_similarity[\"marker1\"].isin(subgraph.nodes())) & (df_similarity[\"marker2\"].isin(subgraph.nodes()))\n",
    "    ]\n",
    "\n",
    "    edge_json = []\n",
    "    for edge in df_edge.itertuples():\n",
    "        edge_json.append(\n",
    "            {\n",
    "                \"data\": {\n",
    "                    \"source\": edge.marker1,\n",
    "                    \"target\": edge.marker2,\n",
    "                    \"edge_size\": edge.phenotype_similarity,\n",
    "                    \"phenotype\": edge.shared_mp,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    network_json = node_json + edge_json\n",
    "\n",
    "    # Output as JSON\n",
    "    if network_json:\n",
    "        output_json = output_dir / f\"{gene_symbol}.json.gz\"\n",
    "        with gzip.open(output_json, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(network_json, f, indent=4)\n",
    "\n",
    "# 10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -lhS data/network/gene_symbol/ | head -n 5\n",
    "# version 0.3.0: total 170M\n",
    "# version 0.3.1: total 168M\n",
    "# version 0.3.2: total 145M\n",
    "\n",
    "# total 287M\n",
    "# -rwxrwxrwx 1 kuno kuno 578K Jun 11 05:35 Ttbk2.json.gz\n",
    "# -rwxrwxrwx 1 kuno kuno 558K Jun 11 05:22 Furin.json.gz\n",
    "# -rwxrwxrwx 1 kuno kuno 558K Jun 11 05:35 Trpm7.json.gz\n",
    "# -rwxrwxrwx 1 kuno kuno 555K Jun 11 05:29 Plpp3.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 200å‰å¾Œã®ãƒãƒ¼ãƒ‰æ•°ã¨ãªã£ã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "zcat data/network/gene_symbol/Crls1.json.gz | grep -c \"node_color\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! DEBUG TEST\n",
    "# number_of_nodes = 200\n",
    "# tolerance = 25  # tolerance for the number of nodes\n",
    "# gene_symbol = \"Map2k1\"\n",
    "# print(gene_symbol)\n",
    "# # ä»Šã®å‡¦ç†\n",
    "# df_filtered = df_similarity[(df_similarity[\"marker1\"] == gene_symbol) | (df_similarity[\"marker2\"] == gene_symbol)]\n",
    "\n",
    "# G = nx.from_pandas_edgelist(df_filtered, \"marker1\", \"marker2\")\n",
    "\n",
    "# # ãƒãƒ¼ãƒ‰Aã¨ç›´æ¥ã¤ãªãŒã£ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã®ã¿ã‚’å–å¾—\n",
    "# neighbors = list(G.neighbors(gene_symbol))\n",
    "# subgraph_nodes = [gene_symbol] + neighbors\n",
    "# subgraph = G.subgraph(subgraph_nodes)\n",
    "\n",
    "# P(len(subgraph.nodes))  # 200\n",
    "\n",
    "# if len(subgraph.nodes) > number_of_nodes:\n",
    "#     # äºŒåˆ†æ¢ç´¢ã®ç¯„å›²\n",
    "#     low, high = df_filtered[\"phenotype_similarity\"].min(), df_filtered[\"phenotype_similarity\"].max()\n",
    "\n",
    "#     # æœ€é©ãªã‚‚ã®ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã«å‚™ãˆã¦ä¿æŒ\n",
    "#     best_under_node_count = -1\n",
    "#     best_under_mid = 0\n",
    "#     best_phenotype_similarity = 0\n",
    "#     closest_diff = float('inf')  # æœ€ã‚‚è¿‘ã„ãƒãƒ¼ãƒ‰æ•°ã®å·®ã‚’è¨˜éŒ²\n",
    "\n",
    "#     while low <= high:\n",
    "#         mid = (low + high) / 2\n",
    "\n",
    "#         # phenotype_similarity >= mid ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "#         df_mid = df_filtered[df_filtered[\"phenotype_similarity\"] >= mid]\n",
    "\n",
    "#         G = nx.from_pandas_edgelist(df_mid, \"marker1\", \"marker2\")\n",
    "#         # ãƒãƒ¼ãƒ‰Aã¨ç›´æ¥ã¤ãªãŒã£ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã®ã¿ã‚’å–å¾—\n",
    "#         if gene_symbol in G:\n",
    "#             neighbors = list(G.neighbors(gene_symbol))\n",
    "#         else:\n",
    "#         # mid ãŒå¤§ãã™ãã¦ãƒãƒ¼ãƒ‰ãŒé™¤å¤–ã•ã‚Œã™ããŸï¼ˆï¼subgraph ãŒå°ã•ããªã‚Šã™ããŸï¼‰ã¨è€ƒãˆã¦ã€æ¢ç´¢ç¯„å›²ã®ä¸Šé™ (high) ã‚’ä¸‹ã’ã‚‹\n",
    "#             high = mid - 1e-6\n",
    "#             continue\n",
    "\n",
    "#         subgraph_nodes = [gene_symbol] + neighbors\n",
    "#         subgraph = G.subgraph(subgraph_nodes)\n",
    "\n",
    "#         node_count = len(subgraph.nodes)\n",
    "\n",
    "#         # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒ¼ãƒ‰æ•°ã«è¿‘ã„å ´åˆã€best_phenotype_similarityã‚’æ›´æ–°\n",
    "#         current_diff = abs(node_count - number_of_nodes)\n",
    "#         if current_diff < closest_diff:\n",
    "#             best_phenotype_similarity = mid\n",
    "#             closest_diff = current_diff \n",
    "\n",
    "#         if number_of_nodes - tolerance < node_count < number_of_nodes + tolerance:\n",
    "#             break\n",
    "#         elif node_count > number_of_nodes:\n",
    "#             # ãƒãƒ¼ãƒ‰æ•°ãŒå¤šã„å ´åˆã€ç¯„å›²ã‚’ä¸Šã’ã‚‹\n",
    "#             low = mid + 1e-6\n",
    "#         else:\n",
    "#             # ãƒãƒ¼ãƒ‰æ•°ãŒå°‘ãªã„å ´åˆã€å€™è£œã¨ã—ã¦è¨˜éŒ²\n",
    "#             if node_count > best_under_node_count:\n",
    "#                 best_under_node_count = node_count\n",
    "#                 best_under_mid = mid\n",
    "#             # ãƒãƒ¼ãƒ‰æ•°ãŒå°‘ãªã„å ´åˆã€ç¯„å›²ã‚’ä¸‹ã’ã‚‹\n",
    "#             high = mid - 1e-6\n",
    "\n",
    "#     # æœ€é©ãªphenotype_similarityã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "#     df_optimized = df_filtered[df_filtered[\"phenotype_similarity\"] >= best_phenotype_similarity]\n",
    "#     G = nx.from_pandas_edgelist(df_optimized, \"marker1\", \"marker2\")\n",
    "\n",
    "#     # ãƒãƒ¼ãƒ‰Aã¨ç›´æ¥ã¤ãªãŒã£ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã®ã¿ã‚’å–å¾—\n",
    "#     neighbors = list(G.neighbors(gene_symbol))\n",
    "#     subgraph_nodes = [gene_symbol] + neighbors\n",
    "#     subgraph = G.subgraph(subgraph_nodes)\n",
    "    \n",
    "# print(node_count, low, high, mid, best_phenotype_similarity, best_under_mid)\n",
    "# P(len(subgraph.nodes))  # 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[\"phenotype_similarity\"].plot(kind=\"hist\", bins=100, title=f\"Phenotype Similarity Distribution for {gene_symbol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data/overlap/available_gene_symbols.txt\").write_text(\"\\n\".join(gene_symbols) + \"\\n\")\n",
    "print(len(gene_symbols))  # 4416 -> 4244 â†’ 6003 â†’ 4139\n",
    "# version 0.2.2: 4139\n",
    "# version 0.3.0: 6812 (Life stageã‚’è€ƒæ…® + é¡ä¼¼åº¦ã‚’è¿½åŠ )\n",
    "# version 0.3.1: 6812\n",
    "# version 0.3.2: 6904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "uname -a # OS name\n",
    "date +\"%Y/%m/%d %H:%M:%S\" # Last update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tsumugi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
