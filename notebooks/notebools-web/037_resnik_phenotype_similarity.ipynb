{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnik Phenotype Similarity (Simplified)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€036_resnik_phenotype_similarity.ipynbã‚’ã‚·ãƒ³ãƒ—ãƒ«ã§å¯èª­æ€§ã®é«˜ã„å®Ÿè£…ã«ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã™ã€‚\n",
    "- ã‚¯ãƒ©ã‚¹ã‚’ä½¿ã‚ãšé–¢æ•°ãƒ™ãƒ¼ã‚¹ã§å®Ÿè£…\n",
    "- ç…©é›‘ãªéƒ¨åˆ†ã‚’ç°¡ç´ åŒ–\n",
    "- ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆGenotype, Sex, Life stageï¼‰ã‚’è€ƒæ…®ã—ãŸResnik similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã«ç§»å‹•\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "while not Path(\"LICENSE\").exists():\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "print(f\"Project root: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MPOã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mpo_obo(obo_file_path: str) -> dict:\n",
    "    \"\"\"MPO OBOãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼æ§‹é€ ã‚’è¿”ã™\"\"\"\n",
    "\n",
    "    with open(obo_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # ã‚¿ãƒ¼ãƒ æƒ…å ±ã‚’æ ¼ç´\n",
    "    terms = {}  # MP_ID -> {name, is_a, ...}\n",
    "    name_to_id = {}  # name -> MP_ID\n",
    "    hierarchy = defaultdict(set)  # child_id -> {parent_ids}\n",
    "\n",
    "    # Termãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆ†å‰²\n",
    "    term_blocks = content.split(\"[Term]\")[1:]\n",
    "\n",
    "    for block in term_blocks:\n",
    "        lines = [line.strip() for line in block.strip().split(\"\\n\") if line.strip()]\n",
    "\n",
    "        term_data = {\"is_a\": [], \"is_obsolete\": False}\n",
    "\n",
    "        for line in lines:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "\n",
    "            key, value = line.split(\":\", 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if key == \"id\":\n",
    "                term_data[\"id\"] = value\n",
    "            elif key == \"name\":\n",
    "                term_data[\"name\"] = value\n",
    "            elif key == \"is_a\":\n",
    "                parent_id = value.split(\" !\")[0].strip()\n",
    "                term_data[\"is_a\"].append(parent_id)\n",
    "            elif key == \"is_obsolete\":\n",
    "                term_data[\"is_obsolete\"] = value.lower() == \"true\"\n",
    "\n",
    "        # æœ‰åŠ¹ãªã‚¿ãƒ¼ãƒ ã®ã¿è¿½åŠ \n",
    "        if \"id\" in term_data and \"name\" in term_data and not term_data[\"is_obsolete\"]:\n",
    "            term_id = term_data[\"id\"]\n",
    "            terms[term_id] = term_data\n",
    "            name_to_id[term_data[\"name\"]] = term_id\n",
    "\n",
    "            # éšå±¤é–¢ä¿‚ã‚’æ§‹ç¯‰\n",
    "            for parent_id in term_data[\"is_a\"]:\n",
    "                hierarchy[term_id].add(parent_id)\n",
    "\n",
    "    return {\"terms\": terms, \"name_to_id\": name_to_id, \"hierarchy\": hierarchy}\n",
    "\n",
    "\n",
    "# MPOã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼ã‚’ãƒ‘ãƒ¼ã‚¹\n",
    "mpo_data = parse_mpo_obo(\"data/ontology/mp.obo\")\n",
    "print(f\"Parsed {len(mpo_data['terms'])} MP terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è¡¨ç¾å‹ã®è§£æã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³åˆ†é›¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_phenotype_annotations(phenotype: str) -> tuple:\n",
    "    \"\"\"è¡¨ç¾å‹æ–‡å­—åˆ—ã‹ã‚‰åŸºæœ¬é …ç›®ã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆ†é›¢\"\"\"\n",
    "\n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³: \"åŸºæœ¬è¡¨ç¾å‹ (ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³)\"\n",
    "    pattern = r\"^(.+?)\\s*\\(([^)]+)\\)$\"\n",
    "    match = re.match(pattern, phenotype.strip())\n",
    "\n",
    "    if not match:\n",
    "        return phenotype.strip(), {}\n",
    "\n",
    "    base_term = match.group(1).strip()\n",
    "    annotation_str = match.group(2).strip()\n",
    "\n",
    "    # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆ†é¡\n",
    "    annotations = {}\n",
    "\n",
    "    # è¤‡æ•°ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã‚«ãƒ³ãƒã§åŒºåˆ‡ã‚‰ã‚Œã‚‹\n",
    "    parts = [part.strip() for part in annotation_str.split(\",\")]\n",
    "\n",
    "    for part in parts:\n",
    "        if part in [\"Homo\", \"Hetero\", \"Hemi\"]:\n",
    "            annotations[\"genotype\"] = part\n",
    "        elif part in [\"Male\", \"Female\"]:\n",
    "            annotations[\"sex\"] = part\n",
    "        elif part in [\"Early\", \"Late\", \"Embryo\", \"Interval\"]:\n",
    "            annotations[\"life_stage\"] = part\n",
    "\n",
    "    return base_term, annotations\n",
    "\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "test_phenotypes = [\n",
    "    \"preweaning lethality, complete penetrance (Homo, Early)\",\n",
    "    \"abnormal skin morphology (Homo, Early)\",\n",
    "    \"hyperactivity (Homo, Early)\",\n",
    "]\n",
    "\n",
    "print(\"è¡¨ç¾å‹è§£æãƒ†ã‚¹ãƒˆ:\")\n",
    "for phenotype in test_phenotypes:\n",
    "    base, annotations = parse_phenotype_annotations(phenotype)\n",
    "    print(f\"  {phenotype}\")\n",
    "    print(f\"    åŸºæœ¬: '{base}'\")\n",
    "    print(f\"    ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: {annotations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è¡¨ç¾å‹é »åº¦ã¨ICå€¤ã®è¨ˆç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_phenotype_frequencies(symbol_mptermname_path: str) -> dict:\n",
    "    \"\"\"éºä¼å­â†’è¡¨ç¾å‹ãƒãƒƒãƒ”ãƒ³ã‚°ã‹ã‚‰å„è¡¨ç¾å‹ã®å‡ºç¾é »åº¦ã‚’è¨ˆç®—\"\"\"\n",
    "\n",
    "    with open(symbol_mptermname_path, \"r\") as f:\n",
    "        symbol_mptermname = json.load(f)\n",
    "\n",
    "    phenotype_counts = Counter()\n",
    "\n",
    "    for gene, phenotypes in symbol_mptermname.items():\n",
    "        if phenotypes:\n",
    "            for phenotype in phenotypes:\n",
    "                if phenotype.strip():\n",
    "                    phenotype_counts[phenotype.strip()] += 1\n",
    "\n",
    "    return dict(phenotype_counts)\n",
    "\n",
    "\n",
    "def calculate_base_phenotype_frequencies(phenotype_frequencies: dict) -> dict:\n",
    "    \"\"\"åŸºæœ¬è¡¨ç¾å‹ï¼ˆã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é™¤å»ï¼‰ã®é »åº¦ã‚’è¨ˆç®—\"\"\"\n",
    "\n",
    "    base_phenotype_counts = Counter()\n",
    "\n",
    "    for full_phenotype, frequency in phenotype_frequencies.items():\n",
    "        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é™¤å»ã—ã¦åŸºæœ¬è¡¨ç¾å‹ã‚’å–å¾—\n",
    "        base_phenotype, _ = parse_phenotype_annotations(full_phenotype)\n",
    "        base_phenotype_counts[base_phenotype] += frequency\n",
    "\n",
    "    return dict(base_phenotype_counts)\n",
    "\n",
    "\n",
    "def calculate_information_content(base_phenotype_frequencies: dict) -> dict:\n",
    "    \"\"\"å„åŸºæœ¬è¡¨ç¾å‹ã®Information Content (IC)ã‚’è¨ˆç®—\"\"\"\n",
    "\n",
    "    total_observations = sum(base_phenotype_frequencies.values())\n",
    "    ic_scores = {}\n",
    "\n",
    "    for base_phenotype, frequency in base_phenotype_frequencies.items():\n",
    "        probability = frequency / total_observations\n",
    "        ic_score = -math.log(probability) if probability > 0 else 0.0\n",
    "        ic_scores[base_phenotype] = ic_score\n",
    "\n",
    "    return ic_scores\n",
    "\n",
    "\n",
    "# é »åº¦ã¨ICå€¤ã‚’è¨ˆç®—\n",
    "phenotype_frequencies = calculate_phenotype_frequencies(\n",
    "    \"data/annotation/symbol_mptermname.json\"\n",
    ")\n",
    "\n",
    "# åŸºæœ¬è¡¨ç¾å‹ã®é »åº¦ã‚’è¨ˆç®—ï¼ˆã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é™¤å»ï¼‰\n",
    "base_phenotype_frequencies = calculate_base_phenotype_frequencies(phenotype_frequencies)\n",
    "\n",
    "# åŸºæœ¬è¡¨ç¾å‹ã®ICå€¤ã‚’è¨ˆç®—\n",
    "ic_scores = calculate_information_content(base_phenotype_frequencies)\n",
    "\n",
    "print(f\"Found {len(phenotype_frequencies)} unique full phenotypes\")\n",
    "print(f\"Found {len(base_phenotype_frequencies)} unique base phenotypes\")\n",
    "print(f\"Total phenotype observations: {sum(phenotype_frequencies.values())}\")\n",
    "\n",
    "# é »å‡ºè¡¨ç¾å‹TOP5ã‚’ç¢ºèªï¼ˆãƒ•ãƒ«è¡¨ç¾å‹ï¼‰\n",
    "top_phenotypes = sorted(\n",
    "    phenotype_frequencies.items(), key=lambda x: x[1], reverse=True\n",
    ")[:5]\n",
    "print(\"\\nTop 5 most frequent full phenotypes:\")\n",
    "for phenotype, count in top_phenotypes:\n",
    "    print(f\"  {count:4d}: {phenotype}\")\n",
    "\n",
    "# é »å‡ºåŸºæœ¬è¡¨ç¾å‹TOP5ã‚’ç¢ºèª\n",
    "top_base_phenotypes = sorted(\n",
    "    base_phenotype_frequencies.items(), key=lambda x: x[1], reverse=True\n",
    ")[:5]\n",
    "print(\"\\nTop 5 most frequent base phenotypes:\")\n",
    "for base_phenotype, count in top_base_phenotypes:\n",
    "    ic_score = ic_scores[base_phenotype]\n",
    "    print(f\"  {count:4d}: {base_phenotype} (IC: {ic_score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼éšå±¤ã¨LCAè¨ˆç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ancestors(term_id: str, hierarchy: dict) -> set:\n",
    "    \"\"\"æŒ‡å®šã•ã‚ŒãŸtermã®å…¨ç¥–å…ˆã‚’å–å¾—\"\"\"\n",
    "\n",
    "    ancestors = set()\n",
    "    stack = [term_id]\n",
    "\n",
    "    while stack:\n",
    "        current = stack.pop()\n",
    "        for parent in hierarchy.get(current, set()):\n",
    "            if parent not in ancestors:\n",
    "                ancestors.add(parent)\n",
    "                stack.append(parent)\n",
    "\n",
    "    return ancestors\n",
    "\n",
    "\n",
    "def calculate_term_depth(\n",
    "    term_id: str, hierarchy: dict, depth_cache: dict = None\n",
    ") -> int:\n",
    "    \"\"\"ãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®æ·±ã•ã‚’è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰\"\"\"\n",
    "\n",
    "    if depth_cache is None:\n",
    "        depth_cache = {}\n",
    "\n",
    "    if term_id in depth_cache:\n",
    "        return depth_cache[term_id]\n",
    "\n",
    "    if not hierarchy.get(term_id):\n",
    "        depth_cache[term_id] = 0\n",
    "        return 0\n",
    "\n",
    "    max_depth = 0\n",
    "    for parent in hierarchy[term_id]:\n",
    "        parent_depth = calculate_term_depth(parent, hierarchy, depth_cache)\n",
    "        max_depth = max(max_depth, parent_depth + 1)\n",
    "\n",
    "    depth_cache[term_id] = max_depth\n",
    "    return max_depth\n",
    "\n",
    "\n",
    "def find_lowest_common_ancestor(term_a: str, term_b: str, mpo_data: dict) -> str | None:\n",
    "    \"\"\"2ã¤ã®termã®æœ€ä¸‹ä½å…±é€šç¥–å…ˆï¼ˆLCAï¼‰ã‚’è¦‹ã¤ã‘ã‚‹\"\"\"\n",
    "\n",
    "    # åå‰ã‹ã‚‰IDã«å¤‰æ›\n",
    "    id_a = mpo_data[\"name_to_id\"].get(term_a, term_a)\n",
    "    id_b = mpo_data[\"name_to_id\"].get(term_b, term_b)\n",
    "\n",
    "    if id_a not in mpo_data[\"terms\"] or id_b not in mpo_data[\"terms\"]:\n",
    "        return None\n",
    "\n",
    "    # ä¸¡æ–¹ã®ç¥–å…ˆã‚’å–å¾—\n",
    "    ancestors_a = get_ancestors(id_a, mpo_data[\"hierarchy\"])\n",
    "    ancestors_a.add(id_a)\n",
    "\n",
    "    ancestors_b = get_ancestors(id_b, mpo_data[\"hierarchy\"])\n",
    "    ancestors_b.add(id_b)\n",
    "\n",
    "    # å…±é€šç¥–å…ˆ\n",
    "    common_ancestors = ancestors_a & ancestors_b\n",
    "\n",
    "    if not common_ancestors:\n",
    "        return None\n",
    "\n",
    "    # æœ€ã‚‚æ·±ã„ï¼ˆå…·ä½“çš„ãªï¼‰å…±é€šç¥–å…ˆã‚’é¸æŠ\n",
    "    depth_cache = {}\n",
    "    lca = max(\n",
    "        common_ancestors,\n",
    "        key=lambda x: calculate_term_depth(x, mpo_data[\"hierarchy\"], depth_cache),\n",
    "    )\n",
    "\n",
    "    return lca\n",
    "\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "print(\"LCAè¨ˆç®—ãƒ†ã‚¹ãƒˆ:\")\n",
    "test_pairs = [\n",
    "    (\"cellular phenotype\", \"abnormal cell morphology\"),\n",
    "    (\"abnormal heart morphology\", \"abnormal blood vessel morphology\"),\n",
    "]\n",
    "\n",
    "for term_a, term_b in test_pairs:\n",
    "    lca_id = find_lowest_common_ancestor(term_a, term_b, mpo_data)\n",
    "    if lca_id:\n",
    "        lca_name = mpo_data[\"terms\"][lca_id][\"name\"]\n",
    "        print(f\"  '{term_a}' & '{term_b}' -> LCA: {lca_name}\")\n",
    "    else:\n",
    "        print(f\"  '{term_a}' & '{term_b}' -> LCA: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resnik Similarityè¨ˆç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_phenotype_ic(base_phenotype: str, ic_scores: dict) -> float:\n",
    "    \"\"\"åŸºæœ¬è¡¨ç¾å‹ã®ICå€¤ã‚’å–å¾—\"\"\"\n",
    "\n",
    "    return ic_scores.get(base_phenotype, 0.0)\n",
    "\n",
    "\n",
    "def get_children_in_data(term_id: str, mpo_data: dict, ic_scores: dict) -> list:\n",
    "    \"\"\"æŒ‡å®šã•ã‚ŒãŸã‚¿ãƒ¼ãƒ ã®å­ãƒãƒ¼ãƒ‰ã®ã†ã¡ã€å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹ã‚‚ã®ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™\"\"\"\n",
    "\n",
    "    children_in_data = []\n",
    "\n",
    "    # å…¨ã¦ã®åŸºæœ¬è¡¨ç¾å‹ã«ã¤ã„ã¦ã€ã“ã®ã‚¿ãƒ¼ãƒ ãŒç¥–å…ˆã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    for base_phenotype in ic_scores.keys():\n",
    "        # åŸºæœ¬è¡¨ç¾å‹ã®IDã‚’å–å¾—\n",
    "        phenotype_id = mpo_data[\"name_to_id\"].get(base_phenotype)\n",
    "        if phenotype_id:\n",
    "            # ã“ã®ã‚¿ãƒ¼ãƒ ãŒç¥–å…ˆã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "            ancestors = get_ancestors(phenotype_id, mpo_data[\"hierarchy\"])\n",
    "            if term_id in ancestors:\n",
    "                children_in_data.append(base_phenotype)\n",
    "\n",
    "    return children_in_data\n",
    "\n",
    "\n",
    "def estimate_lca_ic(\n",
    "    lca_id: str, mpo_data: dict, ic_scores: dict, ic_cache: dict = None\n",
    ") -> float:\n",
    "    \"\"\"LCAã®ICå€¤ã‚’æ¨å®šã™ã‚‹\"\"\"\n",
    "\n",
    "    if ic_cache is None:\n",
    "        ic_cache = {}\n",
    "\n",
    "    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—\n",
    "    if lca_id in ic_cache:\n",
    "        return ic_cache[lca_id]\n",
    "\n",
    "    lca_name = mpo_data[\"terms\"][lca_id][\"name\"]\n",
    "\n",
    "    # ç›´æ¥ICå€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "    if lca_name in ic_scores:\n",
    "        ic_cache[lca_id] = ic_scores[lca_name]\n",
    "        return ic_scores[lca_name]\n",
    "\n",
    "    # å­ãƒãƒ¼ãƒ‰ã®ã†ã¡ã€å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹ã‚‚ã®ã®ICå€¤ã‚’å–å¾—\n",
    "    children_in_data = get_children_in_data(lca_id, mpo_data, ic_scores)\n",
    "\n",
    "    if children_in_data:\n",
    "        # å­ãƒãƒ¼ãƒ‰ã®æœ€å°ICå€¤ã‚’ä½¿ç”¨ï¼ˆã‚ˆã‚Šä¸€èˆ¬çš„ãªæ¦‚å¿µãªã®ã§ï¼‰\n",
    "        child_ic_values = [ic_scores[child] for child in children_in_data]\n",
    "        min_child_ic = min(child_ic_values)\n",
    "\n",
    "        # éšå±¤ã®æ·±ã•ã«åŸºã¥ãèª¿æ•´\n",
    "        depth = calculate_term_depth(lca_id, mpo_data[\"hierarchy\"])\n",
    "        depth_factor = max(0.1, 1.0 - depth * 0.1)  # æ·±ã„ã»ã©å…·ä½“çš„ãªã®ã§é«˜ã„ICå€¤\n",
    "\n",
    "        estimated_ic = min_child_ic * depth_factor\n",
    "        ic_cache[lca_id] = estimated_ic\n",
    "        return estimated_ic\n",
    "\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é©åº¦ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤\n",
    "    default_ic = 2.0  # ä¸­ç¨‹åº¦ã®ä¸€èˆ¬æ€§ã‚’è¡¨ã™\n",
    "    ic_cache[lca_id] = default_ic\n",
    "    return default_ic\n",
    "\n",
    "\n",
    "def calculate_string_similarity(term_a: str, term_b: str) -> float:\n",
    "    \"\"\"æ–‡å­—åˆ—é¡ä¼¼åº¦ã«ã‚ˆã‚‹è£œå®Œè¨ˆç®—\"\"\"\n",
    "\n",
    "    # å…±é€šã®\"abnormal XXX morphology\"ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯\n",
    "    pattern_morphology = r\"^abnormal\\s+\\w+\\s+morphology$\"\n",
    "    if re.match(pattern_morphology, term_a) and re.match(pattern_morphology, term_b):\n",
    "        return 1.0  # å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã¯ä¸­ç¨‹åº¦ã®é¡ä¼¼åº¦\n",
    "\n",
    "    # å˜èªãƒ¬ãƒ™ãƒ«ã®Jaccardé¡ä¼¼åº¦\n",
    "    words_a = set(term_a.lower().split())\n",
    "    words_b = set(term_b.lower().split())\n",
    "\n",
    "    if not words_a or not words_b:\n",
    "        return 0.0\n",
    "\n",
    "    common_words = words_a & words_b\n",
    "    total_words = words_a | words_b\n",
    "\n",
    "    jaccard_sim = len(common_words) / len(total_words)\n",
    "\n",
    "    # é‡è¦ãªå˜èªã«ãƒœãƒ¼ãƒŠã‚¹\n",
    "    important_words = {\"abnormal\", \"morphology\", \"increased\", \"decreased\"}\n",
    "    important_common = common_words & important_words\n",
    "\n",
    "    if important_common:\n",
    "        bonus = len(important_common) * 0.3\n",
    "        jaccard_sim += bonus\n",
    "\n",
    "    return min(jaccard_sim, 3.0)  # æœ€å¤§3.0ã«åˆ¶é™\n",
    "\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥\n",
    "_ic_cache = {}\n",
    "\n",
    "\n",
    "def calculate_base_resnik_similarity(\n",
    "    term_a: str, term_b: str, mpo_data: dict, ic_scores: dict\n",
    ") -> float:\n",
    "    \"\"\"åŸºæœ¬è¡¨ç¾å‹ã®Resnik similarityè¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰\"\"\"\n",
    "\n",
    "    global _ic_cache\n",
    "\n",
    "    # åŒä¸€é …ç›®ã®å ´åˆ\n",
    "    if term_a == term_b:\n",
    "        return get_base_phenotype_ic(term_a, ic_scores)\n",
    "\n",
    "    # MPOã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼ã§LCAã‚’æ¢ã™\n",
    "    lca_id = find_lowest_common_ancestor(term_a, term_b, mpo_data)\n",
    "    ontology_similarity = 0.0\n",
    "\n",
    "    if lca_id is not None:\n",
    "        # LCAã®ICå€¤ã‚’å–å¾—ã¾ãŸã¯æ¨å®š\n",
    "        ontology_similarity = estimate_lca_ic(lca_id, mpo_data, ic_scores, _ic_cache)\n",
    "\n",
    "    # æ–‡å­—åˆ—é¡ä¼¼åº¦ã«ã‚ˆã‚‹è£œå®Œ\n",
    "    string_similarity = calculate_string_similarity(term_a, term_b)\n",
    "\n",
    "    # ã‚ˆã‚Šé«˜ã„é¡ä¼¼åº¦ã‚’æ¡ç”¨\n",
    "    return max(ontology_similarity, string_similarity)\n",
    "\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "print(\"æ”¹è‰¯ç‰ˆåŸºæœ¬Resnik similarity ãƒ†ã‚¹ãƒˆ:\")\n",
    "test_pairs = [\n",
    "    (\"preweaning lethality, complete penetrance\", \"abnormal heart morphology\"),\n",
    "    (\"abnormal skin morphology\", \"abnormal kidney morphology\"),\n",
    "    (\"abnormal spleen morphology\", \"abnormal kidney morphology\"),\n",
    "    (\"abnormal heart morphology\", \"abnormal blood vessel morphology\"),\n",
    "]\n",
    "\n",
    "for term_a, term_b in test_pairs:\n",
    "    similarity = calculate_base_resnik_similarity(term_a, term_b, mpo_data, ic_scores)\n",
    "    # å€‹åˆ¥ã®ICå€¤ã‚‚è¡¨ç¤º\n",
    "    ic_a = get_base_phenotype_ic(term_a, ic_scores)\n",
    "    ic_b = get_base_phenotype_ic(term_b, ic_scores)\n",
    "\n",
    "    # LCAæƒ…å ±ã‚‚è¡¨ç¤º\n",
    "    lca_id = find_lowest_common_ancestor(term_a, term_b, mpo_data)\n",
    "    if lca_id:\n",
    "        lca_name = mpo_data[\"terms\"][lca_id][\"name\"]\n",
    "        lca_ic = estimate_lca_ic(lca_id, mpo_data, ic_scores, _ic_cache)\n",
    "        print(\n",
    "            f\"  '{term_a}' (IC:{ic_a:.3f}) & '{term_b}' (IC:{ic_b:.3f}) -> {similarity:.4f}\"\n",
    "        )\n",
    "        print(f\"    LCA: '{lca_name}' (IC:{lca_ic:.3f})\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"  '{term_a}' (IC:{ic_a:.3f}) & '{term_b}' (IC:{ic_b:.3f}) -> {similarity:.4f}\"\n",
    "        )\n",
    "        print(\"    LCA: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. annotationæƒ…å ±ã‚’çµ±åˆã—ãŸResnik Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_partial_similarity(\n",
    "    annotation_type: str, value_a: str, value_b: str\n",
    ") -> float:\n",
    "    \"\"\"ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å€¤é–“ã®éƒ¨åˆ†çš„é¡ä¼¼åº¦\"\"\"\n",
    "\n",
    "    if annotation_type == \"genotype\":\n",
    "        genotype_similarity = {\n",
    "            (\"Hetero\", \"Homo\"): 0.1,\n",
    "            (\"Hemi\", \"Homo\"): 0.05,\n",
    "            (\"Hemi\", \"Hetero\"): 0.05,\n",
    "        }\n",
    "        key = tuple(sorted([value_a, value_b]))\n",
    "        return genotype_similarity.get(key, 0.0)\n",
    "\n",
    "    elif annotation_type == \"sex\":\n",
    "        return 0.0  # Male vs Female: å®Œå…¨ã«ç•°ãªã‚‹\n",
    "\n",
    "    elif annotation_type == \"life_stage\":\n",
    "        life_stage_similarity = {\n",
    "            (\"Early\", \"Interval\"): 0.1,\n",
    "            (\"Early\", \"Late\"): 0.02,\n",
    "            (\"Early\", \"Embryo\"): 0.02,\n",
    "            (\"Interval\", \"Late\"): 0.2,\n",
    "            (\"Interval\", \"Embryo\"): 0.02,\n",
    "            (\"Late\", \"Embryo\"): 0.02,\n",
    "        }\n",
    "        key = tuple(sorted([value_a, value_b]))\n",
    "        return life_stage_similarity.get(key, 0.0)\n",
    "\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_annotation_similarity(annotations_a: dict, annotations_b: dict) -> float:\n",
    "    \"\"\"ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–“ã®é¡ä¼¼åº¦è¨ˆç®—\"\"\"\n",
    "\n",
    "    # å„ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã®é‡ã¿\n",
    "    annotation_weights = {\"genotype\": 0.5, \"sex\": 0.3, \"life_stage\": 0.2}\n",
    "\n",
    "    total_weight = 0\n",
    "    similarity_sum = 0\n",
    "\n",
    "    for annotation_type in [\"genotype\", \"sex\", \"life_stage\"]:\n",
    "        if annotation_type in annotations_a or annotation_type in annotations_b:\n",
    "            weight = annotation_weights[annotation_type]\n",
    "            total_weight += weight\n",
    "\n",
    "            if annotation_type in annotations_a and annotation_type in annotations_b:\n",
    "                # ä¸¡æ–¹ã«ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å­˜åœ¨\n",
    "                if annotations_a[annotation_type] == annotations_b[annotation_type]:\n",
    "                    similarity_sum += weight * 1.0  # å®Œå…¨ä¸€è‡´\n",
    "                else:\n",
    "                    # éƒ¨åˆ†çš„é¡ä¼¼åº¦\n",
    "                    partial_sim = get_annotation_partial_similarity(\n",
    "                        annotation_type,\n",
    "                        annotations_a[annotation_type],\n",
    "                        annotations_b[annotation_type],\n",
    "                    )\n",
    "                    similarity_sum += weight * partial_sim\n",
    "            else:\n",
    "                # ç‰‡æ–¹ã®ã¿ã«ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å­˜åœ¨ - ä½ã„ãƒšãƒŠãƒ«ãƒ†ã‚£\n",
    "                similarity_sum += weight * 0.01\n",
    "\n",
    "    if total_weight == 0:\n",
    "        return 1.0  # ä¸¡æ–¹ã¨ã‚‚ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã—\n",
    "\n",
    "    return similarity_sum / total_weight\n",
    "\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "print(\"ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é¡ä¼¼åº¦ãƒ†ã‚¹ãƒˆ:\")\n",
    "test_cases = [\n",
    "    (\n",
    "        {\"genotype\": \"Homo\", \"life_stage\": \"Early\"},\n",
    "        {\"genotype\": \"Homo\", \"life_stage\": \"Early\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"genotype\": \"Homo\", \"life_stage\": \"Early\"},\n",
    "        {\"genotype\": \"Hetero\", \"life_stage\": \"Early\"},\n",
    "    ),\n",
    "    ({\"genotype\": \"Homo\", \"sex\": \"Male\"}, {\"genotype\": \"Homo\", \"sex\": \"Female\"}),\n",
    "]\n",
    "\n",
    "for ann_a, ann_b in test_cases:\n",
    "    similarity = calculate_annotation_similarity(ann_a, ann_b)\n",
    "    print(f\"  {ann_a} & {ann_b} -> {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_resnik_similarity(similarity: float, ic_scores: dict) -> float:\n",
    "    \"\"\"Resnik similarityã‚’0-1ã®ç¯„å›²ã«æ­£è¦åŒ–\"\"\"\n",
    "\n",
    "    # ICå€¤ã®æœ€å¤§å€¤ã‚’å–å¾—ï¼ˆæ­£è¦åŒ–ã®åŸºæº–ï¼‰\n",
    "    max_ic = max(ic_scores.values()) if ic_scores else 10.0\n",
    "\n",
    "    # 0-1ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "    normalized = similarity / max_ic\n",
    "\n",
    "    # 0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—\n",
    "    return max(0.0, min(1.0, normalized))\n",
    "\n",
    "\n",
    "def calculate_resnik_similarity(\n",
    "    phenotype_a: str,\n",
    "    phenotype_b: str,\n",
    "    mpo_data: dict,\n",
    "    ic_scores: dict,\n",
    ") -> float:\n",
    "    \"\"\"ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è€ƒæ…®å‹Resnik similarityè¨ˆç®—\"\"\"\n",
    "\n",
    "    # å®Œå…¨ä¸€è‡´ã®å ´åˆ\n",
    "    if phenotype_a == phenotype_b:\n",
    "        # ãƒ•ãƒ«è¡¨ç¾å‹ã‹ã‚‰åŸºæœ¬è¡¨ç¾å‹ã‚’æŠ½å‡ºã—ã¦ICå€¤ã‚’å–å¾—\n",
    "        base_phenotype, _ = parse_phenotype_annotations(phenotype_a)\n",
    "        similarity = ic_scores.get(base_phenotype, 0.0)\n",
    "    else:\n",
    "        # è¡¨ç¾å‹ã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆ†é›¢\n",
    "        base_a, annotations_a = parse_phenotype_annotations(phenotype_a)\n",
    "        base_b, annotations_b = parse_phenotype_annotations(phenotype_b)\n",
    "\n",
    "        # åŸºæœ¬è¡¨ç¾å‹ã®Resnik similarity\n",
    "        base_similarity = calculate_base_resnik_similarity(\n",
    "            base_a, base_b, mpo_data, ic_scores\n",
    "        )\n",
    "        normalized_base_similarity = normalize_resnik_similarity(\n",
    "            base_similarity, ic_scores\n",
    "        )\n",
    "\n",
    "        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é¡ä¼¼åº¦\n",
    "        annotation_similarity = calculate_annotation_similarity(\n",
    "            annotations_a, annotations_b\n",
    "        )\n",
    "\n",
    "        # çµ±åˆé¡ä¼¼åº¦è¨ˆç®—ï¼ˆåŸºæœ¬è¡¨ç¾å‹ã‚’é‡è¦–ï¼‰\n",
    "        if normalized_base_similarity < 0.01:\n",
    "            # åŸºæœ¬è¡¨ç¾å‹ãŒã»ã¼ç•°ãªã‚‹å ´åˆã€ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã»ã¼ç„¡è¦–\n",
    "            similarity = (\n",
    "                0.999 * base_similarity + 0.001 * annotation_similarity\n",
    "            )\n",
    "        elif normalized_base_similarity < 0.1:\n",
    "            # åŸºæœ¬è¡¨ç¾å‹ãŒå°‘ã—é¡ä¼¼ã™ã‚‹å ´åˆ\n",
    "            similarity = (\n",
    "                0.99 * base_similarity + 0.01 * annotation_similarity\n",
    "            )\n",
    "        else:\n",
    "            # é€šå¸¸ã®é‡ã¿\n",
    "            similarity = (\n",
    "                0.9 * base_similarity + 0.1 * annotation_similarity\n",
    "            )\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def calculate_resnik_similarity_batch(\n",
    "    phenotype_pairs: list, mpo_data: dict, ic_scores: dict\n",
    ") -> list:\n",
    "    \"\"\"è¤‡æ•°ã®è¡¨ç¾å‹ãƒšã‚¢ã«å¯¾ã—ã¦Resnik similarityã‚’ä¸€æ‹¬è¨ˆç®—\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for phenotype_a, phenotype_b in phenotype_pairs:\n",
    "        similarity = calculate_resnik_similarity(\n",
    "            phenotype_a, phenotype_b, mpo_data, ic_scores\n",
    "        )\n",
    "        results.append(\n",
    "            {\n",
    "                \"phenotype_a\": phenotype_a,\n",
    "                \"phenotype_b\": phenotype_b,\n",
    "                \"similarity\": similarity,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# # ãƒ†ã‚¹ãƒˆ: æ­£è¦åŒ–ã‚ã‚Šã¨ãªã—ã®æ¯”è¼ƒ\n",
    "# print(\"=== æ­£è¦åŒ–ã‚ã‚Šãªã—ã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆ ===\")\n",
    "\n",
    "# test_phenotype_pairs = [\n",
    "#     (\n",
    "#         \"preweaning lethality, complete penetrance (Homo, Early)\",\n",
    "#         \"abnormal heart morphology (Homo, Early)\",\n",
    "#     ),\n",
    "#     (\n",
    "#         \"abnormal skin morphology (Homo, Early)\",\n",
    "#         \"abnormal kidney morphology (Homo, Early)\",\n",
    "#     ),\n",
    "#     (\n",
    "#         \"abnormal heart morphology (Homo, Early)\",\n",
    "#         \"abnormal blood vessel morphology (Homo, Early)\",\n",
    "#     ),\n",
    "#     (\n",
    "#         \"hyperactivity (Homo, Early)\",\n",
    "#         \"hyperactivity (Hetero, Early)\",\n",
    "#     ),\n",
    "#     (\n",
    "#         \"hyperactivity (Homo, Early)\",\n",
    "#         \"hyperactivity (Homo, Early)\",\n",
    "#     ),  # åŒä¸€è¡¨ç¾å‹\n",
    "# ]\n",
    "\n",
    "# print(f\"\\nICå€¤ã®ç¯„å›²: {min(ic_scores.values()):.3f} - {max(ic_scores.values()):.3f}\")\n",
    "# print(f\"æ­£è¦åŒ–åŸºæº–ï¼ˆæœ€å¤§ICå€¤ï¼‰: {max(ic_scores.values()):.3f}\")\n",
    "\n",
    "# print(\"\\nçµæœ:\")\n",
    "# for phenotype_a, phenotype_b in test_phenotype_pairs:\n",
    "#     # æ­£è¦åŒ–ãªã—\n",
    "#     raw_similarity = calculate_resnik_similarity(\n",
    "#         phenotype_a, phenotype_b, mpo_data, ic_scores\n",
    "#     )\n",
    "\n",
    "#     # æ­£è¦åŒ–ã‚ã‚Š\n",
    "#     normalized_similarity = calculate_resnik_similarity(\n",
    "#         phenotype_a, phenotype_b, mpo_data, ic_scores\n",
    "#     )\n",
    "\n",
    "#     print(f\"\\nğŸ“ {phenotype_a}\")\n",
    "#     print(f\"   vs\")\n",
    "#     print(f\"   {phenotype_b}\")\n",
    "#     print(f\"   Raw: {raw_similarity:.4f} -> Normalized: {normalized_similarity:.4f}\")\n",
    "\n",
    "#     # è©³ç´°æƒ…å ±\n",
    "#     base_a, ann_a = parse_phenotype_annotations(phenotype_a)\n",
    "#     base_b, ann_b = parse_phenotype_annotations(phenotype_b)\n",
    "#     ic_a = get_base_phenotype_ic(base_a, ic_scores)\n",
    "#     ic_b = get_base_phenotype_ic(base_b, ic_scores)\n",
    "#     print(f\"   åŸºæœ¬è¡¨ç¾å‹IC: {ic_a:.3f} vs {ic_b:.3f}\")\n",
    "\n",
    "# print(f\"\\nğŸ¯ æ­£è¦åŒ–ã®åˆ©ç‚¹:\")\n",
    "# print(f\"   1. é¡ä¼¼åº¦ãŒ0-1ã®ç›´æ„Ÿçš„ãªç¯„å›²ã«åã¾ã‚‹\")\n",
    "# print(f\"   2. ä»–ã®é¡ä¼¼åº¦æŒ‡æ¨™ã¨ã®æ¯”è¼ƒãŒå®¹æ˜“\")\n",
    "# print(f\"   3. æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã®åˆ©ç”¨ã«é©ã—ã¦ã„ã‚‹\")\n",
    "# print(f\"   4. è¦–è¦šåŒ–ã‚„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã®è¡¨ç¤ºãŒåˆ†ã‹ã‚Šã‚„ã™ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Jaccardã¨Resnikã®æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(phenotype_a: str, phenotype_b: str) -> float:\n",
    "    \"\"\"å¾“æ¥ã®Jaccard similarityï¼ˆå‚è€ƒç”¨ï¼‰\"\"\"\n",
    "    return 1.0 if phenotype_a == phenotype_b else 0.0\n",
    "\n",
    "\n",
    "print(\"Jaccard vs Resnik Similarity æ¯”è¼ƒï¼ˆæ­£è¦åŒ–ç‰ˆï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_pairs = [\n",
    "    (\n",
    "        \"preweaning lethality, complete penetrance (Homo, Early)\",\n",
    "        \"abnormal heart morphology (Homo, Early)\",\n",
    "    ),\n",
    "    (\n",
    "        \"abnormal heart morphology (Homo, Early)\",\n",
    "        \"abnormal blood vessel morphology (Homo, Early)\",\n",
    "    ),\n",
    "    (\"hyperactivity (Homo, Early)\", \"hyperactivity (Homo, Early)\"),  # åŒä¸€è¡¨ç¾å‹\n",
    "]\n",
    "\n",
    "print(f\"\\næ­£è¦åŒ–åŸºæº–: æœ€å¤§ICå€¤ = {max(ic_scores.values()):.3f}\")\n",
    "\n",
    "for phenotype_a, phenotype_b in comparison_pairs:\n",
    "    jaccard_sim = jaccard_similarity(phenotype_a, phenotype_b)\n",
    "\n",
    "    # æ­£è¦åŒ–ã•ã‚ŒãŸResnik similarity\n",
    "    resnik_sim_normalized = calculate_resnik_similarity(\n",
    "        phenotype_a, phenotype_b, mpo_data, ic_scores, normalize=True\n",
    "    )\n",
    "\n",
    "    # æ¯”è¼ƒç”¨ã«ç”Ÿã®Resnik similarity\n",
    "    resnik_sim_raw = calculate_resnik_similarity(\n",
    "        phenotype_a, phenotype_b, mpo_data, ic_scores, normalize=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\nğŸ“ {phenotype_a}\")\n",
    "    print(\"   vs\")\n",
    "    print(f\"   {phenotype_b}\")\n",
    "    print(\n",
    "        f\"   ğŸ“Š Jaccard: {jaccard_sim:.4f}  |  Resnik (æ­£è¦åŒ–): {resnik_sim_normalized:.4f}  |  Resnik (ç”Ÿ): {resnik_sim_raw:.4f}\"\n",
    "    )\n",
    "\n",
    "    # æ”¹å–„åŠ¹æœã‚’è©•ä¾¡\n",
    "    if jaccard_sim == 0.0 and resnik_sim_normalized < 0.1:\n",
    "        print(\"   âœ… æ”¹å–„æˆåŠŸ: éé–¢é€£è¡¨ç¾å‹ã®é¡ä¼¼åº¦ãŒé©åˆ‡ã«ä½ã„\")\n",
    "    elif jaccard_sim == 0.0 and resnik_sim_normalized >= 0.1:\n",
    "        print(\"   ğŸ” æ³¨ç›®: é–¢é€£æ€§ã®ã‚ã‚‹è¡¨ç¾å‹ã¨ã—ã¦æ¤œå‡º\")\n",
    "    elif jaccard_sim == 1.0:\n",
    "        print(\"   ğŸ¯ åŒä¸€è¡¨ç¾å‹: ICå€¤ã‚’æ´»ç”¨\")\n",
    "\n",
    "print(\"\\nğŸ¯ æ­£è¦åŒ–Resnik Similarity ã®åˆ©ç‚¹:\")\n",
    "print(\"   1. é »å‡ºè¡¨ç¾å‹ï¼ˆpreweaning lethalityç­‰ï¼‰ã®åã‚Šã‚’è»½æ¸›\")\n",
    "print(\"   2. ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼éšå±¤ã‚’è€ƒæ…®ã—ãŸæ„å‘³çš„é¡ä¼¼åº¦\")\n",
    "print(\"   3. ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’é©åˆ‡ã«è€ƒæ…®\")\n",
    "print(\"   4. 0-1ã®ç›´æ„Ÿçš„ãªç¯„å›²ã§é¡ä¼¼åº¦ã‚’è¡¨ç¾\")\n",
    "print(\"   5. Jaccardãªã©ä»–ã®é¡ä¼¼åº¦æŒ‡æ¨™ã¨ã®æ¯”è¼ƒãŒå®¹æ˜“\")\n",
    "\n",
    "# çµ±è¨ˆæƒ…å ±\n",
    "print(\"\\nğŸ“Š é¡ä¼¼åº¦åˆ†å¸ƒã®çµ±è¨ˆ:\")\n",
    "similarities = []\n",
    "for phenotype_a, phenotype_b in comparison_pairs:\n",
    "    sim = calculate_resnik_similarity(\n",
    "        phenotype_a, phenotype_b, mpo_data, ic_scores, normalize=True\n",
    "    )\n",
    "    similarities.append(sim)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(f\"   å¹³å‡: {np.mean(similarities):.4f}\")\n",
    "print(f\"   æ¨™æº–åå·®: {np.std(similarities):.4f}\")\n",
    "print(f\"   æœ€å°å€¤: {np.min(similarities):.4f}\")\n",
    "print(f\"   æœ€å¤§å€¤: {np.max(similarities):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. çµæœã®ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœã‚’ä¿å­˜\n",
    "output_dir = Path(\"data/resnik_similarity_simplified\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. MPOãƒ‡ãƒ¼ã‚¿ä¿å­˜\n",
    "with open(output_dir / \"mpo_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mpo_data, f)\n",
    "\n",
    "# 2. åŸºæœ¬è¡¨ç¾å‹ã®IC scoresä¿å­˜\n",
    "with open(output_dir / \"ic_scores.json\", \"w\") as f:\n",
    "    json.dump(ic_scores, f, indent=2)\n",
    "\n",
    "# 3. ãƒ•ãƒ«è¡¨ç¾å‹ã®é »åº¦ãƒ‡ãƒ¼ã‚¿ä¿å­˜\n",
    "with open(output_dir / \"phenotype_frequencies.json\", \"w\") as f:\n",
    "    json.dump(phenotype_frequencies, f, indent=2)\n",
    "\n",
    "# 4. åŸºæœ¬è¡¨ç¾å‹ã®é »åº¦ãƒ‡ãƒ¼ã‚¿ä¿å­˜\n",
    "with open(output_dir / \"base_phenotype_frequencies.json\", \"w\") as f:\n",
    "    json.dump(base_phenotype_frequencies, f, indent=2)\n",
    "\n",
    "# 5. æ­£è¦åŒ–ã®ãŸã‚ã®çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜\n",
    "normalization_stats = {\n",
    "    \"max_ic\": float(max(ic_scores.values())),\n",
    "    \"min_ic\": float(min(ic_scores.values())),\n",
    "    \"mean_ic\": float(sum(ic_scores.values()) / len(ic_scores)),\n",
    "    \"num_base_phenotypes\": len(ic_scores),\n",
    "}\n",
    "\n",
    "with open(output_dir / \"normalization_stats.json\", \"w\") as f:\n",
    "    json.dump(normalization_stats, f, indent=2)\n",
    "\n",
    "print(f\"Simplified Resnik similarity data saved to {output_dir}/\")\n",
    "print(\"Files saved:\")\n",
    "for file_path in output_dir.glob(\"*\"):\n",
    "    print(f\"  - {file_path.name}\")\n",
    "\n",
    "print(\"\\nâœ… ç°¡ç•¥åŒ–å®Ÿè£…å®Œäº†!\")\n",
    "print(\"ä¸»è¦ãªå¤‰æ›´ç‚¹:\")\n",
    "print(\"  - ICå€¤ã¯åŸºæœ¬è¡¨ç¾å‹ã®ã¿ã‚’ã‚­ãƒ¼ã¨ã—ã¦è¨ˆç®—\")\n",
    "print(\"  - LCA ICå€¤æ¨å®šæ©Ÿèƒ½ã‚’è¿½åŠ \")\n",
    "print(\"  - 0-1æ­£è¦åŒ–æ©Ÿèƒ½ã‚’å®Ÿè£…\")\n",
    "print(\"  - ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã¯åˆ¥é€”è€ƒæ…®\")\n",
    "print(f\"  - {len(base_phenotype_frequencies)} unique base phenotypes\")\n",
    "print(f\"  - {len(phenotype_frequencies)} full phenotype observations\")\n",
    "\n",
    "print(\"\\nä¸»è¦é–¢æ•°:\")\n",
    "print(\"  - calculate_resnik_similarity(): ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆæ­£è¦åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãï¼‰\")\n",
    "print(\"  - normalize_resnik_similarity(): 0-1æ­£è¦åŒ–é–¢æ•°\")\n",
    "print(\"  - calculate_resnik_similarity_batch(): ä¸€æ‹¬è¨ˆç®—é–¢æ•°\")\n",
    "print(\"  - parse_phenotype_annotations(): è¡¨ç¾å‹è§£æ\")\n",
    "print(\"  - calculate_base_resnik_similarity(): åŸºæœ¬é¡ä¼¼åº¦\")\n",
    "print(\"  - calculate_annotation_similarity(): ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é¡ä¼¼åº¦\")\n",
    "\n",
    "print(\"\\nä½¿ç”¨ä¾‹:\")\n",
    "print(\"  # æ­£è¦åŒ–ã‚ã‚Šï¼ˆæ¨å¥¨ï¼‰\")\n",
    "print(\n",
    "    \"  similarity = calculate_resnik_similarity(phenotype_a, phenotype_b, mpo_data, ic_scores, normalize=True)\"\n",
    ")\n",
    "print(\"  # æ­£è¦åŒ–ãªã—\")\n",
    "print(\n",
    "    \"  similarity = calculate_resnik_similarity(phenotype_a, phenotype_b, mpo_data, ic_scores, normalize=False)\"\n",
    ")\n",
    "\n",
    "print(f\"\\næ­£è¦åŒ–çµ±è¨ˆ:\")\n",
    "print(\n",
    "    f\"  - ICå€¤ç¯„å›²: {normalization_stats['min_ic']:.3f} - {normalization_stats['max_ic']:.3f}\"\n",
    ")\n",
    "print(f\"  - å¹³å‡ICå€¤: {normalization_stats['mean_ic']:.3f}\")\n",
    "print(f\"  - æ­£è¦åŒ–å¾Œã®é¡ä¼¼åº¦ç¯„å›²: 0.000 - 1.000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tsumugi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
