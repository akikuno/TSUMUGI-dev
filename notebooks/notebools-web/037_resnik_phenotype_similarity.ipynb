{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnik Phenotype Similarity (Simplified)\n",
    "\n",
    "このノートブックは、036_resnik_phenotype_similarity.ipynbをシンプルで可読性の高い実装にリファクタリングしたものです。\n",
    "- クラスを使わず関数ベースで実装\n",
    "- 煩雑な部分を簡素化\n",
    "- アノテーション（Genotype, Sex, Life stage）を考慮したResnik similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロジェクトルートに移動\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "while not Path(\"LICENSE\").exists():\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "print(f\"Project root: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MPOオントロジーの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mpo_obo(obo_file_path: str) -> dict:\n",
    "    \"\"\"MPO OBOファイルをパースしてオントロジー構造を返す\"\"\"\n",
    "\n",
    "    with open(obo_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # ターム情報を格納\n",
    "    terms = {}  # MP_ID -> {name, is_a, ...}\n",
    "    name_to_id = {}  # name -> MP_ID\n",
    "    hierarchy = defaultdict(set)  # child_id -> {parent_ids}\n",
    "\n",
    "    # Termブロックを分割\n",
    "    term_blocks = content.split(\"[Term]\")[1:]\n",
    "\n",
    "    for block in term_blocks:\n",
    "        lines = [line.strip() for line in block.strip().split(\"\\n\") if line.strip()]\n",
    "\n",
    "        term_data = {\"is_a\": [], \"is_obsolete\": False}\n",
    "\n",
    "        for line in lines:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "\n",
    "            key, value = line.split(\":\", 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if key == \"id\":\n",
    "                term_data[\"id\"] = value\n",
    "            elif key == \"name\":\n",
    "                term_data[\"name\"] = value\n",
    "            elif key == \"is_a\":\n",
    "                parent_id = value.split(\" !\")[0].strip()\n",
    "                term_data[\"is_a\"].append(parent_id)\n",
    "            elif key == \"is_obsolete\":\n",
    "                term_data[\"is_obsolete\"] = value.lower() == \"true\"\n",
    "\n",
    "        # 有効なタームのみ追加\n",
    "        if \"id\" in term_data and \"name\" in term_data and not term_data[\"is_obsolete\"]:\n",
    "            term_id = term_data[\"id\"]\n",
    "            terms[term_id] = term_data\n",
    "            name_to_id[term_data[\"name\"]] = term_id\n",
    "\n",
    "            # 階層関係を構築\n",
    "            for parent_id in term_data[\"is_a\"]:\n",
    "                hierarchy[term_id].add(parent_id)\n",
    "\n",
    "    return {\"terms\": terms, \"name_to_id\": name_to_id, \"hierarchy\": hierarchy}\n",
    "\n",
    "\n",
    "# MPOオントロジーをパース\n",
    "mpo_data = parse_mpo_obo(\"data/ontology/mp.obo\")\n",
    "print(f\"Parsed {len(mpo_data['terms'])} MP terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 表現型の解析とアノテーション分離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_phenotype_annotations(phenotype: str) -> tuple:\n",
    "    \"\"\"表現型文字列から基本項目とアノテーションを分離\"\"\"\n",
    "\n",
    "    # パターン: \"基本表現型 (アノテーション)\"\n",
    "    pattern = r\"^(.+?)\\s*\\(([^)]+)\\)$\"\n",
    "    match = re.match(pattern, phenotype.strip())\n",
    "\n",
    "    if not match:\n",
    "        return phenotype.strip(), {}\n",
    "\n",
    "    base_term = match.group(1).strip()\n",
    "    annotation_str = match.group(2).strip()\n",
    "\n",
    "    # アノテーションを分類\n",
    "    annotations = {}\n",
    "\n",
    "    # 複数アノテーションはカンマで区切られる\n",
    "    parts = [part.strip() for part in annotation_str.split(\",\")]\n",
    "\n",
    "    for part in parts:\n",
    "        if part in [\"Homo\", \"Hetero\", \"Hemi\"]:\n",
    "            annotations[\"genotype\"] = part\n",
    "        elif part in [\"Male\", \"Female\"]:\n",
    "            annotations[\"sex\"] = part\n",
    "        elif part in [\"Early\", \"Late\", \"Embryo\", \"Interval\"]:\n",
    "            annotations[\"life_stage\"] = part\n",
    "\n",
    "    return base_term, annotations\n",
    "\n",
    "\n",
    "# テスト\n",
    "test_phenotypes = [\n",
    "    \"preweaning lethality, complete penetrance (Homo, Early)\",\n",
    "    \"abnormal skin morphology (Homo, Early)\",\n",
    "    \"hyperactivity (Homo, Early)\",\n",
    "]\n",
    "\n",
    "print(\"表現型解析テスト:\")\n",
    "for phenotype in test_phenotypes:\n",
    "    base, annotations = parse_phenotype_annotations(phenotype)\n",
    "    print(f\"  {phenotype}\")\n",
    "    print(f\"    基本: '{base}'\")\n",
    "    print(f\"    アノテーション: {annotations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 表現型頻度とIC値の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_phenotype_frequencies(symbol_mptermname_path: str) -> dict:\n",
    "    \"\"\"遺伝子→表現型マッピングから各表現型の出現頻度を計算\"\"\"\n",
    "\n",
    "    with open(symbol_mptermname_path, \"r\") as f:\n",
    "        symbol_mptermname = json.load(f)\n",
    "\n",
    "    phenotype_counts = Counter()\n",
    "\n",
    "    for gene, phenotypes in symbol_mptermname.items():\n",
    "        if phenotypes:\n",
    "            for phenotype in phenotypes:\n",
    "                if phenotype.strip():\n",
    "                    phenotype_counts[phenotype.strip()] += 1\n",
    "\n",
    "    return dict(phenotype_counts)\n",
    "\n",
    "\n",
    "def calculate_base_phenotype_frequencies(phenotype_frequencies: dict) -> dict:\n",
    "    \"\"\"基本表現型（アノテーション除去）の頻度を計算\"\"\"\n",
    "\n",
    "    base_phenotype_counts = Counter()\n",
    "\n",
    "    for full_phenotype, frequency in phenotype_frequencies.items():\n",
    "        # アノテーションを除去して基本表現型を取得\n",
    "        base_phenotype, _ = parse_phenotype_annotations(full_phenotype)\n",
    "        base_phenotype_counts[base_phenotype] += frequency\n",
    "\n",
    "    return dict(base_phenotype_counts)\n",
    "\n",
    "\n",
    "def calculate_information_content(base_phenotype_frequencies: dict) -> dict:\n",
    "    \"\"\"各基本表現型のInformation Content (IC)を計算\"\"\"\n",
    "\n",
    "    total_observations = sum(base_phenotype_frequencies.values())\n",
    "    ic_scores = {}\n",
    "\n",
    "    for base_phenotype, frequency in base_phenotype_frequencies.items():\n",
    "        probability = frequency / total_observations\n",
    "        ic_score = -math.log(probability) if probability > 0 else 0.0\n",
    "        ic_scores[base_phenotype] = ic_score\n",
    "\n",
    "    return ic_scores\n",
    "\n",
    "\n",
    "# 頻度とIC値を計算\n",
    "phenotype_frequencies = calculate_phenotype_frequencies(\n",
    "    \"data/annotation/symbol_mptermname.json\"\n",
    ")\n",
    "\n",
    "# 基本表現型の頻度を計算（アノテーション除去）\n",
    "base_phenotype_frequencies = calculate_base_phenotype_frequencies(phenotype_frequencies)\n",
    "\n",
    "# 基本表現型のIC値を計算\n",
    "ic_scores = calculate_information_content(base_phenotype_frequencies)\n",
    "\n",
    "print(f\"Found {len(phenotype_frequencies)} unique full phenotypes\")\n",
    "print(f\"Found {len(base_phenotype_frequencies)} unique base phenotypes\")\n",
    "print(f\"Total phenotype observations: {sum(phenotype_frequencies.values())}\")\n",
    "\n",
    "# 頻出表現型TOP5を確認（フル表現型）\n",
    "top_phenotypes = sorted(\n",
    "    phenotype_frequencies.items(), key=lambda x: x[1], reverse=True\n",
    ")[:5]\n",
    "print(\"\\nTop 5 most frequent full phenotypes:\")\n",
    "for phenotype, count in top_phenotypes:\n",
    "    print(f\"  {count:4d}: {phenotype}\")\n",
    "\n",
    "# 頻出基本表現型TOP5を確認\n",
    "top_base_phenotypes = sorted(\n",
    "    base_phenotype_frequencies.items(), key=lambda x: x[1], reverse=True\n",
    ")[:5]\n",
    "print(\"\\nTop 5 most frequent base phenotypes:\")\n",
    "for base_phenotype, count in top_base_phenotypes:\n",
    "    ic_score = ic_scores[base_phenotype]\n",
    "    print(f\"  {count:4d}: {base_phenotype} (IC: {ic_score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. オントロジー階層とLCA計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ancestors(term_id: str, hierarchy: dict) -> set:\n",
    "    \"\"\"指定されたtermの全祖先を取得\"\"\"\n",
    "\n",
    "    ancestors = set()\n",
    "    stack = [term_id]\n",
    "\n",
    "    while stack:\n",
    "        current = stack.pop()\n",
    "        for parent in hierarchy.get(current, set()):\n",
    "            if parent not in ancestors:\n",
    "                ancestors.add(parent)\n",
    "                stack.append(parent)\n",
    "\n",
    "    return ancestors\n",
    "\n",
    "\n",
    "def calculate_term_depth(\n",
    "    term_id: str, hierarchy: dict, depth_cache: dict = None\n",
    ") -> int:\n",
    "    \"\"\"ルートからの深さを計算（キャッシュ付き）\"\"\"\n",
    "\n",
    "    if depth_cache is None:\n",
    "        depth_cache = {}\n",
    "\n",
    "    if term_id in depth_cache:\n",
    "        return depth_cache[term_id]\n",
    "\n",
    "    if not hierarchy.get(term_id):\n",
    "        depth_cache[term_id] = 0\n",
    "        return 0\n",
    "\n",
    "    max_depth = 0\n",
    "    for parent in hierarchy[term_id]:\n",
    "        parent_depth = calculate_term_depth(parent, hierarchy, depth_cache)\n",
    "        max_depth = max(max_depth, parent_depth + 1)\n",
    "\n",
    "    depth_cache[term_id] = max_depth\n",
    "    return max_depth\n",
    "\n",
    "\n",
    "def find_lowest_common_ancestor(term_a: str, term_b: str, mpo_data: dict) -> str | None:\n",
    "    \"\"\"2つのtermの最下位共通祖先（LCA）を見つける\"\"\"\n",
    "\n",
    "    # 名前からIDに変換\n",
    "    id_a = mpo_data[\"name_to_id\"].get(term_a, term_a)\n",
    "    id_b = mpo_data[\"name_to_id\"].get(term_b, term_b)\n",
    "\n",
    "    if id_a not in mpo_data[\"terms\"] or id_b not in mpo_data[\"terms\"]:\n",
    "        return None\n",
    "\n",
    "    # 両方の祖先を取得\n",
    "    ancestors_a = get_ancestors(id_a, mpo_data[\"hierarchy\"])\n",
    "    ancestors_a.add(id_a)\n",
    "\n",
    "    ancestors_b = get_ancestors(id_b, mpo_data[\"hierarchy\"])\n",
    "    ancestors_b.add(id_b)\n",
    "\n",
    "    # 共通祖先\n",
    "    common_ancestors = ancestors_a & ancestors_b\n",
    "\n",
    "    if not common_ancestors:\n",
    "        return None\n",
    "\n",
    "    # 最も深い（具体的な）共通祖先を選択\n",
    "    depth_cache = {}\n",
    "    lca = max(\n",
    "        common_ancestors,\n",
    "        key=lambda x: calculate_term_depth(x, mpo_data[\"hierarchy\"], depth_cache),\n",
    "    )\n",
    "\n",
    "    return lca\n",
    "\n",
    "\n",
    "# テスト\n",
    "print(\"LCA計算テスト:\")\n",
    "test_pairs = [\n",
    "    (\"cellular phenotype\", \"abnormal cell morphology\"),\n",
    "    (\"abnormal heart morphology\", \"abnormal blood vessel morphology\"),\n",
    "]\n",
    "\n",
    "for term_a, term_b in test_pairs:\n",
    "    lca_id = find_lowest_common_ancestor(term_a, term_b, mpo_data)\n",
    "    if lca_id:\n",
    "        lca_name = mpo_data[\"terms\"][lca_id][\"name\"]\n",
    "        print(f\"  '{term_a}' & '{term_b}' -> LCA: {lca_name}\")\n",
    "    else:\n",
    "        print(f\"  '{term_a}' & '{term_b}' -> LCA: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resnik Similarity計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_phenotype_ic(base_phenotype: str, ic_scores: dict) -> float:\n",
    "    \"\"\"基本表現型のIC値を取得\"\"\"\n",
    "\n",
    "    return ic_scores.get(base_phenotype, 0.0)\n",
    "\n",
    "\n",
    "def get_children_in_data(term_id: str, mpo_data: dict, ic_scores: dict) -> list:\n",
    "    \"\"\"指定されたタームの子ノードのうち、実際のデータに存在するもののリストを返す\"\"\"\n",
    "\n",
    "    children_in_data = []\n",
    "\n",
    "    # 全ての基本表現型について、このタームが祖先かどうかチェック\n",
    "    for base_phenotype in ic_scores.keys():\n",
    "        # 基本表現型のIDを取得\n",
    "        phenotype_id = mpo_data[\"name_to_id\"].get(base_phenotype)\n",
    "        if phenotype_id:\n",
    "            # このタームが祖先に含まれるかチェック\n",
    "            ancestors = get_ancestors(phenotype_id, mpo_data[\"hierarchy\"])\n",
    "            if term_id in ancestors:\n",
    "                children_in_data.append(base_phenotype)\n",
    "\n",
    "    return children_in_data\n",
    "\n",
    "\n",
    "def estimate_lca_ic(\n",
    "    lca_id: str, mpo_data: dict, ic_scores: dict, ic_cache: dict = None\n",
    ") -> float:\n",
    "    \"\"\"LCAのIC値を推定する\"\"\"\n",
    "\n",
    "    if ic_cache is None:\n",
    "        ic_cache = {}\n",
    "\n",
    "    # キャッシュから取得\n",
    "    if lca_id in ic_cache:\n",
    "        return ic_cache[lca_id]\n",
    "\n",
    "    lca_name = mpo_data[\"terms\"][lca_id][\"name\"]\n",
    "\n",
    "    # 直接IC値が存在する場合\n",
    "    if lca_name in ic_scores:\n",
    "        ic_cache[lca_id] = ic_scores[lca_name]\n",
    "        return ic_scores[lca_name]\n",
    "\n",
    "    # 子ノードのうち、実際のデータに存在するもののIC値を取得\n",
    "    children_in_data = get_children_in_data(lca_id, mpo_data, ic_scores)\n",
    "\n",
    "    if children_in_data:\n",
    "        # 子ノードの最小IC値を使用（より一般的な概念なので）\n",
    "        child_ic_values = [ic_scores[child] for child in children_in_data]\n",
    "        min_child_ic = min(child_ic_values)\n",
    "\n",
    "        # 階層の深さに基づく調整\n",
    "        depth = calculate_term_depth(lca_id, mpo_data[\"hierarchy\"])\n",
    "        depth_factor = max(0.1, 1.0 - depth * 0.1)  # 深いほど具体的なので高いIC値\n",
    "\n",
    "        estimated_ic = min_child_ic * depth_factor\n",
    "        ic_cache[lca_id] = estimated_ic\n",
    "        return estimated_ic\n",
    "\n",
    "    # フォールバック: 適度なデフォルト値\n",
    "    default_ic = 2.0  # 中程度の一般性を表す\n",
    "    ic_cache[lca_id] = default_ic\n",
    "    return default_ic\n",
    "\n",
    "\n",
    "def calculate_string_similarity(term_a: str, term_b: str) -> float:\n",
    "    \"\"\"文字列類似度による補完計算\"\"\"\n",
    "\n",
    "    # 共通の\"abnormal XXX morphology\"パターンチェック\n",
    "    pattern_morphology = r\"^abnormal\\s+\\w+\\s+morphology$\"\n",
    "    if re.match(pattern_morphology, term_a) and re.match(pattern_morphology, term_b):\n",
    "        return 1.0  # 共通パターンには中程度の類似度\n",
    "\n",
    "    # 単語レベルのJaccard類似度\n",
    "    words_a = set(term_a.lower().split())\n",
    "    words_b = set(term_b.lower().split())\n",
    "\n",
    "    if not words_a or not words_b:\n",
    "        return 0.0\n",
    "\n",
    "    common_words = words_a & words_b\n",
    "    total_words = words_a | words_b\n",
    "\n",
    "    jaccard_sim = len(common_words) / len(total_words)\n",
    "\n",
    "    # 重要な単語にボーナス\n",
    "    important_words = {\"abnormal\", \"morphology\", \"increased\", \"decreased\"}\n",
    "    important_common = common_words & important_words\n",
    "\n",
    "    if important_common:\n",
    "        bonus = len(important_common) * 0.3\n",
    "        jaccard_sim += bonus\n",
    "\n",
    "    return min(jaccard_sim, 3.0)  # 最大3.0に制限\n",
    "\n",
    "\n",
    "# グローバルキャッシュ\n",
    "_ic_cache = {}\n",
    "\n",
    "\n",
    "def calculate_base_resnik_similarity(\n",
    "    term_a: str, term_b: str, mpo_data: dict, ic_scores: dict\n",
    ") -> float:\n",
    "    \"\"\"基本表現型のResnik similarity計算（改良版）\"\"\"\n",
    "\n",
    "    global _ic_cache\n",
    "\n",
    "    # 同一項目の場合\n",
    "    if term_a == term_b:\n",
    "        return get_base_phenotype_ic(term_a, ic_scores)\n",
    "\n",
    "    # MPOオントロジーでLCAを探す\n",
    "    lca_id = find_lowest_common_ancestor(term_a, term_b, mpo_data)\n",
    "    ontology_similarity = 0.0\n",
    "\n",
    "    if lca_id is not None:\n",
    "        # LCAのIC値を取得または推定\n",
    "        ontology_similarity = estimate_lca_ic(lca_id, mpo_data, ic_scores, _ic_cache)\n",
    "\n",
    "    # 文字列類似度による補完\n",
    "    string_similarity = calculate_string_similarity(term_a, term_b)\n",
    "\n",
    "    # より高い類似度を採用\n",
    "    return max(ontology_similarity, string_similarity)\n",
    "\n",
    "\n",
    "# テスト\n",
    "print(\"改良版基本Resnik similarity テスト:\")\n",
    "test_pairs = [\n",
    "    (\"preweaning lethality, complete penetrance\", \"abnormal heart morphology\"),\n",
    "    (\"abnormal skin morphology\", \"abnormal kidney morphology\"),\n",
    "    (\"abnormal spleen morphology\", \"abnormal kidney morphology\"),\n",
    "    (\"abnormal heart morphology\", \"abnormal blood vessel morphology\"),\n",
    "]\n",
    "\n",
    "for term_a, term_b in test_pairs:\n",
    "    similarity = calculate_base_resnik_similarity(term_a, term_b, mpo_data, ic_scores)\n",
    "    # 個別のIC値も表示\n",
    "    ic_a = get_base_phenotype_ic(term_a, ic_scores)\n",
    "    ic_b = get_base_phenotype_ic(term_b, ic_scores)\n",
    "\n",
    "    # LCA情報も表示\n",
    "    lca_id = find_lowest_common_ancestor(term_a, term_b, mpo_data)\n",
    "    if lca_id:\n",
    "        lca_name = mpo_data[\"terms\"][lca_id][\"name\"]\n",
    "        lca_ic = estimate_lca_ic(lca_id, mpo_data, ic_scores, _ic_cache)\n",
    "        print(\n",
    "            f\"  '{term_a}' (IC:{ic_a:.3f}) & '{term_b}' (IC:{ic_b:.3f}) -> {similarity:.4f}\"\n",
    "        )\n",
    "        print(f\"    LCA: '{lca_name}' (IC:{lca_ic:.3f})\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"  '{term_a}' (IC:{ic_a:.3f}) & '{term_b}' (IC:{ic_b:.3f}) -> {similarity:.4f}\"\n",
    "        )\n",
    "        print(\"    LCA: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. annotation情報を統合したResnik Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_partial_similarity(\n",
    "    annotation_type: str, value_a: str, value_b: str\n",
    ") -> float:\n",
    "    \"\"\"アノテーション値間の部分的類似度\"\"\"\n",
    "\n",
    "    if annotation_type == \"genotype\":\n",
    "        genotype_similarity = {\n",
    "            (\"Hetero\", \"Homo\"): 0.1,\n",
    "            (\"Hemi\", \"Homo\"): 0.05,\n",
    "            (\"Hemi\", \"Hetero\"): 0.05,\n",
    "        }\n",
    "        key = tuple(sorted([value_a, value_b]))\n",
    "        return genotype_similarity.get(key, 0.0)\n",
    "\n",
    "    elif annotation_type == \"sex\":\n",
    "        return 0.0  # Male vs Female: 完全に異なる\n",
    "\n",
    "    elif annotation_type == \"life_stage\":\n",
    "        life_stage_similarity = {\n",
    "            (\"Early\", \"Interval\"): 0.1,\n",
    "            (\"Early\", \"Late\"): 0.02,\n",
    "            (\"Early\", \"Embryo\"): 0.02,\n",
    "            (\"Interval\", \"Late\"): 0.2,\n",
    "            (\"Interval\", \"Embryo\"): 0.02,\n",
    "            (\"Late\", \"Embryo\"): 0.02,\n",
    "        }\n",
    "        key = tuple(sorted([value_a, value_b]))\n",
    "        return life_stage_similarity.get(key, 0.0)\n",
    "\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_annotation_similarity(annotations_a: dict, annotations_b: dict) -> float:\n",
    "    \"\"\"アノテーション間の類似度計算\"\"\"\n",
    "\n",
    "    # 各アノテーションタイプの重み\n",
    "    annotation_weights = {\"genotype\": 0.5, \"sex\": 0.3, \"life_stage\": 0.2}\n",
    "\n",
    "    total_weight = 0\n",
    "    similarity_sum = 0\n",
    "\n",
    "    for annotation_type in [\"genotype\", \"sex\", \"life_stage\"]:\n",
    "        if annotation_type in annotations_a or annotation_type in annotations_b:\n",
    "            weight = annotation_weights[annotation_type]\n",
    "            total_weight += weight\n",
    "\n",
    "            if annotation_type in annotations_a and annotation_type in annotations_b:\n",
    "                # 両方にアノテーション存在\n",
    "                if annotations_a[annotation_type] == annotations_b[annotation_type]:\n",
    "                    similarity_sum += weight * 1.0  # 完全一致\n",
    "                else:\n",
    "                    # 部分的類似度\n",
    "                    partial_sim = get_annotation_partial_similarity(\n",
    "                        annotation_type,\n",
    "                        annotations_a[annotation_type],\n",
    "                        annotations_b[annotation_type],\n",
    "                    )\n",
    "                    similarity_sum += weight * partial_sim\n",
    "            else:\n",
    "                # 片方のみにアノテーション存在 - 低いペナルティ\n",
    "                similarity_sum += weight * 0.01\n",
    "\n",
    "    if total_weight == 0:\n",
    "        return 1.0  # 両方ともアノテーションなし\n",
    "\n",
    "    return similarity_sum / total_weight\n",
    "\n",
    "\n",
    "# テスト\n",
    "print(\"アノテーション類似度テスト:\")\n",
    "test_cases = [\n",
    "    (\n",
    "        {\"genotype\": \"Homo\", \"life_stage\": \"Early\"},\n",
    "        {\"genotype\": \"Homo\", \"life_stage\": \"Early\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"genotype\": \"Homo\", \"life_stage\": \"Early\"},\n",
    "        {\"genotype\": \"Hetero\", \"life_stage\": \"Early\"},\n",
    "    ),\n",
    "    ({\"genotype\": \"Homo\", \"sex\": \"Male\"}, {\"genotype\": \"Homo\", \"sex\": \"Female\"}),\n",
    "]\n",
    "\n",
    "for ann_a, ann_b in test_cases:\n",
    "    similarity = calculate_annotation_similarity(ann_a, ann_b)\n",
    "    print(f\"  {ann_a} & {ann_b} -> {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_resnik_similarity(similarity: float, ic_scores: dict) -> float:\n",
    "    \"\"\"Resnik similarityを0-1の範囲に正規化\"\"\"\n",
    "\n",
    "    # IC値の最大値を取得（正規化の基準）\n",
    "    max_ic = max(ic_scores.values()) if ic_scores else 10.0\n",
    "\n",
    "    # 0-1にスケーリング\n",
    "    normalized = similarity / max_ic\n",
    "\n",
    "    # 0-1の範囲にクリップ\n",
    "    return max(0.0, min(1.0, normalized))\n",
    "\n",
    "\n",
    "def calculate_resnik_similarity(\n",
    "    phenotype_a: str,\n",
    "    phenotype_b: str,\n",
    "    mpo_data: dict,\n",
    "    ic_scores: dict,\n",
    ") -> float:\n",
    "    \"\"\"アノテーション考慮型Resnik similarity計算\"\"\"\n",
    "\n",
    "    # 完全一致の場合\n",
    "    if phenotype_a == phenotype_b:\n",
    "        # フル表現型から基本表現型を抽出してIC値を取得\n",
    "        base_phenotype, _ = parse_phenotype_annotations(phenotype_a)\n",
    "        similarity = ic_scores.get(base_phenotype, 0.0)\n",
    "    else:\n",
    "        # 表現型とアノテーションを分離\n",
    "        base_a, annotations_a = parse_phenotype_annotations(phenotype_a)\n",
    "        base_b, annotations_b = parse_phenotype_annotations(phenotype_b)\n",
    "\n",
    "        # 基本表現型のResnik similarity\n",
    "        base_similarity = calculate_base_resnik_similarity(\n",
    "            base_a, base_b, mpo_data, ic_scores\n",
    "        )\n",
    "        normalized_base_similarity = normalize_resnik_similarity(\n",
    "            base_similarity, ic_scores\n",
    "        )\n",
    "\n",
    "        # アノテーション類似度\n",
    "        annotation_similarity = calculate_annotation_similarity(\n",
    "            annotations_a, annotations_b\n",
    "        )\n",
    "\n",
    "        # 統合類似度計算（基本表現型を重視）\n",
    "        if normalized_base_similarity < 0.01:\n",
    "            # 基本表現型がほぼ異なる場合、アノテーションをほぼ無視\n",
    "            similarity = (\n",
    "                0.999 * base_similarity + 0.001 * annotation_similarity\n",
    "            )\n",
    "        elif normalized_base_similarity < 0.1:\n",
    "            # 基本表現型が少し類似する場合\n",
    "            similarity = (\n",
    "                0.99 * base_similarity + 0.01 * annotation_similarity\n",
    "            )\n",
    "        else:\n",
    "            # 通常の重み\n",
    "            similarity = (\n",
    "                0.9 * base_similarity + 0.1 * annotation_similarity\n",
    "            )\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def calculate_resnik_similarity_batch(\n",
    "    phenotype_pairs: list, mpo_data: dict, ic_scores: dict\n",
    ") -> list:\n",
    "    \"\"\"複数の表現型ペアに対してResnik similarityを一括計算\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for phenotype_a, phenotype_b in phenotype_pairs:\n",
    "        similarity = calculate_resnik_similarity(\n",
    "            phenotype_a, phenotype_b, mpo_data, ic_scores\n",
    "        )\n",
    "        results.append(\n",
    "            {\n",
    "                \"phenotype_a\": phenotype_a,\n",
    "                \"phenotype_b\": phenotype_b,\n",
    "                \"similarity\": similarity,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# # テスト: 正規化ありとなしの比較\n",
    "# print(\"=== 正規化ありなしの比較テスト ===\")\n",
    "\n",
    "# test_phenotype_pairs = [\n",
    "#     (\n",
    "#         \"preweaning lethality, complete penetrance (Homo, Early)\",\n",
    "#         \"abnormal heart morphology (Homo, Early)\",\n",
    "#     ),\n",
    "#     (\n",
    "#         \"abnormal skin morphology (Homo, Early)\",\n",
    "#         \"abnormal kidney morphology (Homo, Early)\",\n",
    "#     ),\n",
    "#     (\n",
    "#         \"abnormal heart morphology (Homo, Early)\",\n",
    "#         \"abnormal blood vessel morphology (Homo, Early)\",\n",
    "#     ),\n",
    "#     (\n",
    "#         \"hyperactivity (Homo, Early)\",\n",
    "#         \"hyperactivity (Hetero, Early)\",\n",
    "#     ),\n",
    "#     (\n",
    "#         \"hyperactivity (Homo, Early)\",\n",
    "#         \"hyperactivity (Homo, Early)\",\n",
    "#     ),  # 同一表現型\n",
    "# ]\n",
    "\n",
    "# print(f\"\\nIC値の範囲: {min(ic_scores.values()):.3f} - {max(ic_scores.values()):.3f}\")\n",
    "# print(f\"正規化基準（最大IC値）: {max(ic_scores.values()):.3f}\")\n",
    "\n",
    "# print(\"\\n結果:\")\n",
    "# for phenotype_a, phenotype_b in test_phenotype_pairs:\n",
    "#     # 正規化なし\n",
    "#     raw_similarity = calculate_resnik_similarity(\n",
    "#         phenotype_a, phenotype_b, mpo_data, ic_scores\n",
    "#     )\n",
    "\n",
    "#     # 正規化あり\n",
    "#     normalized_similarity = calculate_resnik_similarity(\n",
    "#         phenotype_a, phenotype_b, mpo_data, ic_scores\n",
    "#     )\n",
    "\n",
    "#     print(f\"\\n📍 {phenotype_a}\")\n",
    "#     print(f\"   vs\")\n",
    "#     print(f\"   {phenotype_b}\")\n",
    "#     print(f\"   Raw: {raw_similarity:.4f} -> Normalized: {normalized_similarity:.4f}\")\n",
    "\n",
    "#     # 詳細情報\n",
    "#     base_a, ann_a = parse_phenotype_annotations(phenotype_a)\n",
    "#     base_b, ann_b = parse_phenotype_annotations(phenotype_b)\n",
    "#     ic_a = get_base_phenotype_ic(base_a, ic_scores)\n",
    "#     ic_b = get_base_phenotype_ic(base_b, ic_scores)\n",
    "#     print(f\"   基本表現型IC: {ic_a:.3f} vs {ic_b:.3f}\")\n",
    "\n",
    "# print(f\"\\n🎯 正規化の利点:\")\n",
    "# print(f\"   1. 類似度が0-1の直感的な範囲に収まる\")\n",
    "# print(f\"   2. 他の類似度指標との比較が容易\")\n",
    "# print(f\"   3. 機械学習アルゴリズムでの利用に適している\")\n",
    "# print(f\"   4. 視覚化やユーザーインターフェースでの表示が分かりやすい\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. JaccardとResnikの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(phenotype_a: str, phenotype_b: str) -> float:\n",
    "    \"\"\"従来のJaccard similarity（参考用）\"\"\"\n",
    "    return 1.0 if phenotype_a == phenotype_b else 0.0\n",
    "\n",
    "\n",
    "print(\"Jaccard vs Resnik Similarity 比較（正規化版）\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_pairs = [\n",
    "    (\n",
    "        \"preweaning lethality, complete penetrance (Homo, Early)\",\n",
    "        \"abnormal heart morphology (Homo, Early)\",\n",
    "    ),\n",
    "    (\n",
    "        \"abnormal heart morphology (Homo, Early)\",\n",
    "        \"abnormal blood vessel morphology (Homo, Early)\",\n",
    "    ),\n",
    "    (\"hyperactivity (Homo, Early)\", \"hyperactivity (Homo, Early)\"),  # 同一表現型\n",
    "]\n",
    "\n",
    "print(f\"\\n正規化基準: 最大IC値 = {max(ic_scores.values()):.3f}\")\n",
    "\n",
    "for phenotype_a, phenotype_b in comparison_pairs:\n",
    "    jaccard_sim = jaccard_similarity(phenotype_a, phenotype_b)\n",
    "\n",
    "    # 正規化されたResnik similarity\n",
    "    resnik_sim_normalized = calculate_resnik_similarity(\n",
    "        phenotype_a, phenotype_b, mpo_data, ic_scores, normalize=True\n",
    "    )\n",
    "\n",
    "    # 比較用に生のResnik similarity\n",
    "    resnik_sim_raw = calculate_resnik_similarity(\n",
    "        phenotype_a, phenotype_b, mpo_data, ic_scores, normalize=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\n📍 {phenotype_a}\")\n",
    "    print(\"   vs\")\n",
    "    print(f\"   {phenotype_b}\")\n",
    "    print(\n",
    "        f\"   📊 Jaccard: {jaccard_sim:.4f}  |  Resnik (正規化): {resnik_sim_normalized:.4f}  |  Resnik (生): {resnik_sim_raw:.4f}\"\n",
    "    )\n",
    "\n",
    "    # 改善効果を評価\n",
    "    if jaccard_sim == 0.0 and resnik_sim_normalized < 0.1:\n",
    "        print(\"   ✅ 改善成功: 非関連表現型の類似度が適切に低い\")\n",
    "    elif jaccard_sim == 0.0 and resnik_sim_normalized >= 0.1:\n",
    "        print(\"   🔍 注目: 関連性のある表現型として検出\")\n",
    "    elif jaccard_sim == 1.0:\n",
    "        print(\"   🎯 同一表現型: IC値を活用\")\n",
    "\n",
    "print(\"\\n🎯 正規化Resnik Similarity の利点:\")\n",
    "print(\"   1. 頻出表現型（preweaning lethality等）の偏りを軽減\")\n",
    "print(\"   2. オントロジー階層を考慮した意味的類似度\")\n",
    "print(\"   3. アノテーション情報を適切に考慮\")\n",
    "print(\"   4. 0-1の直感的な範囲で類似度を表現\")\n",
    "print(\"   5. Jaccardなど他の類似度指標との比較が容易\")\n",
    "\n",
    "# 統計情報\n",
    "print(\"\\n📊 類似度分布の統計:\")\n",
    "similarities = []\n",
    "for phenotype_a, phenotype_b in comparison_pairs:\n",
    "    sim = calculate_resnik_similarity(\n",
    "        phenotype_a, phenotype_b, mpo_data, ic_scores, normalize=True\n",
    "    )\n",
    "    similarities.append(sim)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(f\"   平均: {np.mean(similarities):.4f}\")\n",
    "print(f\"   標準偏差: {np.std(similarities):.4f}\")\n",
    "print(f\"   最小値: {np.min(similarities):.4f}\")\n",
    "print(f\"   最大値: {np.max(similarities):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 結果の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を保存\n",
    "output_dir = Path(\"data/resnik_similarity_simplified\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. MPOデータ保存\n",
    "with open(output_dir / \"mpo_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mpo_data, f)\n",
    "\n",
    "# 2. 基本表現型のIC scores保存\n",
    "with open(output_dir / \"ic_scores.json\", \"w\") as f:\n",
    "    json.dump(ic_scores, f, indent=2)\n",
    "\n",
    "# 3. フル表現型の頻度データ保存\n",
    "with open(output_dir / \"phenotype_frequencies.json\", \"w\") as f:\n",
    "    json.dump(phenotype_frequencies, f, indent=2)\n",
    "\n",
    "# 4. 基本表現型の頻度データ保存\n",
    "with open(output_dir / \"base_phenotype_frequencies.json\", \"w\") as f:\n",
    "    json.dump(base_phenotype_frequencies, f, indent=2)\n",
    "\n",
    "# 5. 正規化のための統計情報を保存\n",
    "normalization_stats = {\n",
    "    \"max_ic\": float(max(ic_scores.values())),\n",
    "    \"min_ic\": float(min(ic_scores.values())),\n",
    "    \"mean_ic\": float(sum(ic_scores.values()) / len(ic_scores)),\n",
    "    \"num_base_phenotypes\": len(ic_scores),\n",
    "}\n",
    "\n",
    "with open(output_dir / \"normalization_stats.json\", \"w\") as f:\n",
    "    json.dump(normalization_stats, f, indent=2)\n",
    "\n",
    "print(f\"Simplified Resnik similarity data saved to {output_dir}/\")\n",
    "print(\"Files saved:\")\n",
    "for file_path in output_dir.glob(\"*\"):\n",
    "    print(f\"  - {file_path.name}\")\n",
    "\n",
    "print(\"\\n✅ 簡略化実装完了!\")\n",
    "print(\"主要な変更点:\")\n",
    "print(\"  - IC値は基本表現型のみをキーとして計算\")\n",
    "print(\"  - LCA IC値推定機能を追加\")\n",
    "print(\"  - 0-1正規化機能を実装\")\n",
    "print(\"  - アノテーション情報は別途考慮\")\n",
    "print(f\"  - {len(base_phenotype_frequencies)} unique base phenotypes\")\n",
    "print(f\"  - {len(phenotype_frequencies)} full phenotype observations\")\n",
    "\n",
    "print(\"\\n主要関数:\")\n",
    "print(\"  - calculate_resnik_similarity(): メイン関数（正規化オプション付き）\")\n",
    "print(\"  - normalize_resnik_similarity(): 0-1正規化関数\")\n",
    "print(\"  - calculate_resnik_similarity_batch(): 一括計算関数\")\n",
    "print(\"  - parse_phenotype_annotations(): 表現型解析\")\n",
    "print(\"  - calculate_base_resnik_similarity(): 基本類似度\")\n",
    "print(\"  - calculate_annotation_similarity(): アノテーション類似度\")\n",
    "\n",
    "print(\"\\n使用例:\")\n",
    "print(\"  # 正規化あり（推奨）\")\n",
    "print(\n",
    "    \"  similarity = calculate_resnik_similarity(phenotype_a, phenotype_b, mpo_data, ic_scores, normalize=True)\"\n",
    ")\n",
    "print(\"  # 正規化なし\")\n",
    "print(\n",
    "    \"  similarity = calculate_resnik_similarity(phenotype_a, phenotype_b, mpo_data, ic_scores, normalize=False)\"\n",
    ")\n",
    "\n",
    "print(f\"\\n正規化統計:\")\n",
    "print(\n",
    "    f\"  - IC値範囲: {normalization_stats['min_ic']:.3f} - {normalization_stats['max_ic']:.3f}\"\n",
    ")\n",
    "print(f\"  - 平均IC値: {normalization_stats['mean_ic']:.3f}\")\n",
    "print(f\"  - 正規化後の類似度範囲: 0.000 - 1.000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tsumugi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
