{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run allですべてのデータを準備する\n",
    "\n",
    "* URL: https://ftp.ebi.ac.uk/pub/databases/impc/all-data-releases/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSUMUGI_VERSION = \"1.0.0\"\n",
    "IMPC_RELEASE = 23.0\n",
    "\n",
    "columns = [\n",
    "    \"marker_symbol\",\n",
    "    \"marker_accession_id\",\n",
    "    \"mp_term_name\",\n",
    "    \"mp_term_id\",\n",
    "    \"p_value\",\n",
    "    \"effect_size\",\n",
    "    \"female_ko_effect_p_value\",\n",
    "    \"male_ko_effect_p_value\",\n",
    "    \"female_ko_parameter_estimate\",\n",
    "    \"sex_effect_p_value\",\n",
    "    \"male_ko_parameter_estimate\",  # sex differences\n",
    "    \"genotype_effect_p_value\",\n",
    "    \"genotype_effect_parameter_estimate\",\n",
    "    \"zygosity\",  # zygosity\n",
    "    \"pipeline_name\",\n",
    "    \"procedure_name\",  # life-stage\n",
    "    \"allele_symbol\",  # map to Phendigm\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = print\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up to top directory\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "while not Path(\"LICENSE\").exists():\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download IMPC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phenodigm dataが存在していない場合には、ダウンロードを促す\n",
    "\n",
    "if not Path(\"data\", \"phenodigm\", \"impc_phenodigm.csv\").exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Please download impc phenodigm data from https://diseasemodels.research.its.qmul.ac.uk/.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パスの設定\n",
    "data_dir = Path(\"data/impc\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = data_dir / f\"statistical-results-ALL-{IMPC_RELEASE}.csv\"\n",
    "\n",
    "# ファイルが存在しない場合にダウンロードして解凍\n",
    "if not csv_path.exists():\n",
    "    # ダウンロード URL\n",
    "    url = f\"https://ftp.ebi.ac.uk/pub/databases/impc/all-data-releases/release-{IMPC_RELEASE}/results/statistical-results-ALL.csv.gz\"\n",
    "\n",
    "    print(f\"Downloading and extracting: {url}\")\n",
    "\n",
    "    # URL からファイルサイズ取得（tqdmのため）\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        total_size = int(response.info().get(\"Content-Length\", -1))\n",
    "        with tqdm.wrapattr(\n",
    "            response,\n",
    "            \"read\",\n",
    "            total=total_size,\n",
    "            desc=\"Downloading\",\n",
    "            unit=\"B\",\n",
    "            unit_scale=True,\n",
    "        ) as r:\n",
    "            with gzip.GzipFile(fileobj=r) as uncompressed:\n",
    "                with open(csv_path, \"wb\") as out_file:\n",
    "                    shutil.copyfileobj(uncompressed, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# wc -l data/impc/statistical-results*.csv\n",
    "# Release 22.1: 3165335\n",
    "# Release 23.0: 2159931\n",
    "\n",
    "# 1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter dataset by P value < 0.0001 (10^-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"data\", f\"statistical_filtered-{IMPC_RELEASE}.csv\").exists():\n",
    "    path_df_statistical_filtered = Path(\n",
    "        \"data\", \"impc\", f\"statistical-results-ALL-{IMPC_RELEASE}.csv\"\n",
    "    )\n",
    "    df_statistical_all = pd.read_csv(path_df_statistical_filtered)\n",
    "    df_statistical_all = df_statistical_all[columns]\n",
    "\n",
    "    # Filter by p_value < 0.0001\n",
    "    threshold = 0.0001\n",
    "    filter_pvalue = df_statistical_all[\"p_value\"] < threshold\n",
    "    filter_female_ko_pvalue = df_statistical_all[\"female_ko_effect_p_value\"] < threshold\n",
    "    filter_male_ko_pvalue = df_statistical_all[\"male_ko_effect_p_value\"] < threshold\n",
    "\n",
    "    df_statistical_filtered = df_statistical_all[\n",
    "        filter_pvalue | filter_female_ko_pvalue | filter_male_ko_pvalue\n",
    "    ]\n",
    "\n",
    "    # Filter by mp_term_id and mp_term_name are not NaN\n",
    "    df_statistical_filtered = df_statistical_filtered.dropna(subset=[\"mp_term_id\"])\n",
    "    df_statistical_filtered = df_statistical_filtered.dropna(subset=[\"mp_term_name\"])\n",
    "\n",
    "    # Filter by effect_size is not NaN\n",
    "    df_statistical_filtered = df_statistical_filtered.dropna(subset=[\"effect_size\"])\n",
    "    df_statistical_filtered.to_csv(\n",
    "        f\"data/statistical_filtered-{IMPC_RELEASE}.csv\", index=False\n",
    "    )  # 2 sec\n",
    "\n",
    "# 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistical_filtered = pd.read_csv(f\"data/statistical_filtered-{IMPC_RELEASE}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_statistical_filtered))\n",
    "# Release 22.0: 54059 rows\n",
    "# Release 22.1: 54059 rows\n",
    "# Release 23.0: 49299 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data by mp_term_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistical_filtered = pd.read_csv(f\"data/statistical_filtered-{IMPC_RELEASE}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/mp_term_nameを作成\n",
    "\n",
    "output_path = Path(\"data\", \"mp_term_name\")\n",
    "if output_path.exists():\n",
    "    shutil.rmtree(output_path)\n",
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名前をクリーンにする関数を定義\n",
    "def clean_name(name):\n",
    "    return name.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "# mp_term_nameをクリーニングし、ユニークな値を取得\n",
    "unique_mp_term_names = df_statistical_filtered[\"mp_term_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニークなmp_term_nameごとにフィルタリングしてCSVに保存: 5 sec\n",
    "for mp_term_name in unique_mp_term_names:\n",
    "    df_mp_term = df_statistical_filtered[\n",
    "        df_statistical_filtered[\"mp_term_name\"] == mp_term_name\n",
    "    ]\n",
    "    clean_mp_term_name = clean_name(mp_term_name)\n",
    "    df_mp_term.to_csv(f\"data/mp_term_name/{clean_mp_term_name}.csv\", index=False)\n",
    "# 5 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TSUMUGIに必要なアノテーション情報を整理する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistical_filtered = pd.read_csv(f\"data/statistical_filtered-{IMPC_RELEASE}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate life stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# life_stageの初期割り当て\n",
    "def assign_life_stage(pipeline_name):\n",
    "    if pd.isna(pipeline_name):\n",
    "        return \"Early\"\n",
    "    if \"Interval\" in pipeline_name or \"interval\" in pipeline_name:\n",
    "        return \"Interval\"\n",
    "    elif \"Late\" in pipeline_name or \"late\" in pipeline_name:\n",
    "        return \"Late\"\n",
    "    else:\n",
    "        return \"Early\"\n",
    "\n",
    "\n",
    "df_statistical_filtered[\"life_stage\"] = df_statistical_filtered[\"pipeline_name\"].apply(\n",
    "    assign_life_stage\n",
    ")\n",
    "\n",
    "# Embryo 表現型に該当する procedure_name の一覧\n",
    "embryo_phenotyping = [\n",
    "    \"Gross Morphology Embryo E9.5\",\n",
    "    \"Viability E9.5 Secondary Screen\",\n",
    "    \"OPT E9.5\",\n",
    "    \"MicroCT E9.5\",\n",
    "    \"Gross Morphology Placenta E9.5\",\n",
    "    \"Gross Morphology Embryo E12.5\",\n",
    "    \"Embryo LacZ\",\n",
    "    \"Gross Morphology Placenta E12.5\",\n",
    "    \"Viability E12.5 Secondary Screen\",\n",
    "    \"Viability E14.5-E15.5 Secondary Screen\",\n",
    "    \"Gross Morphology Placenta E14.5-E15.5\",\n",
    "    \"MicroCT E14.5-E15.5\",\n",
    "    \"Gross Morphology Embryo E14.5-E15.5\",\n",
    "    \"Viability E18.5 Secondary Screen\",\n",
    "    \"MicroCT E18.5\",\n",
    "    \"Gross Morphology Embryo E18.5\",\n",
    "    \"Gross Morphology Placenta E18.5\",\n",
    "]\n",
    "\n",
    "# life_stageをEmbryoに上書き\n",
    "df_statistical_filtered.loc[\n",
    "    df_statistical_filtered[\"procedure_name\"].isin(embryo_phenotyping), \"life_stage\"\n",
    "] = \"Embryo\"\n",
    "df_annotated = df_statistical_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_annotated))\n",
    "print(df_annotated[\"life_stage\"].value_counts())\n",
    "# 54059\n",
    "# life_stage\n",
    "# Early       45724\n",
    "# Embryo       4253\n",
    "# Late         4024\n",
    "# Interval       58\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Sex differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.0001\n",
    "\n",
    "# 条件リスト\n",
    "conditions = [\n",
    "    (df_annotated[\"sex_effect_p_value\"] < threshold)\n",
    "    & (df_annotated[\"female_ko_effect_p_value\"] < threshold)\n",
    "    & (df_annotated[\"male_ko_effect_p_value\"] > threshold),\n",
    "    (df_annotated[\"sex_effect_p_value\"] < threshold)\n",
    "    & (df_annotated[\"male_ko_effect_p_value\"] < threshold)\n",
    "    & (df_annotated[\"female_ko_effect_p_value\"] > threshold),\n",
    "]\n",
    "\n",
    "# 条件に対応する値\n",
    "choices = [\"female\", \"male\"]\n",
    "\n",
    "# np.selectで列を設定\n",
    "df_annotated[\"sexdual_dimorphism\"] = np.select(conditions, choices, default=None)\n",
    "df_annotated = df_annotated.reset_index(drop=True)\n",
    "\n",
    "# 結果を確認\n",
    "print(IMPC_RELEASE)\n",
    "print(df_annotated[\"sexdual_dimorphism\"].value_counts())\n",
    "\n",
    "# RELEASE 22.1\n",
    "# male      4915\n",
    "# female    4146\n",
    "\n",
    "# RELEASE 23.0\n",
    "# male      5026\n",
    "# female    4344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認\n",
    "df_annotated.dropna(subset=[\"sexdual_dimorphism\"])[\n",
    "    [\"p_value\", \"sexdual_dimorphism\", \"effect_size\"]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遺伝型、性差、ライフステージのアノテーションを統合する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_annotated[\"zygosity\"].value_counts())\n",
    "# RELEASE 22.1\n",
    "# zygosity\n",
    "# homozygote      41444\n",
    "# heterozygote    11921\n",
    "# hemizygote        694\n",
    "\n",
    "# RELEASE 23.0\n",
    "# homozygote      37820\n",
    "# heterozygote    10896\n",
    "# hemizygote        583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アノテーション列を追加（inplace）\n",
    "def make_annotation(row) -> list[str]:\n",
    "    # 遺伝型\n",
    "    if row[\"zygosity\"] == \"homozygote\":\n",
    "        annotate = \"Homo\"\n",
    "    elif row[\"zygosity\"] == \"heterozygote\":\n",
    "        annotate = \"Hetero\"\n",
    "    else:\n",
    "        annotate = \"Hemi\"\n",
    "\n",
    "    # 性別\n",
    "    if row[\"sexdual_dimorphism\"] == \"female\":\n",
    "        annotate += \", Female\"\n",
    "    elif row[\"sexdual_dimorphism\"] == \"male\":\n",
    "        annotate += \", Male\"\n",
    "\n",
    "    # life stage\n",
    "    if row[\"life_stage\"] in {\"Embryo\", \"Early\", \"Interval\", \"Late\"}:\n",
    "        annotate += f\", {row['life_stage']}\"\n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    annotations.append(f\"{row['mp_term_name']} ({annotate})\")\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "df_annotated[\"annotation\"] = df_annotated.apply(make_annotation, axis=1)\n",
    "\n",
    "df_exploded = df_annotated.explode(\"annotation\").reset_index(drop=True)\n",
    "\n",
    "# marker_symbol ごとに annotation をリスト化＆ソート\n",
    "marker_annotation_map = df_exploded.groupby(\"marker_symbol\")[\"annotation\"].apply(\n",
    "    lambda x: sorted(set(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：Rhdの注釈を表示\n",
    "print(marker_annotation_map[\"Rhd\"])\n",
    "# 例：Amtの注釈を表示 (Embryo)\n",
    "print(marker_annotation_map[\"Amt\"])\n",
    "# 例：Spag4の注釈を表示 (重複が削除されているか)\n",
    "print(marker_annotation_map[\"Spag4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data/annotation\").mkdir(exist_ok=True, parents=True)\n",
    "file_path = \"data/annotation/symbol_mptermname.json\"\n",
    "marker_annotation_map.to_json(file_path, indent=4)\n",
    "\n",
    "# json.dump(marker_annotation_map, open(file_path, \"w\"), indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "grep -c \"Male\" data/annotation/symbol_mptermname.json | sed \"s|^|Male: |\"\n",
    "grep -c \"Female\" data/annotation/symbol_mptermname.json | sed \"s|^|Feale: |\"\n",
    "\n",
    "grep -c \"Homo\" data/annotation/symbol_mptermname.json | sed \"s|^|Homo: |\"\n",
    "grep -c \"Hetero\" data/annotation/symbol_mptermname.json | sed \"s|^|Hetero: |\"\n",
    "grep -c \"Hemi\" data/annotation/symbol_mptermname.json | sed \"s|^|Hemi: |\"\n",
    "\n",
    "grep -c \"Embryo\" data/annotation/symbol_mptermname.json | sed \"s|^|Embryo: |\"\n",
    "grep -c \"Early\" data/annotation/symbol_mptermname.json | sed \"s|^|Early: |\"\n",
    "grep -c \"Interval\" data/annotation/symbol_mptermname.json | sed \"s|^|Interval: |\"\n",
    "grep -c \"Late\" data/annotation/symbol_mptermname.json | sed \"s|^|Late: |\"\n",
    "\n",
    "# RELEASE 22.1\n",
    "# Male: 4915\n",
    "# Feale: 4146\n",
    "# Homo: 41444\n",
    "# Hetero: 11921\n",
    "# Hemi: 694\n",
    "# Embryo: 4253\n",
    "# Early: 45724\n",
    "# Interval: 58\n",
    "# Late: 4024\n",
    "\n",
    "# RELEASE 23.0\n",
    "# Male: 4480\n",
    "# Feale: 3557\n",
    "# Homo: 30977\n",
    "# Hetero: 9625\n",
    "# Hemi: 492\n",
    "# Embryo: 4207\n",
    "# Early: 34324\n",
    "# Interval: 54\n",
    "# Late: 2509"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Phenodigmを用いたヒト疾患情報を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phenodigm = pd.read_csv(Path(\"data\", \"phenodigm\", \"impc_phenodigm.csv\"))\n",
    "P(len(df_phenodigm))\n",
    "# 3405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各行について空白の数をカウント\n",
    "space_counts = df_phenodigm[\"Mouse model description\"].str.count(\" \")\n",
    "\n",
    "# 空白の数が2でない行を抽出（== split して3つにならない行）\n",
    "invalid_rows = df_phenodigm[space_counts != 2]\n",
    "\n",
    "# 結果表示\n",
    "print(f\"全体の件数: {len(df_phenodigm)}\")\n",
    "print(f\"空白がちょうど2つでない行数: {len(invalid_rows)}\")\n",
    "print(invalid_rows.head())\n",
    "# -> たった2つしかなく、`Phex<not yet available>`なので、この2つは無視する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phenodigm = df_phenodigm[space_counts == 2]\n",
    "P(len(df_phenodigm))\n",
    "# 3403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phenodigm[[\"allele_symbol\", \"zygosity\", \"life_stage\"]] = df_phenodigm[\n",
    "    \"Mouse model description\"\n",
    "].str.split(\" \", n=2, expand=True)\n",
    "df_phenodigm = df_phenodigm.drop(columns=[\"Mouse model description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P(df_phenodigm.columns)\n",
    "P(df_phenodigm[\"allele_symbol\"].head(3))\n",
    "P(df_phenodigm[\"zygosity\"].head(3))\n",
    "P(df_phenodigm[\"life_stage\"].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phenodigmの表記とimpcデータの表記を揃える\n",
    "\n",
    "df_phenodigm = df_phenodigm.replace(\n",
    "    {\"zygosity\": {\"hom\": \"homozygote\", \"het\": \"heterozygote\", \"hem\": \"hemizygote\"}}\n",
    ")\n",
    "df_phenodigm[\"life_stage\"] = df_phenodigm[\"life_stage\"].str.capitalize()\n",
    "print(df_phenodigm[\"zygosity\"].value_counts())\n",
    "print(df_phenodigm[\"life_stage\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotated_phenodigm = (\n",
    "    df_annotated.set_index([\"allele_symbol\", \"life_stage\", \"zygosity\"])\n",
    "    .join(\n",
    "        df_phenodigm.set_index([\"allele_symbol\", \"life_stage\", \"zygosity\"]),\n",
    "        how=\"left\",\n",
    "        rsuffix=\"_phenodigm\",\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "print(len(df_annotated_phenodigm))\n",
    "# 63645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"marker_symbol\", \"Disorder name\", \"life_stage\", \"zygosity\"]\n",
    "df_annotated_phenodigm = (\n",
    "    df_annotated_phenodigm[columns_to_keep]\n",
    "    .dropna(subset=[\"Disorder name\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_annotated_phenodigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アノテーション列を追加（inplace）\n",
    "def make_annotation(row) -> list[str]:\n",
    "    # 遺伝型\n",
    "    if row[\"zygosity\"] == \"homozygote\":\n",
    "        annotate = \"Homo\"\n",
    "    elif row[\"zygosity\"] == \"heterozygote\":\n",
    "        annotate = \"Hetero\"\n",
    "    else:\n",
    "        annotate = \"Hemi\"\n",
    "\n",
    "    # life stage\n",
    "    if row[\"life_stage\"] in {\"Embryo\", \"Early\", \"Interval\", \"Late\"}:\n",
    "        annotate += f\", {row['life_stage']}\"\n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    annotations.append(f\"{row['Disorder name']} ({annotate})\")\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "df_annotated_phenodigm[\"annotation\"] = df_annotated_phenodigm.apply(\n",
    "    make_annotation, axis=1\n",
    ")\n",
    "\n",
    "df_exploded = df_annotated_phenodigm.explode(\"annotation\").reset_index(drop=True)\n",
    "\n",
    "# marker_symbol ごとに annotation をリスト化＆ソート\n",
    "marker_annotation_map = df_exploded.groupby(\"marker_symbol\")[\"annotation\"].apply(\n",
    "    lambda x: sorted(set(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：Phenodigmの注釈を表示 (Embryo)\n",
    "print(marker_annotation_map[\"Arhgap31\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data/annotation\").mkdir(exist_ok=True, parents=True)\n",
    "file_path = \"data/annotation/symbol_disordername.json\"\n",
    "marker_annotation_map.to_json(file_path, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mp term nameとIMPCのPhenotype URLを紐付ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_statistical_filtered[[\"mp_term_id\", \"mp_term_name\"]].drop_duplicates()\n",
    "# df_select = data[['marker_symbol', 'marker_accession_id', 'mp_term_name', 'mp_term_id']].drop_duplicates()\n",
    "df_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_phenotype_url = dict()\n",
    "for index, row in df_select.iterrows():\n",
    "    mp_tern_id = row[\"mp_term_id\"]\n",
    "    impc_url = f\"https://www.mousephenotype.org/data/phenotypes/{mp_tern_id}\"\n",
    "    mp_term_name = row[\"mp_term_name\"]\n",
    "    dict_phenotype_url[mp_term_name] = impc_url\n",
    "\n",
    "print(dict_phenotype_url[\"small lymph nodes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/annotation/mptermname_phenotypeurl.tsv\", \"w\") as f:\n",
    "    for term, url in dict_phenotype_url.items():\n",
    "        f.write(f\"{term}\\t{url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "head -n 3 data/annotation/mptermname_phenotypeurl.tsv\n",
    "wc -l data/annotation/mptermname_phenotypeurl.tsv\n",
    "# Release 22.0: 664\n",
    "# Release 23.0: 659"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### marker symbolとMGI accession idを紐付ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_statistical_filtered[\n",
    "    [\"marker_symbol\", \"marker_accession_id\"]\n",
    "].drop_duplicates()\n",
    "# df_select = data[['marker_symbol', 'marker_accession_id', 'mp_term_name', 'mp_term_id']].drop_duplicates()\n",
    "df_select\n",
    "# Release 22.1: 7746 rows\n",
    "# Release 23.0: 7934 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_symbol_id = dict()\n",
    "for index, row in df_select.iterrows():\n",
    "    dict_symbol_id[row[\"marker_symbol\"]] = row[\"marker_accession_id\"]\n",
    "print(dict_symbol_id[\"Ncam1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(\n",
    "    dict_symbol_id,\n",
    "    open(\"data/annotation/symbol_mgiid.json\", \"w\"),\n",
    "    indent=4,\n",
    "    sort_keys=True,\n",
    ")\n",
    "Path(\"data/annotation/symbol_mgiid.tsv\").write_text(\n",
    "    \"\\n\".join([f\"{k}\\t{v}\" for k, v in dict_symbol_id.items()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n 3 data/annotation/symbol_mgiid.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 表現型の類似度を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"data/overlap/gene_pair_mp_similarity_phenodigm.pkl\")\n",
    "\n",
    "if not file_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Please run 038-resnik-simple.ipynb to generate {file_path}.\"\n",
    "    )\n",
    "\n",
    "gene_pair_mp_similarity = pickle.load(open(\"data/overlap/gene_pair_mp_similarity_phenodigm.pkl\", \"rb\"))\n",
    "gene_pair_mp_similarity_filtered = [\n",
    "    item for item in gene_pair_mp_similarity if item[4] >= 2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gene_pair_mp_similarity_filtered[-3:])\n",
    "print(len(gene_pair_mp_similarity_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生データをCSV形式で出力 （ダウンロード用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity = pd.DataFrame(gene_pair_mp_similarity)\n",
    "df_similarity.columns = [\n",
    "    \"Gene1\",\n",
    "    \"Gene2\",\n",
    "    \"Phenodigm Score\",\n",
    "    \"Jaccard Similarity\",\n",
    "    \"Number of shared phenotype\",\n",
    "    \"List of shared phenotypes\",\n",
    "]\n",
    "df_similarity.reindex(\n",
    "    columns=[\n",
    "        \"Gene1\",\n",
    "        \"Gene2\",\n",
    "        \"Phenodigm Score\",\n",
    "        \"Number of shared phenotype\",\n",
    "        \"Jaccard Similarity\",\n",
    "        \"List of shared phenotypes\",\n",
    "    ]\n",
    ")\n",
    "# df_similarity[\"List of shared phenotypes\"] = df_similarity[\"List of shared phenotypes\"].apply(json.dumps)\n",
    "# 30 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"data\", \"TSUMUGI_RawData\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "path_csv = output_dir / Path(f\"TSUMUGI_v{TSUMUGI_VERSION}_raw_data.csv.gz\")\n",
    "path_parquet = output_dir / Path(f\"TSUMUGI_v{TSUMUGI_VERSION}_raw_data.parquet\")\n",
    "\n",
    "\n",
    "def get_head1000_hash(df: pd.DataFrame) -> str:\n",
    "    # head(1000)だけを対象にハッシュ化\n",
    "    csv_bytes = df.head(1000).to_csv(index=False, lineterminator=\"\\n\").encode(\"utf-8\")\n",
    "    return hashlib.md5(csv_bytes).hexdigest()\n",
    "\n",
    "\n",
    "def file_head1000_hash(path: Path) -> str | None:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        lines = [next(f) for _ in range(1001)]  # 1行目がヘッダー\n",
    "        csv_content = \"\".join(lines).encode(\"utf-8\")\n",
    "        return hashlib.md5(csv_content).hexdigest()\n",
    "\n",
    "\n",
    "# 比較\n",
    "new_hash = get_head1000_hash(df_similarity)\n",
    "existing_hash = file_head1000_hash(path_csv)\n",
    "\n",
    "if new_hash != existing_hash:\n",
    "    df_similarity.to_csv(path_csv, index=False, compression=\"gzip\", lineterminator=\"\\n\")\n",
    "    df_similarity.to_parquet(path_parquet, index=False)\n",
    "    print(\"🔄 ファイルを更新しました\")\n",
    "    # 3 min\n",
    "else:\n",
    "    print(\"✅ 内容に変更がないためスキップしました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 表現型ごとのネットワークを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_pair_mp_similarity_filtered = pickle.load(\n",
    "#     open(\"data/overlap/gene_pair_mp_similarity_filtered_phenodigm.pkl\", \"rb\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity = pd.DataFrame(\n",
    "    gene_pair_mp_similarity_filtered,\n",
    "    columns=[\n",
    "        \"marker1\",\n",
    "        \"marker2\",\n",
    "        \"phenodigm_score\",\n",
    "        \"phenotype_similarity\",\n",
    "        \"shared_mp_number\",\n",
    "        \"shared_mp\",\n",
    "    ],\n",
    ")\n",
    "print(len(df_similarity))\n",
    "# version 0.2.2: 133281  rows × 5 columns\n",
    "# version 0.3.0: 261216  rows × 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phenodigmスコアを対数変換\n",
    "df_similarity[\"phenodigm_score\"] = np.log1p(df_similarity[\"phenodigm_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marker_phenotype = json.load(open(\"data/annotation/symbol_mptermname.json\"))\n",
    "df_marker_phenotype = pd.DataFrame(\n",
    "    df_marker_phenotype.items(), columns=[\"marker_symbol\", \"mp_term_name\"]\n",
    ")\n",
    "print(len(df_marker_phenotype))\n",
    "# TSUMUGI v0.2.2: 7626 rows\n",
    "# TSUMUGI v0.3.0: 7746 rows\n",
    "# TSUMUGI v0.3.1: 7746 rows\n",
    "# TSUMUGI v0.3.2: 7954 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_marker_phenotype = dict(\n",
    "    zip(df_marker_phenotype.marker_symbol, df_marker_phenotype.mp_term_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_marker_disease = json.load(open(\"data/annotation/symbol_disordername.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"data/network/mp_term_name\")\n",
    "# remove network directory\n",
    "if output_dir.exists():\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_target_phenotypes = list(Path(\"data\", \"mp_term_name\").glob(\"*.csv\"))\n",
    "\n",
    "\"\"\"\n",
    "ノードが多すぎるとWebページが描画できない問題を回避するため、\n",
    "ノード数を閾値（upper_limit）以下にするために最適なphenodigm_scoreを求める\n",
    "\"\"\"\n",
    "number_of_nodes = 125\n",
    "tolerance = 25\n",
    "upper_limit = number_of_nodes + tolerance\n",
    "lower_limit = number_of_nodes - tolerance\n",
    "\n",
    "for path_target_phenotype in tqdm(path_target_phenotypes, desc=\"Processing MP terms\"):\n",
    "    columns = [\"marker_symbol\", \"effect_size\"]\n",
    "    df_marker_effect = pd.read_csv(path_target_phenotype, usecols=columns).dropna(\n",
    "        subset=[\"effect_size\"]\n",
    "    )\n",
    "    df_marker_effect[\"effect_size\"] = df_marker_effect[\"effect_size\"].abs()\n",
    "\n",
    "    # バイナリ表現型以外では、effect_sizeを対数変換する\n",
    "    is_binary = df_marker_effect[\"effect_size\"].isin([0, 1]).all()\n",
    "    if not is_binary:\n",
    "        df_marker_effect[\"effect_size\"] = np.log1p(df_marker_effect[\"effect_size\"])\n",
    "\n",
    "    # * effect sizeの絶対値が最大の行を取得 (Homo/Heteroで異なる効果量がある場合に、ひとまず最大値を採用する← 今後の考慮事項)\n",
    "    idx = df_marker_effect.groupby(\"marker_symbol\")[\"effect_size\"].idxmax()\n",
    "    df_max = df_marker_effect.loc[idx]\n",
    "\n",
    "    dict_marker_effect = dict(zip(df_max[\"marker_symbol\"], df_max[\"effect_size\"]))\n",
    "\n",
    "    target_phenotype = path_target_phenotype.stem\n",
    "    target_phenotype_space = target_phenotype.replace(\"_\", \" \")\n",
    "    gene_symbols = df_marker_effect[\"marker_symbol\"]\n",
    "\n",
    "    # --- 1. phenotypeを生じるgene_symbolsを含むエッジのみ抽出 ---------------------------\n",
    "    df_filtered = df_similarity[\n",
    "        df_similarity[\"marker1\"].isin(gene_symbols)\n",
    "        & df_similarity[\"marker2\"].isin(gene_symbols)\n",
    "        & df_similarity[\"shared_mp\"].apply(\n",
    "            lambda lst: any(target_phenotype_space in term for term in lst)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # --- 2. 初期状態のノードの数を確認 -------------------------------\n",
    "    nodes = set(\n",
    "        pd.concat([df_filtered[\"marker1\"], df_filtered[\"marker2\"]], ignore_index=True)\n",
    "    )\n",
    "    num_nodes = len(nodes)\n",
    "\n",
    "    if num_nodes > upper_limit:\n",
    "        # --- 3. 離散スコア値で探索 ----------------------------------------\n",
    "        discrete_scores = df_filtered.loc[:, \"phenodigm_score\"].unique()\n",
    "        discrete_scores = np.sort(discrete_scores)[::-1]  # 降順\n",
    "\n",
    "        best_thr = None\n",
    "        best_diff = float(\"inf\")\n",
    "\n",
    "        lo, hi = 0, len(discrete_scores) - 1\n",
    "        while lo <= hi:\n",
    "            mid_idx = (lo + hi) // 2\n",
    "            thr = discrete_scores[mid_idx]\n",
    "\n",
    "            df_mid = df_filtered[df_filtered[\"phenodigm_score\"] >= thr]\n",
    "            nodes = set(\n",
    "                pd.concat([df_mid[\"marker1\"], df_mid[\"marker2\"]], ignore_index=True)\n",
    "            )\n",
    "            num_nodes = len(nodes)\n",
    "\n",
    "            # ▼ ノード数が許容範囲なら候補にする\n",
    "            if num_nodes <= upper_limit:\n",
    "                diff = abs(num_nodes - number_of_nodes)\n",
    "                if diff < best_diff:\n",
    "                    best_diff = diff\n",
    "                    best_thr = thr\n",
    "                # さらにノードを減らせるか？ → 閾値を **上げる**（スコアを大きく）\n",
    "                lo = mid_idx + 1\n",
    "            else:\n",
    "                # ノードが多すぎ → 閾値を **上げる**（スコアを大きく）\n",
    "                hi = mid_idx - 1\n",
    "\n",
    "        # ------------ 最終の閾値 ---------------------------------------\n",
    "        if best_thr is None:\n",
    "            # Upper limit以下がどうしても存在しないケース → 最小ノードになる閾値\n",
    "            best_thr = discrete_scores[hi + 1]  # hi は最後に -1 されているので +1\n",
    "\n",
    "        df_filtered = df_filtered[df_filtered[\"phenodigm_score\"] >= best_thr]\n",
    "\n",
    "    # --- 4. フィルタリングされたエッジからノードを入手 -------------------------------\n",
    "    nodes = set(\n",
    "        pd.concat([df_filtered[\"marker1\"], df_filtered[\"marker2\"]], ignore_index=True)\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # ネットワーク図のためのノードとエッジを作成\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # NodeをJSON形式に変換\n",
    "    # ----------------------------------------------------\n",
    "    node_json = []\n",
    "    for node in nodes:\n",
    "        phenotype = dict_marker_phenotype.get(node, \"\")\n",
    "        disease = dict_marker_disease.get(node, \"\")\n",
    "        node_color = dict_marker_effect[node] if node in dict_marker_effect else 0.0\n",
    "        node_json.append(\n",
    "            {\n",
    "                \"data\": {\n",
    "                    \"id\": node,\n",
    "                    \"label\": node,\n",
    "                    \"phenotype\": phenotype,\n",
    "                    \"disease\": disease,\n",
    "                    \"node_color\": node_color,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # edgesを用意\n",
    "    # ----------------------------------------------------\n",
    "    df_edge = df_filtered[[\"marker1\", \"marker2\", \"phenodigm_score\", \"shared_mp\"]]\n",
    "    rows = df_edge.to_dict(orient=\"records\")\n",
    "    # EdgeをJSON形式に変換\n",
    "    edge_json = [\n",
    "        {\n",
    "            \"data\": {\n",
    "                \"source\": r[\"marker1\"],\n",
    "                \"target\": r[\"marker2\"],\n",
    "                \"phenotype\": r[\"shared_mp\"],\n",
    "                \"edge_size\": r[\"phenodigm_score\"],\n",
    "            }\n",
    "        }\n",
    "        for r in rows\n",
    "    ]\n",
    "    # ----------------------------------------------------\n",
    "    # EdgeとNodeを統合して、出力\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    network_json = node_json + edge_json\n",
    "\n",
    "    # Output as JSON\n",
    "    if network_json:\n",
    "        output_json = output_dir / f\"{target_phenotype}.json.gz\"\n",
    "        with gzip.open(output_json, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(network_json, f, indent=4)\n",
    "\n",
    "\n",
    "# 3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -lhS data/network/mp_term_name/ | head -n 5\n",
    "echo \"----------------------\"\n",
    "ls -lhS data/network/mp_term_name/ | tail -n 5\n",
    "\n",
    "# TSUMUGI v0.2.2: total 5.3M\n",
    "# TSUMUGI v0.3.0: total 5.5M\n",
    "# TSUMUGI v0.3.1: total 5.1M <- 該当の表現型を含むネットワークのみを表示 （Issue: #54）\n",
    "# TSUMUGI v0.3.2: total 3.0M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# ファイルサイズが最大、最小のgene symbolのnode数を確認\n",
    "zcat data/network/mp_term_name/edema.json.gz | grep -c \"node_color\"\n",
    "zcat data/network/mp_term_name/prenatal_lethality_prior_to_heart_atrial_septation.json.gz | grep -c \"node_color\"\n",
    "zcat data/network/mp_term_name/preweaning_lethality,_complete_penetrance.json.gz | grep -c \"node_color\"\n",
    "zcat data/network/mp_term_name/convulsive_seizures.json.gz | grep -c \"node_color\"\n",
    "\n",
    "# 137\n",
    "# 119\n",
    "# 264\n",
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遺伝子ごとのネットワークを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_symbols = df_similarity.marker1.unique().tolist()\n",
    "gene_symbols += df_similarity.marker2.unique().tolist()\n",
    "gene_symbols = list(set(gene_symbols))\n",
    "gene_symbols.sort()  # 以下のfor文で、どこまで遺伝子が処理されたのか途中経過を見積もるためのソート\n",
    "P(gene_symbols[:3])\n",
    "P(len(gene_symbols))\n",
    "# version 0.2.2: 4139\n",
    "# version 0.3.0: 6812 (Life stageを考慮 + 類似度を追加)\n",
    "# version 0.3.1: 6812\n",
    "# version 0.3.2: 5583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data/overlap/available_gene_symbols.txt\").write_text(\n",
    "    \"\\n\".join(gene_symbols) + \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"data\", \"network\", \"gene_symbol\")\n",
    "# remove network directory\n",
    "if output_dir.exists():\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "# 10 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_nodes = 125\n",
    "tolerance = 25  # tolerance for the number of nodes\n",
    "upper_limit = number_of_nodes + tolerance\n",
    "lower_limit = number_of_nodes - tolerance\n",
    "\n",
    "for gene_symbol in tqdm(gene_symbols, desc=\"Processing Gene Symbols\"):\n",
    "    \"\"\"\n",
    "    ノードが多すぎるとWebページが描画できない問題を回避するため、\n",
    "    ノード数を200以下にするために最適なphenodigm_scoreを求める\n",
    "    \"\"\"\n",
    "    # --- 1. gene_symbol を含むエッジのみ抽出 ---------------------------\n",
    "    df_filtered = df_similarity[\n",
    "        (df_similarity[\"marker1\"] == gene_symbol)\n",
    "        | (df_similarity[\"marker2\"] == gene_symbol)\n",
    "    ]\n",
    "\n",
    "    # --- 2. 初期状態のノードの数を確認 -------------------------------\n",
    "    nodes = set(\n",
    "        pd.concat([df_filtered[\"marker1\"], df_filtered[\"marker2\"]], ignore_index=True)\n",
    "    )\n",
    "    num_nodes = len(nodes)\n",
    "\n",
    "    if num_nodes > upper_limit:\n",
    "        # --- 3. 離散スコア値で探索 ----------------------------------------\n",
    "        # gene_symbol と結ばれたエッジのスコア一覧（重複なし）を降順で取得\n",
    "        discrete_scores = df_filtered.loc[:, \"phenodigm_score\"].unique()\n",
    "        discrete_scores = np.sort(discrete_scores)[::-1]  # 降順\n",
    "\n",
    "        best_thr = None\n",
    "        best_diff = float(\"inf\")\n",
    "\n",
    "        lo, hi = 0, len(discrete_scores) - 1\n",
    "        while lo <= hi:\n",
    "            mid_idx = (lo + hi) // 2\n",
    "            thr = discrete_scores[mid_idx]\n",
    "\n",
    "            df_mid = df_filtered[df_filtered[\"phenodigm_score\"] >= thr]\n",
    "            # gene_symbol を含むエッジのみ抽出\n",
    "            df_mid = df_mid[\n",
    "                (df_mid[\"marker1\"] == gene_symbol) | (df_mid[\"marker2\"] == gene_symbol)\n",
    "            ]\n",
    "            nodes = set(\n",
    "                pd.concat([df_mid[\"marker1\"], df_mid[\"marker2\"]], ignore_index=True)\n",
    "            )\n",
    "\n",
    "            if gene_symbol not in nodes:\n",
    "                # gene_symbol が落ちた → 閾値が高すぎる（スコアを下げる）\n",
    "                lo = mid_idx + 1\n",
    "                continue\n",
    "\n",
    "            num_nodes = len(nodes)\n",
    "            # ▼ ノード数が許容範囲なら候補にする\n",
    "            if num_nodes <= upper_limit:\n",
    "                diff = abs(num_nodes - number_of_nodes)\n",
    "                if diff < best_diff:\n",
    "                    best_diff = diff\n",
    "                    best_thr = thr\n",
    "                # さらにノードを減らせるか？ → 閾値を **上げる**（スコアを大きく）\n",
    "                lo = mid_idx + 1\n",
    "            else:\n",
    "                # ノードが多すぎ → 閾値を **上げる**（スコアを大きく）\n",
    "                hi = mid_idx - 1\n",
    "\n",
    "        # ------------ 最終の閾値 ---------------------------------------\n",
    "        if best_thr is None:\n",
    "            # Upper limit 以下がどうしても存在しない極端ケース → 最小ノードになる閾値\n",
    "            best_thr = discrete_scores[hi + 1]  # hi は最後に -1 されているので +1\n",
    "\n",
    "        df_filtered = df_filtered[df_filtered[\"phenodigm_score\"] >= best_thr]\n",
    "        # gene_symbol を含むエッジのみ抽出\n",
    "        df_filtered = df_filtered[\n",
    "            (df_filtered[\"marker1\"] == gene_symbol)\n",
    "            | (df_filtered[\"marker2\"] == gene_symbol)\n",
    "        ]\n",
    "\n",
    "    nodes = set(\n",
    "        pd.concat([df_filtered[\"marker1\"], df_filtered[\"marker2\"]], ignore_index=True)\n",
    "    )\n",
    "\n",
    "    # ------------\n",
    "    # ネットワーク図のためのノードとエッジを作成\n",
    "    # ------------\n",
    "\n",
    "    # nodesを用意\n",
    "    node_json = []\n",
    "    for node in nodes:\n",
    "        phenotype = dict_marker_phenotype[node]\n",
    "        disease = dict_marker_disease.get(node, \"\")\n",
    "        # ノードの色を決定（gene_symbol の場合は 1、それ以外は 0）\n",
    "        node_color = 1.0 if node == gene_symbol else 0.0\n",
    "\n",
    "        node_json.append(\n",
    "            {\n",
    "                \"data\": {\n",
    "                    \"id\": node,\n",
    "                    \"label\": node,\n",
    "                    \"node_color\": node_color,\n",
    "                    \"phenotype\": phenotype,\n",
    "                    \"disease\": disease,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # edgesを用意\n",
    "    rows = df_similarity[\n",
    "        (df_similarity[\"marker1\"].isin(nodes)) & (df_similarity[\"marker2\"].isin(nodes))\n",
    "    ].to_dict(orient=\"records\")\n",
    "\n",
    "    # EdgeをJSON形式に変換\n",
    "    edge_json = [\n",
    "        {\n",
    "            \"data\": {\n",
    "                \"source\": r[\"marker1\"],\n",
    "                \"target\": r[\"marker2\"],\n",
    "                \"phenotype\": r[\"shared_mp\"],\n",
    "                \"edge_size\": r[\"phenodigm_score\"],\n",
    "            }\n",
    "        }\n",
    "        for r in rows\n",
    "    ]\n",
    "\n",
    "    network_json = node_json + edge_json\n",
    "\n",
    "    # Output as JSON\n",
    "    if network_json:\n",
    "        output_json = output_dir / f\"{gene_symbol}.json.gz\"\n",
    "        with gzip.open(output_json, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(network_json, f, indent=4)\n",
    "\n",
    "# 10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available_mp_termsを作成\n",
    "\n",
    "mp_terms = {}\n",
    "for path_mp_term in Path(\"data\", \"mp_term_name\").glob(\"*.csv\"):\n",
    "    mp_term = path_mp_term.stem\n",
    "    if not Path(\"data\", \"network\", \"mp_term_name\", f\"{mp_term}.json.gz\").exists():\n",
    "        continue\n",
    "    mp_term_name_space = mp_term.replace(\"_\", \" \")\n",
    "    mp_terms[mp_term_name_space] = mp_term\n",
    "\n",
    "json.dump(mp_terms, open(\"data/overlap/available_mp_terms.json\", \"w\"), indent=2)\n",
    "pd.DataFrame(mp_terms.keys()).to_csv(\n",
    "    \"data/overlap/available_mp_terms.txt\", index=False, header=False, sep=\"\\t\"\n",
    ")\n",
    "\n",
    "print(len(mp_terms))\n",
    "\n",
    "# TSUMUGI v0.3.2: 440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -lhS data/network/gene_symbol/ | head -n 5\n",
    "echo \"----------------------\"\n",
    "ls -lhS data/network/gene_symbol/ | tail -n 5\n",
    "# 30 sec\n",
    "# version 0.3.0: total 170M\n",
    "# version 0.3.1: total 168M\n",
    "# version 0.3.2: total 50M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# ファイルサイズが最大、最小のgene symbolのnode数を確認\n",
    "zcat data/network/gene_symbol/Dstn.json.gz | grep -c \"node_color\"\n",
    "zcat data/network/gene_symbol/Rab10.json.gz | grep -c \"node_color\"\n",
    "zcat data/network/gene_symbol/Plekha8.json.gz | grep -c \"node_color\"\n",
    "# 144\n",
    "# 95\n",
    "# 2\n",
    "\n",
    "zcat data/network/gene_symbol/Tcerg1.json.gz | grep -c \"node_color\"\n",
    "zcat data/network/gene_symbol/Jmjd7.json.gz | grep -c \"node_color\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "uname -a # OS name\n",
    "date +\"%Y/%m/%d %H:%M:%S\" # Last update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tsumugi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
